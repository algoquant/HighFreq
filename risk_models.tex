% Define knitr options
% !Rnw weave=knitr
% Set global chunk options



% Define document options
\documentclass[10pt]{beamer}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0, 0, 0}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.502,0,0.502}{\textbf{#1}}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.651,0.522,0}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{1,0.502,0}{#1}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{1,0,0.502}{\textbf{#1}}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.733,0.475,0.467}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.502,0.502,0.753}{\textbf{#1}}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0,0.502,0.753}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0,0.267,0.4}{#1}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usepackage{array}
\usepackage{multirow}
% mathtools package for math symbols
\usepackage{mathtools}
% bbm package for unitary vector or matrix symbol
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
\usepackage{listings}
\usepackage{xcolor}
\definecolor{anti_flashwhite}{rgb}{0.95, 0.95, 0.96}
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}


% Title page setup
\title[Risk Analysis and Model Construction]{Risk Analysis and Model Construction}
\subtitle{FRE6871 \& FRE7241, Spring 2021}
\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color.png}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{\today}



%%%%%%%%%%%%%%%

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}


%%%%%%%%%%%%%%%
\maketitle



%%%%%%%%%%%%%%%
\section{Modeling and Fitting Asset Returns}


%%%%%%%%%%%%%%%
\subsection{Kernel Density of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The kernel density is proportional to the number of data points close to a given point.
      \vskip1ex
      The kernel density is analogous to a histogram, but it provides more detailed information about the distribution of the data.
      \vskip1ex
      The kernel $K(x)$ is a symmetric function which decreases with the distance $x$.
      \vskip1ex
      The kernel density $d_i$ of a data sample $r_i$ is equal to the sum over the kernel function $K(x)$:
      \begin{displaymath}
        d_i = \sum_{j=1}^n {K(r_i - r_j)}
      \end{displaymath}
      The function \texttt{density()} calculates a kernel estimate of the probability density for a sample of data.
      \vskip1ex
      The function \texttt{density()} returns a vector of densities at equally spaced points, not for the original data points.
      \vskip1ex
      The function \texttt{approx()} interpolates a vector of data into another vector.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(rutils)  # Load package rutils
> # Calculate VTI percentage returns
> re_turns <- rutils::etf_env$re_turns$VTI
> re_turns <- drop(coredata(na.omit(re_turns)))
> n_rows <- NROW(re_turns)
> # Mean and standard deviation of returns
> c(mean(re_turns), sd(re_turns))
> # Calculate the MAD of returns 10 points apart
> re_turns <- sort(re_turns)
> b_w <- 10*mad(rutils::diff_it(re_turns, lagg=10))
> # Calculate the kernel density
> den_sity <- sapply(1:n_rows, function(i_d) {
+   sum(dnorm(re_turns-re_turns[i_d], sd=b_w))
+ })  # end sapply
> ma_d <- mad(re_turns)
> plot(re_turns, den_sity, xlim=c(-5*ma_d, 5*ma_d),
+      t="l", col="blue", lwd=3,
+      xlab="returns", ylab="density",
+      main="Density of VTI Returns")
> # Calculate the kernel density using density()
> den_sity <- density(re_turns, bw=b_w)
> NROW(den_sity$y)
> x11(width=6, height=5)
> plot(den_sity, xlim=c(-5*ma_d, 5*ma_d),
+      xlab="returns", ylab="density",
+      col="blue", lwd=3, main="Density of VTI Returns")
> # Interpolate the den_sity vector into re_turns
> den_sity <- approx(den_sity$x, den_sity$y, xout=re_turns)
> all.equal(den_sity$x, re_turns)
> plot(den_sity, xlim=c(-5*ma_d, 5*ma_d),
+      xlab="returns", ylab="density",
+      t="l", col="blue", lwd=3,
+      main="Density of VTI Returns")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Distribution of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Asset returns are usually not normally distributed and they exhibit \emph{leptokurtosis} (large kurtosis, or fat tails).
      \vskip1ex
      The function \texttt{hist()} calculates and plots a histogram, and returns its data \emph{invisibly}.
      \vskip1ex
      The parameter \texttt{breaks} is the number of cells of the histogram.
      \vskip1ex
      The function \texttt{lines()} draws a line through specified points.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hist_vti_dens.png}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot histogram
> histo_gram <- hist(re_turns, breaks=100, freq=FALSE,
+   xlim=c(-5*ma_d, 5*ma_d), xlab="", ylab="",
+   main="VTI Return Distribution")
> # Draw kernel density of histogram
> lines(den_sity, col="red", lwd=2)
> # Add density of normal distribution
> curve(expr=dnorm(x, mean=mean(re_turns), sd=sd(re_turns)),
+ add=TRUE, lwd=2, col="blue")
> # Add legend
> legend("topright", inset=0.05, cex=0.8, title=NULL,
+  leg=c("VTI", "Normal"), bty="n",
+  lwd=6, bg="white", col=c("red", "blue"))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Distribution of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Asset returns are usually not normally distributed and they exhibit \emph{leptokurtosis} (large kurtosis, or fat tails).
      \vskip1ex
      The function \texttt{hist()} calculates and plots a histogram, and returns its data \emph{invisibly}.
      \vskip1ex
      The parameter \texttt{breaks} is the number of cells of the histogram.
      \vskip1ex
      The function \texttt{density()} calculates a kernel estimate of the probability density for a sample of data.
      \vskip1ex
      The function \texttt{lines()} draws a line through specified points.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(rutils)  # Load package rutils
> # Calculate VTI percentage returns
> re_turns <- na.omit(rutils::etf_env$re_turns$VTI)
> # Mean and standard deviation of returns
> c(mean(re_turns), sd(re_turns))
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hist_vti.png}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot histogram
> x11(width=6, height=5)
> par(mar=c(1, 1, 1, 1), oma=c(2, 2, 2, 0))
> ma_d <- mad(re_turns)
> histo_gram <- hist(re_turns, breaks=100,
+   main="", xlim=c(-5*ma_d, 5*ma_d),
+   xlab="", ylab="", freq=FALSE)
> # Draw kernel density of histogram
> lines(density(re_turns), col="red", lwd=2)
> # Add density of normal distribution
> curve(expr=dnorm(x, mean=mean(re_turns), sd=sd(re_turns)),
+ add=TRUE, type="l", lwd=2, col="blue")
> title(main="VTI Return Distribution", line=0)  # Add title
> # Add legend
> legend("topright", inset=0.05, cex=0.8, title=NULL,
+  leg=c("VTI", "Normal"), bty="n",
+  lwd=6, bg="white", col=c("red", "blue"))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Quantile-Quantile Plot}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{Quantile-Quantile} (\emph{Q-Q}) plot is a plot of points with the same \emph{quantiles}, from two probability distributions.
      \vskip1ex
      If the two distributions are similar then all the points in the \emph{Q-Q} plot lie along the diagonal.
      \vskip1ex
      The \emph{VTI} \emph{Q-Q} plot shows that the \emph{VTI} return distribution has fat tails.
      \vskip1ex
      The \emph{p}-value of the \emph{Shapiro-Wilk} test is very close to zero, which shows that the \emph{VTI} returns are very unlikely to be normal.
      \vskip1ex
      The function \texttt{qqnorm()} produces a normal \emph{Q-Q} plot.
      \vskip1ex
      The function \texttt{qqline()} fits a line to the normal quantiles.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Create normal Q-Q plot
> qqnorm(re_turns, ylim=c(-0.1, 0.1), main="VTI Q-Q Plot",
+  xlab="Normal Quantiles")
> # Fit a line to the normal quantiles
> qqline(re_turns, col="red", lwd=2)
> # Perform Shapiro-Wilk test
> shapiro.test(as.numeric(re_turns))
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/qq_plot.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Boxplots of Distributions of Values}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Box-and-whisker plots (\emph{boxplots}) are graphical representations of a distribution of values.
      \vskip1ex
      The bottom and top box edges (\emph{hinges}) are equal to the first and third quartiles, and the \emph{box} width is equal to the interquartile range (\emph{IQR}).
      \vskip1ex
      The nominal range is equal to 1.5 times the \emph{IQR} above and below the box \emph{hinges}.
      \vskip1ex
      The \emph{whiskers} are dashed vertical lines representing values beyond the first and third quartiles, but within the nominal range.
      \vskip1ex
      The \emph{whiskers} end at the last values within the nominal range, while the open circles represent outlier values beyond the nominal range.
      \vskip1ex
      The function \texttt{boxplot()} has two \texttt{methods}: one for \texttt{formula} objects (for categorical variables), and another for \texttt{data frames}.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Boxplot method for formula
> boxplot(formula=mpg ~ cyl, data=mtcars,
+   main="Mileage by number of cylinders",
+   xlab="Cylinders", ylab="Miles per gallon")
> # Boxplot method for data frame of EuStockMarkets percentage returns
> boxplot(x=diff(log(EuStockMarkets)))
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-2em}
      \includegraphics[width=0.45\paperwidth]{figure/box_plots-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Higher Moments of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The estimators of moments of a probability distribution are given by:
      \vskip1ex
      Sample mean: $\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i$
      \vskip1ex
      Sample variance: $\hat\sigma^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})^2$
      \vskip1ex
      With their expected values equal to the population mean and standard deviation:\\
      $\mathbb{E}[\bar{x}] = \mu$ \hskip0.5em and \hskip0.5em $\mathbb{E}[\hat\sigma] = \sigma$
      \vskip1ex
      The sample skewness (third moment):
      {\scriptsize
      \begin{displaymath}
        \varsigma = \frac{n}{(n-1)(n-2)} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\hat\sigma})^3
      \end{displaymath}
      }
      The sample kurtosis (fourth moment):
      {\scriptsize
      \begin{displaymath}
        \kappa = \frac{n(n+1)}{(n-1)(n-2)(n-3)} \sum_{i=1}^n (\frac{x_i-\bar{x}}{\hat\sigma})^4
      \end{displaymath}
      }
      The normal distribution has skewness equal to $0$ and kurtosis equal to $3$.
      \vskip1ex
      Stock returns typically have negative skewness and kurtosis much greater than $3$.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # VTI percentage returns
> re_turns <- na.omit(rutils::etf_env$re_turns$VTI)
> # Number of observations
> n_rows <- NROW(re_turns)
> # Mean of VTI returns
> mean_rets <- mean(re_turns)
> # Standard deviation of VTI returns
> sd_rets <- sd(re_turns)
> # Skewness of VTI returns
> n_rows/((n_rows-1)*(n_rows-2))*
+   sum(((re_turns - mean_rets)/sd_rets)^3)
> # Kurtosis of VTI returns
> n_rows*(n_rows+1)/((n_rows-1)^3)*
+   sum(((re_turns - mean_rets)/sd_rets)^4)
> # Random normal returns
> re_turns <- rnorm(n_rows, sd=sd_rets)
> # Mean and standard deviation of random normal returns
> mean_rets <- mean(re_turns)
> sd_rets <- sd(re_turns)
> # Skewness of random normal returns
> n_rows/((n_rows-1)*(n_rows-2))*
+   sum(((re_turns - mean_rets)/sd_rets)^3)
> # Kurtosis of random normal returns
> n_rows*(n_rows+1)/((n_rows-1)^3)*
+   sum(((re_turns - mean_rets)/sd_rets)^4)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Functions for Calculating Skew and Kurtosis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{R} provides an easy way for users to write functions.
      \vskip1ex
      The function \texttt{calc\_skew()} calculates the skew of returns, and \texttt{calc\_kurt()} calculates the kurtosis.
      \vskip1ex
      Functions return the value of the last expression that is evaluated.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # calc_skew() calculates skew of returns
> calc_skew <- function(re_turns) {
+   re_turns <- na.omit(re_turns)
+   sum(((re_turns - mean(re_turns))/sd(re_turns))^3)/NROW(re_turns)
+ }  # end calc_skew
> # calc_kurt() calculates kurtosis of returns
> calc_kurt <- function(re_turns) {
+   re_turns <- na.omit(re_turns)
+   sum(((re_turns - mean(re_turns))/sd(re_turns))^4)/NROW(re_turns)
+ }  # end calc_kurt
> # Calculate skew and kurtosis of VTI returns
> calc_skew(re_turns)
> calc_kurt(re_turns)
> # calc_mom() calculates the moments of returns
> calc_mom <- function(re_turns, mo_ment=3) {
+   re_turns <- na.omit(re_turns)
+   sum(((re_turns - mean(re_turns))/sd(re_turns))^mo_ment)/NROW(re_turns)
+ }  # end calc_mom
> # Calculate skew and kurtosis of VTI returns
> calc_mom(re_turns, mo_ment=3)
> calc_mom(re_turns, mo_ment=4)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Standard Errors of Estimators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Statistical estimators are functions of samples (which are random variables), and therefore are themselves \emph{random variables}.
      \vskip1ex
      The \emph{standard error} (SE) of an estimator is defined as its \emph{standard deviation} (not to be confused with the \emph{population standard deviation} of the underlying random variable).
      \vskip1ex
      For example, the \emph{standard error} of the estimator of the mean is equal to:
      \begin{displaymath}
        \sigma_{\mu} = \frac{\sigma}{\sqrt{n}}
      \end{displaymath}
      Where $\sigma$ is the \emph{population standard deviation} (which is usually unkown).
      \vskip1ex
      The \emph{estimator} of this \emph{standard error} is equal to:
      \begin{displaymath}
        SE_{\mu} = \frac{\hat\sigma}{\sqrt{n}}
      \end{displaymath}
      where: $\hat\sigma^2=\frac{1}{n-1} \sum_{i=1}^n (x_i-\bar{x})^2$ is the sample standard deviation (the estimator of the population standard deviation).
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> set.seed(1121)  # Reset random number generator
> # Sample from Standard Normal Distribution
> n_rows <- 1000
> da_ta <- rnorm(n_rows)
> # Sample mean
> mean(da_ta)
> # Sample standard deviation
> sd(da_ta)
> # Standard error of sample mean
> sd(da_ta)/sqrt(n_rows)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Normal (Gaussian)} Probability Distribution}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Normal (Gaussian)} probability density function is given by:
      \begin{displaymath}
        \phi(x, \mu, \sigma) = \frac{e^{-(x-\mu)^2/2\sigma^2}}{\sigma\sqrt{2 \pi}}
      \end{displaymath}
      The \emph{Standard Normal} distribution $\phi(0, 1)$ is a special case of the \emph{Normal} $\phi(\mu, \sigma)$ with $\mu=0$ and $\sigma=1$.
      \vskip1ex
      The function \texttt{dnorm()} calculates the \emph{Normal} probability density.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> x_var <- seq(-5, 7, length=100)
> y_var <- dnorm(x_var, mean=1.0, sd=2.0)
> plot(x_var, y_var, type="l", lty="solid", xlab="", ylab="")
> title(main="Normal Density Function", line=0.5)
> star_t <- 3; fin_ish <- 5  # Set lower and upper bounds
> # Plot polygon area
> are_a <- ((x_var >= star_t) & (x_var <= fin_ish))
> polygon(c(star_t, x_var[are_a], fin_ish),
+   c(-1, y_var[are_a], -1), col="red")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
    \includegraphics[width=0.45\paperwidth]{figure/norm_dist}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Normal (Gaussian)} Probability Distributions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Plots of several \emph{Normal} distributions with different values of $\sigma$, using the function \texttt{curve()} for plotting functions given by their name.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> sig_mas <- c(0.5, 1, 1.5, 2)  # Sigma values
> # Create plot colors
> col_ors <- c("red", "black", "blue", "green")
> # Create legend labels
> lab_els <- paste("sigma", sig_mas, sep="=")
> for (in_dex in 1:4) {  # Plot four curves
+   curve(expr=dnorm(x, sd=sig_mas[in_dex]),
+   xlim=c(-4, 4), xlab="", ylab="", lwd=2,
+   col=col_ors[in_dex], add=as.logical(in_dex-1))
+ }  # end for
> # Add title
> title(main="Normal Distributions", line=0.5)
> # Add legend
> legend("topright", inset=0.05, title="Sigmas",
+  lab_els, cex=0.8, lwd=2, lty=1, bty="n", col=col_ors)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/norm_dist_mult_curves-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Student's \protect\emph{t-distribution}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Let $z_{1},\ldots , z_{\nu}$ be independent standard normal random variables, with sample mean: $\bar{z}=\frac{1}{\nu} \sum_{i=1}^{\nu} z_i$ ($\mathbb{E}[\bar{z}]=\mu$) and sample variance: $\hat\sigma^2=\frac{1}{\nu-1} \sum_{i=1}^{\nu} (z_i-\bar{z})^2$
      \vskip1ex
      Then the random variable (\emph{t-ratio}):
      \begin{displaymath}
        t = \frac{\bar{z} - \mu}{\hat\sigma / \sqrt{\nu}}
      \end{displaymath}
      Follows the \emph{t-distribution} with $\nu$ degrees of freedom, with the probability density function:
      \begin{displaymath}
        f(t) = \frac{\Gamma((\nu+1)/2)}{\sqrt{\pi \nu}\,\Gamma(\nu/2)}\, (1 + t^2/\nu)^{-(\nu+1)/2}
      \end{displaymath}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> deg_free <- c(3, 6, 9)  # Df values
> col_ors <- c("black", "red", "blue", "green")
> lab_els <- c("normal", paste("df", deg_free, sep="="))
> # Plot a Normal probability distribution
> curve(expr=dnorm, xlim=c(-4, 4), xlab="", ylab="", lwd=2)
> for (in_dex in 1:3) {  # Plot three t-distributions
+   curve(expr=dt(x, df=deg_free[in_dex]), xlab="", ylab="",
+ lwd=2, col=col_ors[in_dex+1], add=TRUE)
+ }  # end for
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/t_dist_mult.png}\\
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Add title
> title(main="t-distributions", line=0.5)
> # Add legend
> legend("topright", inset=0.05, bty="n",
+        title="Degrees\n of freedom", lab_els,
+        cex=0.8, lwd=6, lty=1, col=col_ors)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Mixture Models of Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Mixture models} are produced by randomly sampling data from different distributions.
      \vskip1ex
      The mixture of two normal distributions with different variances produces a distribution with \emph{leptokurtosis} (large kurtosis, or fat tails).
      \vskip1ex
      Student's \emph{t-distribution} has fat tails because the sample variance in the denominator of the \emph{t-ratio} is variable.
      \vskip1ex
      The time-dependent volatility of asset returns is referred to as \emph{heteroskedasticity}.
      \vskip1ex
      Random processes with \emph{heteroskedasticity} can be considered a type of mixture model.
      \vskip1ex
      The \emph{heteroskedasticity} produces \emph{leptokurtosis} (large kurtosis, or fat tails).
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Mixture of two normal distributions with sd=1 and sd=2
> n_rows <- 1e5
> re_turns <- c(rnorm(n_rows/2), 2*rnorm(n_rows/2))
> re_turns <- (re_turns-mean(re_turns))/sd(re_turns)
> # Kurtosis of normal
> calc_kurt(rnorm(n_rows))
> # Kurtosis of mixture
> calc_kurt(re_turns)
> # Or
> n_rows*sum(re_turns^4)/(n_rows-1)^2
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/mix_normal.png}
      \vspace{-2em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot the distributions
> plot(density(re_turns), xlab="", ylab="",
+   main="Mixture of Normal Returns",
+   xlim=c(-3, 3), type="l", lwd=3, col="red")
> curve(expr=dnorm, lwd=2, col="blue", add=TRUE)
> curve(expr=dt(x, df=3), lwd=2, col="green", add=TRUE)
> # Add legend
> legend("topright", inset=0.05, lty=1, lwd=6, bty="n",
+   legend=c("Mixture", "Normal", "t-distribution"),
+   col=c("red", "blue", "green"))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Likelihood Function of Student's \protect\emph{t-distribution}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The non-standard Student's \emph{t-distribution} is:
      {\scriptsize
      \begin{displaymath}
        f(t) = \frac{\Gamma((\nu+1)/2)}{\sqrt{\pi \nu} \, \sigma \, \Gamma(\nu/2)} \, (1 + (\frac{t - \mu}{\sigma})^2/\nu)^{-(\nu+1)/2}
      \end{displaymath}
      }
       It has non-zero mean equal to the location parameter $\mu$, and a standard deviation proportional to the scale parameter $\sigma$.
      \vskip1ex
      The negative logarithm of the probability density is equal to:
      {\scriptsize
      \begin{multline*}
        -\log(f(t)) = -\log(\frac{\Gamma((\nu+1)/2)}{\sqrt{\pi \nu} \, \Gamma(\nu/2)}) + \log(\sigma) + \\
        \frac{\nu+1}{2} \, \log(1 + (\frac{t - \mu}{\sigma})^2/\nu)
      \end{multline*}
      }
      The \emph{likelihood} function $\mathcal{L}(\theta|\bar{x})$ is a function of the model parameters $\theta$, given the observed values $\bar{x}$, under the model's probability distribution $f(x|\theta)$:
      {\scriptsize
      \begin{displaymath}
        \mathcal{L}(\theta|x) = \prod_{i=1}^{n} f(x_i|\theta)
      \end{displaymath}
      }
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Objective function is log-likelihood
> likeli_hood <- function(pa_r, free_dom, da_ta) {
+   sum(
+     -log(gamma((free_dom+1)/2) /
+       (sqrt(pi*free_dom) * gamma(free_dom/2))) +
+     log(pa_r[2]) +
+     (free_dom+1)/2 * log(1 + ((da_ta - pa_r[1])/
+                     pa_r[2])^2/free_dom))
+ }  # end likeli_hood
> # Demonstrate equivalence with log(dt())
> likeli_hood(c(1, 0.5), 2, 2:5)
> -sum(log(dt(x=(2:5-1)/0.5, df=2)/0.5))
> # Simpler objective function
> likeli_hood <- function(pa_r, free_dom, da_ta) {
+   -sum(log(dt(x=(da_ta-pa_r[1])/pa_r[2],
+       df=free_dom)/pa_r[2]))
+ }  # end likeli_hood
\end{verbatim}
\end{kframe}
\end{knitrout}
      \vspace{-1em}
      The \emph{likelihood} function measures how \emph{likely} are the parameters, given the observed values $\bar{x}$.
      \vskip1ex
      The \emph{maximum-likelihood} estimate (\emph{MLE}) of the parameters are those that maximize the \emph{likelihood} function:
      \begin{displaymath}
        \theta_{MLE} = \operatorname*{arg\,max}_{\theta} {\mathcal{L}(\theta|x)}
      \end{displaymath}
      In practice the logarithm of the \emph{likelihood} $\log(\mathcal{L})$ is maximized, instead of the \emph{likelihood} itself.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fitting Asset Returns into Student's \protect\emph{t-distribution}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{fitdistr()} from package \emph{MASS} fits a univariate distribution to a sample of data, by performing \emph{maximum likelihood} optimization.
      \vskip1ex
      The function \texttt{fitdistr()} performs a \emph{maximum likelihood} optimization to find the non-standardized Student's \emph{t-distribution} location and scale parameters.
    \column{0.5\textwidth}
    \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # VTI percentage returns
> re_turns <- na.omit(rutils::etf_env$re_turns$VTI)
> # Initial parameters
> par_init <- c(mean=0, scale=0.01)
> # Fit distribution using optim()
> optim_fit <- optim(par=par_init,
+   fn=likeli_hood, # Log-likelihood function
+   da_ta=re_turns,
+   free_dom=2, # Degrees of freedom
+   method="L-BFGS-B", # quasi-Newton method
+   upper=c(1, 0.1), # upper constraint
+   lower=c(-1, 1e-7)) # Lower constraint
> # optimal parameters
> lo_cation <- optim_fit$par["mean"]
> scal_e <- optim_fit$par["scale"]
> # Fit VTI returns using MASS::fitdistr()
> optim_fit <- MASS::fitdistr(re_turns,
+   densfun="t", df=2)
> optim_fit$estimate
> optim_fit$sd
> lo_cation <- optim_fit$estimate[1]
> scal_e <- optim_fit$estimate[2]
> summary(optim_fit)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Student's \protect\emph{t-distribution} Fitted to Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Asset returns typically exhibit \emph{negative skewness} and \emph{large kurtosis} (leptokurtosis), or fat tails.
      \vskip1ex
      The function \texttt{hist()} calculates and plots a histogram, and returns its data \emph{invisibly}.
      \vskip1ex
      The parameter \texttt{breaks} is the number of cells of the histogram.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot histogram of VTI returns
> histo_gram <- hist(re_turns, col="lightgrey",
+   xlab="returns", breaks=100, xlim=c(-0.05, 0.05),
+   ylab="frequency", freq=FALSE, main="VTI Returns Histogram")
> lines(density(re_turns, adjust=1.5), lwd=3, col="blue")
> # Plot the Normal probability distribution
> curve(expr=dnorm(x, mean=mean(re_turns),
+   sd=sd(re_turns)), add=TRUE, lwd=3, col="green")
> # Plot t-distribution function
> curve(expr=dt((x-lo_cation)/scal_e, df=2)/scal_e,
+ type="l", lwd=3, col="red", add=TRUE)
> # Add legend
> legend("topright", inset=0.05, bty="n",
+   leg=c("density", "t-distr", "normal"),
+   lwd=6, lty=1, col=c("blue", "red", "green"))
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/t_dist_rets.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Leptokurtosis Fat Tails of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The probability under the \emph{normal} distribution decreases exponentially for large values of $x$:
      \begin{displaymath}
        \phi(x) \propto e^{-{x^2/2\sigma^2}} \qquad (as \, {\left| x \right|} \to \infty)
      \end{displaymath}
      This is because a normal variable can be thought of as the sum of a large number of independent binomial variables of equal size.
      \vskip1ex
      So large values are produced only when all the contributing binomial variables are of the same sign, which is very improbable, so it produces extremely low tail probabilities (thin tails),
      \vskip1ex
      But in reality, the probability of large negative asset returns decreases much slower, as the negative power of the returns (fat tails).
      \vskip1ex
      The probability under Student's \emph{t-distribution} decreases as a power for large values of $x$:
      \begin{displaymath}
        f(x) \propto {\left| x \right|}^{-(\nu+1)} \qquad (as \, {\left| x \right|} \to \infty)
      \end{displaymath}
      This is because a \emph{t-variable} can be thought of as the sum of normal variables with different volatilities (different sizes).
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/t_dist_tail_rets.png}
      \vspace{-2em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot histogram of VTI returns
> histo_gram <- hist(re_turns, breaks=100, plot=FALSE)
> plot(histo_gram, xlab="returns", ylab="frequency",
+      col="lightgrey", freq=FALSE, main="VTI Left Tail Returns Histogram",
+      xlim=c(min(re_turns), -0.02),
+      ylim=c(0.0, histo_gram$density[findInterval(-0.02, histo_gram$breaks)]))
> lines(density(re_turns, adjust=1.5), lwd=4, col="blue")
> # Plot t-distribution function
> curve(expr=dt((x-lo_cation)/scal_e, df=2)/scal_e, type="l", lwd=4, col="red", add=TRUE)
> # Plot the Normal probability distribution
> curve(expr=dnorm(x, mean=mean(re_turns), sd=sd(re_turns)), add=TRUE, lwd=4, col="green")
> # Add legend
> legend("topleft", inset=0.05, bty="n",
+   leg=c("density", "t-distr", "normal"),
+   lwd=6, lty=1, col=c("blue", "red", "green"))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: (move to Hurst slides?): Asset Returns in Trading Time}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The time-dependent volatility of asset returns (\emph{heteroskedasticity}) produces their fat tails (\emph{leptokurtosis}).
      \vskip1ex
      Higher levels of volatility coincide with higher \emph{trading volumes}.
      \vskip1ex
      If asset returns were measured at fixed intervals of \emph{trading volumes} (\emph{trading time} instead of clock time), then the volatility would be lower and less time-dependent.
      \vskip1ex
      The asset returns can be adjusted to \emph{trading time} by dividing them by the \emph{trading volume}, to obtain scaled returns over equal trading volumes.
      \vskip1ex
      The scaled returns have a smaller \emph{skewness} and \emph{kurtosis} than unscaled returns.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate VTI returns and volume
> # oh_lc <- HighFreq::SPY
> oh_lc <- rutils::etf_env$VTI
> clos_e <- drop(coredata(quantmod::Cl(oh_lc)))
> re_turns <- rutils::diff_it(log(clos_e))
> vol_ume <- drop(coredata(quantmod::Vo(oh_lc)))
> # Calculate moving average volume
> look_back <- 121
> volume_roll <- HighFreq::roll_vec(vol_ume, look_back=look_back)/look_back
> dygraphs::dygraph(xts(volume_roll, index(oh_lc)),
+   main="Moving Average VTI Volume") %>%
+   dySeries(strokeWidth=3, col="blue")
> # Scale returns using volume (volume clock)
> # rets_scaled <- sqrt(volume_roll)*re_turns/sqrt(vol_ume)
> rets_scaled <- ifelse(vol_ume > 0, volume_roll*re_turns/vol_ume, 0)
> rets_scaled <- sd(re_turns)*rets_scaled/sd(rets_scaled)
> 
> 
> # rets_scaled <- ifelse(vol_ume > 1e4, re_turns/vol_ume, 0)
> # Calculate moments of scaled returns
> # n_rows <- NROW(re_turns)
> # sapply(list(re_turns=re_turns, rets_scaled=rets_scaled),
> #   function(rets) {sapply(c(skew=3, kurt=4),
> #            function(x) sum((rets/sd(rets))^x)/n_rows)
> # })  # end sapply
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hf_scaled.png}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> x11(width=6, height=5)
> par(mar=c(3, 3, 2, 1), oma=c(1, 1, 1, 1))
> # Plot densities of SPY returns
> ma_d <- mad(re_turns)
> # b_w <- mad(rutils::diff_it(re_turns))
> plot(density(re_turns, bw=ma_d/10), xlim=c(-5*ma_d, 5*ma_d),
+      lwd=3, mgp=c(2, 1, 0), col="blue",
+      xlab="returns (standardized)", ylab="frequency",
+      main="Density of Volume-scaled VTI Returns")
> lines(density(rets_scaled, bw=ma_d/10), lwd=3, col="red")
> curve(expr=dnorm(x, mean=mean(re_turns), sd=sd(re_turns)),
+ add=TRUE, lwd=3, col="green")
> # Add legend
> legend("topright", inset=0.05, bty="n",
+   leg=c("minutely", "scaled", "normal"),
+   lwd=6, lty=1, col=c("blue", "red", "green"))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Central Limit Theorem}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Let $x_1,\ldots , x_n$ be independent and identically distributed (i.i.d.) random variables with expected value $\mu$ and variance $\sigma^2$, and let $\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i$ be their mean.
      \vskip1ex
      The random variables $x_i$ don't have to be normally distributed, they only need a finite second moment $\sigma$.
      \vskip1ex
      The \emph{Central Limit Theorem} states that as $n \to \infty$, then in the limit, the random variable $z$:
      \begin{displaymath}
        z = \frac{\bar{x} - \mu}{\sigma / \sqrt{n}}
      \end{displaymath}
      Follows the \emph{standard normal} distribution $\phi(0, 1)$.

    \column{0.5\textwidth}
      The \emph{normal} distribution is the limiting distribution of sums of random variables which have a finite second moment.
      \vskip1ex
      For example, the sums of random variables with fat tails, which decrease as a power for large values of $x$:
      \begin{displaymath}
        f(x) \propto {\left| x \right|}^{-(\nu+1)} \qquad (with \; \nu > 1)
      \end{displaymath}
      Tend to the \emph{standard normal} distribution $\phi(0, 1)$.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Kolmogorov-Smirnov} Test for Probability Distributions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Kolmogorov-Smirnov} test is designed to test the \emph{null hypothesis} that two samples: $\{x_1, \ldots , x_n\}$ and $\{y_1, \ldots , y_n\}$ were obtained from the same probability distribution.
      \vskip1ex
      The \emph{Kolmogorov-Smirnov} statistic is the maximum difference between two empirical cumulative distribution functions (cumulative frequencies):
      \begin{displaymath}
        D = \sup_i | P(x_i) - P(y_i) |
      \end{displaymath}
      The function \texttt{ks.test()} performs the \emph{Kolmogorov-Smirnov} test and returns the statistic and its \emph{p}-value \emph{invisibly}.
      \vskip1ex
      The second argument is either a \texttt{numeric} vector of data values, or a name of a cumulative distribution function.
      \vskip1ex
      The \emph{Kolmogorov-Smirnov} test can be used as a \emph{goodness of fit} test, to test if a set of observations fits a probability distribution.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # KS test for normal distribution
> ks.test(rnorm(100), pnorm)
> # KS test for uniform distribution
> ks.test(runif(100), pnorm)
> # KS test for two similar normal distributions
> ks.test(rnorm(100), rnorm(100, mean=0.1))
> # KS test for two different normal distributions
> ks.test(rnorm(100), rnorm(100, mean=1.0))
> # Fit t-dist into VTI returns
> re_turns <- na.omit(rutils::etf_env$re_turns$VTI)
> optim_fit <- MASS::fitdistr(re_turns, densfun="t", df=2)
> lo_cation <- optim_fit$estimate[1]
> scal_e <- optim_fit$estimate[2]
> # Perform Kolmogorov-Smirnov test on VTI returns
> da_ta <- lo_cation + scal_e*rt(NROW(re_turns), df=2)
> ks.test(as.numeric(re_turns), da_ta)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{Chi-squared} Distribution}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Let $z_1, \ldots , z_k$ be independent standard \emph{Normal} random variables.
      \vskip1ex
      Then the random variable $X = \sum_{i=1}^k z_i^2$ is distributed according to the \emph{Chi-squared} distribution with $k$ degrees of freedom: $X \sim \chi_k^2$, and its probability density function is given by:
      \begin{displaymath}
        f(x) = \frac{x^{k/2-1}\,e^{-x/2}}{2^{k/2}\, \Gamma(k/2)}
      \end{displaymath}
      \vskip1ex
      The \emph{Chi-squared} distribution with $k$ degrees of freedom has mean equal to $k$ and variance equal to $2k$.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Degrees of freedom
> deg_free <- c(2, 5, 8, 11)
> # Plot four curves in loop
> col_ors <- c("red", "black", "blue", "green")
> for (in_dex in 1:4) {
+   curve(expr=dchisq(x, df=deg_free[in_dex]),
+   xlim=c(0, 20), ylim=c(0, 0.3),
+   xlab="", ylab="", col=col_ors[in_dex],
+   lwd=2, add=as.logical(in_dex-1))
+ }  # end for
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/chisq_dist_mult.png}\\
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Add title
> title(main="Chi-squared Distributions", line=0.5)
> # Add legend
> lab_els <- paste("df", deg_free, sep="=")
> legend("topright", inset=0.05, bty="n",
+        title="Degrees of freedom", lab_els,
+        cex=0.8, lwd=6, lty=1, col=col_ors)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Chi-squared} Test for the Goodness of Fit}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Goodness of Fit} tests are designed to test if a set of observations fits an assumed theoretical probability distribution.
      \vskip1ex
      The \emph{Chi-squared} test tests if a frequency of counts fits the specified distribution.
      \vskip1ex
      The \emph{Chi-squared} statistic is the sum of squared differences between the observed frequencies $o_i$ and the theoretical frequencies $p_i$:
      \begin{displaymath}
        \chi^2 = N \sum_{i=1}^{n} {\frac{(o_i - p_i )^2}{p_i}}
      \end{displaymath}
      Where $N$ is the total number of observations.
      \vskip1ex
      The \emph{null hypothesis} is that the observed frequencies are consistent with the theoretical distribution.
      \vskip1ex
      The function \texttt{chisq.test()} performs the \emph{Chi-squared} test and returns the statistic and its \emph{p}-value \emph{invisibly}.
      \vskip1ex
      The parameter \texttt{breaks} in the function \texttt{hist()} should be chosen large enough to capture the shape of the frequency distribution.
    \column{0.5\textwidth}
    \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Observed frequencies from random normal data
> histo_gram <- hist(rnorm(1e3, mean=0), breaks=100, plot=FALSE)
> freq_o <- histo_gram$counts
> # Theoretical frequencies
> freq_t <- rutils::diff_it(pnorm(histo_gram$breaks))
> # Perform Chi-squared test for normal data
> chisq.test(x=freq_o, p=freq_t, rescale.p=TRUE, simulate.p.value=TRUE)
> # Return p-value
> chisq_test <- chisq.test(x=freq_o, p=freq_t, rescale.p=TRUE, simulate.p.value=TRUE)
> chisq_test$p.value
> # Observed frequencies from shifted normal data
> histo_gram <- hist(rnorm(1e3, mean=2), breaks=100, plot=FALSE)
> freq_o <- histo_gram$counts/sum(histo_gram$counts)
> # Theoretical frequencies
> freq_t <- rutils::diff_it(pnorm(histo_gram$breaks))
> # Perform Chi-squared test for shifted normal data
> chisq.test(x=freq_o, p=freq_t, rescale.p=TRUE, simulate.p.value=TRUE)
> # Calculate histogram of VTI returns
> histo_gram <- hist(re_turns, breaks=100, plot=FALSE)
> freq_o <- histo_gram$counts
> # Calculate cumulative probabilities and then difference them
> freq_t <- pt((histo_gram$breaks-lo_cation)/scal_e, df=2)
> freq_t <- rutils::diff_it(freq_t)
> # Perform Chi-squared test for VTI returns
> chisq.test(x=freq_o, p=freq_t, rescale.p=TRUE, simulate.p.value=TRUE)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Risk and Performance Analysis}


%%%%%%%%%%%%%%%
\subsection{Package \protect\emph{PerformanceAnalytics} for Risk and Performance Analysis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{PerformanceAnalytics} contains functions for risk and performance analysis.
      \vskip1ex
      The function \texttt{data()} loads external data or lists data sets in a package.
      \vskip1ex
      \texttt{managers} is an \emph{xts} time series containing monthly percentage returns of six asset managers (HAM1 through HAM6), the EDHEC Long-Short Equity hedge fund index, the \texttt{S\&P 500}, and US Treasury 10-year bond and 3-month bill total returns.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Load package PerformanceAnalytics
> library(PerformanceAnalytics)
> # Get documentation for package PerformanceAnalytics
> # Get short description
> packageDescription("PerformanceAnalytics")
> # Load help page
> help(package="PerformanceAnalytics")
> # List all objects in PerformanceAnalytics
> ls("package:PerformanceAnalytics")
> # List all datasets in PerformanceAnalytics
> data(package="PerformanceAnalytics")
> # Remove PerformanceAnalytics from search path
> detach("package:PerformanceAnalytics")
\end{verbatim}
\end{kframe}
\end{knitrout}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> perf_data <- unclass(data(
+     package="PerformanceAnalytics"))$results[, -(1:2)]
> apply(perf_data, 1, paste, collapse=" - ")
> # Load "managers" data set
> data(managers)
> class(managers)
> dim(managers)
> head(managers, 3)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plots of Cumulative Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{chart.CumReturns()} from package \emph{PerformanceAnalytics} plots the cumulative returns of a time series of returns.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Load package "PerformanceAnalytics"
> library(PerformanceAnalytics)
> # Calculate ETF returns
> re_turns <- rutils::etf_env$re_turns[, c("VTI", "DBC", "IEF")]
> re_turns <- na.omit(re_turns)
> # Plot cumulative ETF returns
> x11(width=6, height=5)
> chart.CumReturns(re_turns, lwd=2, ylab="",
+   legend.loc="topleft", main="ETF Cumulative Returns")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/perf_analytics_cum_returns.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Distribution of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{chart.Histogram()} from package \emph{PerformanceAnalytics} plots the histogram (frequency distribution) and the density of returns.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> re_turns <- rutils::etf_env$re_turns$VTI
> re_turns <- na.omit(re_turns)
> x11(width=6, height=5)
> chart.Histogram(re_turns, xlim=c(-0.04, 0.04),
+   colorset = c("lightgray", "red", "blue"), lwd=3,
+   main=paste("Distribution of", colnames(re_turns), "Returns"),
+   methods = c("add.density", "add.normal"))
> legend("topright", inset=0.05, bty="n",
+  leg=c("VTI Density", "Normal"),
+  lwd=6, lty=1, col=c("red", "blue"))
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/returns_histogram.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Boxplots of Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{chart.Boxplot()} from package \emph{PerformanceAnalytics} plots a box-and-whisker plot for a distribution of returns.
      \vskip1ex
      The function \texttt{chart.Boxplot()} is a wrapper and calls the function \texttt{graphics::boxplot()} to plot the box plots.
      \vskip1ex
      A \emph{box plot} (box-and-whisker plot) is a graphical display of a distribution of data: \\
      The \emph{box} represents the upper and lower quartiles, \\
      The vertical lines (whiskers) represent values beyond the quartiles, \\
      Open circles represent values beyond the nominal range (outliers).
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> re_turns <- rutils::etf_env$re_turns[,
+   c("VTI", "IEF", "IVW", "VYM", "IWB", "DBC", "VXX")]
> x11(width=6, height=5)
> chart.Boxplot(names=FALSE, re_turns)
> par(cex.lab=0.8, cex.axis=0.8)
> axis(side=2, at=(1:NCOL(re_turns))/7.5-0.05,labels=colnames(re_turns))
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/perf_analytics_box_plot.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Median Absolute Deviation Estimator of Dispersion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Median Absolute Deviation} (\emph{MAD}) is a robust measure of dispersion (variability), defined using the median instead of the mean:
      \begin{displaymath}
        \operatorname{MAD} = \operatorname{median}(\operatorname{abs}(x_i - \operatorname{median}(\mathbf{x})))
      \end{displaymath}
      The advantage of \emph{MAD} is that it's always well defined, even for data that has infinite variance.
      \vskip1ex
      The \emph{MAD} for normally distributed data is equal to $\Phi^{-1}(0.75) \cdot \hat\sigma = 0.6745 \cdot \hat\sigma$.
      \vskip1ex
      The function \texttt{mad()} calculates the \emph{MAD} and divides it by $\Phi^{-1}(0.75)$ to make it comparable to the standard deviation.
      \vskip1ex
      For normally distributed data the \emph{MAD} has a larger standard error than the standard deviation.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Simulate normally distributed data
> n_rows <- 1000
> da_ta <- rnorm(n_rows)
> sd(da_ta)
> mad(da_ta)
> median(abs(da_ta - median(da_ta)))
> median(abs(da_ta - median(da_ta)))/qnorm(0.75)
> # Bootstrap of sd and mad estimators
> boot_data <- sapply(1:10000, function(x) {
+   sampl_e <- da_ta[sample.int(n_rows, replace=TRUE)]
+   c(sd=sd(sampl_e), mad=mad(sampl_e))
+ })  # end sapply
> boot_data <- t(boot_data)
> # Analyze bootstrapped variance
> head(boot_data)
> sum(is.na(boot_data))
> # Means and standard errors from bootstrap
> apply(boot_data, MARGIN=2, function(x)
+   c(mean=mean(x), std_error=sd(x)))
> # Parallel bootstrap under Windows
> library(parallel)  # Load package parallel
> n_cores <- detectCores() - 1  # Number of cores
> clus_ter <- makeCluster(n_cores)  # Initialize compute cluster
> boot_data <- parLapply(clus_ter, 1:10000,
+   function(x, da_ta) {
+     sampl_e <- da_ta[sample.int(n_rows, replace=TRUE)]
+     c(sd=sd(sampl_e), mad=mad(sampl_e))
+   }, da_ta=da_ta)  # end parLapply
> # Parallel bootstrap under Mac-OSX or Linux
> boot_data <- mclapply(1:10000, function(x) {
+     sampl_e <- da_ta[sample.int(n_rows, replace=TRUE)]
+     c(sd=sd(sampl_e), mad=mad(sampl_e))
+   }, mc.cores=n_cores)  # end mclapply
> stopCluster(clus_ter)  # Stop R processes over cluster
> boot_data <- rutils::do_call(rbind, boot_data)
> # Means and standard errors from bootstrap
> apply(boot_data, MARGIN=2, function(x)
+   c(mean=mean(x), std_error=sd(x)))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Median Absolute Deviation of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      For normally distributed data the \emph{MAD} has a larger standard error than the standard deviation.
      \vskip1ex
      But for distributions with fat tails (like asset returns), the standard deviation has a larger standard error than the \emph{MAD}.
      \vskip1ex
      The \emph{bootstrap} procedure performs a loop, which naturally lends itself to parallel computing.
      \vskip1ex
      The function \texttt{makeCluster()} starts running \texttt{R} processes on several CPU cores under \emph{Windows}.
      \vskip1ex
      The function \texttt{parLapply()} is similar to \texttt{lapply()}, and performs loops under \emph{Windows} using parallel computing on several CPU cores.
      \vskip1ex
      The \texttt{R} processes started by \texttt{makeCluster()} don't inherit any data from the parent \texttt{R} process.
      \vskip1ex
      Therefore the required data must be either passed into \texttt{parLapply()} via the dots \texttt{"..."} argument, or by calling the function \texttt{clusterExport()}.
      \vskip1ex
      The function \texttt{mclapply()} performs loops using parallel computing on several CPU cores under \emph{Mac-OSX} or \emph{Linux}.
      \vskip1ex
      The function \texttt{stopCluster()} stops the \texttt{R} processes running on several CPU cores.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # VTI returns
> re_turns <- rutils::etf_env$re_turns$VTI
> re_turns <- na.omit(re_turns)
> n_rows <- NROW(re_turns)
> sd(re_turns)
> mad(re_turns)
> # Bootstrap of sd and mad estimators
> boot_data <- sapply(1:10000, function(x) {
+   sampl_e <- re_turns[sample.int(n_rows, replace=TRUE)]
+   c(sd=sd(sampl_e), mad=mad(sampl_e))
+ })  # end sapply
> boot_data <- t(boot_data)
> # Means and standard errors from bootstrap
> 100*apply(boot_data, MARGIN=2, function(x)
+   c(mean=mean(x), std_error=sd(x)))
> # Parallel bootstrap under Windows
> library(parallel)  # Load package parallel
> n_cores <- detectCores() - 1  # Number of cores
> clus_ter <- makeCluster(n_cores)  # Initialize compute cluster
> clusterExport(clus_ter, c("n_rows", "re_turns"))
> boot_data <- parLapply(clus_ter, 1:10000,
+   function(x) {
+     sampl_e <- re_turns[sample.int(n_rows, replace=TRUE)]
+     c(sd=sd(sampl_e), mad=mad(sampl_e))
+   })  # end parLapply
> # Parallel bootstrap under Mac-OSX or Linux
> boot_data <- mclapply(1:10000, function(x) {
+     sampl_e <- re_turns[sample.int(n_rows, replace=TRUE)]
+     c(sd=sd(sampl_e), mad=mad(sampl_e))
+   }, mc.cores=n_cores)  # end mclapply
> stopCluster(clus_ter)  # Stop R processes over cluster
> boot_data <- rutils::do_call(rbind, boot_data)
> # Means and standard errors from bootstrap
> apply(boot_data, MARGIN=2, function(x)
+   c(mean=mean(x), std_error=sd(x)))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Downside Deviation of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Some investors argue that positive returns don't represent risk, only those returns less than the target rate of return $r_t$.
      \vskip1ex
      The \emph{Downside Deviation} (semi-deviation) $\sigma_{d}$ is equal to the standard deviation of returns less than the target rate of return $r_t$:
      \begin{displaymath}
        \sigma_{d} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} ([r_i-r_t]_{-})^2}
      \end{displaymath}
      The function \texttt{DownsideDeviation()} from package \emph{PerformanceAnalytics} calculates the downside deviation, for either the full time series (\texttt{method="full"}) or only for the subseries less than the target rate of return $r_t$ (\texttt{method="subset"}).
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(PerformanceAnalytics)
> # Define target rate of return of 50 bps
> tar_get <- 0.005
> # Calculate the full downside returns
> returns_sub <- (re_turns - tar_get)
> returns_sub <- ifelse(returns_sub < 0, returns_sub, 0)
> n_rows <- NROW(returns_sub)
> # Calculate the downside deviation
> all.equal(sqrt(sum(returns_sub^2)/n_rows),
+   drop(DownsideDeviation(re_turns, MAR=tar_get, method="full")))
> # Calculate the subset downside returns
> returns_sub <- (re_turns - tar_get)
> returns_sub <- returns_sub[returns_sub < 0]
> n_rows <- NROW(returns_sub)
> # Calculate the downside deviation
> all.equal(sqrt(sum(returns_sub^2)/n_rows),
+   DownsideDeviation(re_turns, MAR=tar_get, method="subset"))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Drawdown Risk}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{drawdown} is the drop in prices from their historical peak, and is equal to the difference between the prices minus the cumulative maximum of the prices.
      \vskip1ex
      \emph{Drawdown risk} determines the risk of liquidation due to stop loss limits.
      \vskip1ex
      The function \texttt{table.Drawdowns()} from package \emph{PerformanceAnalytics} calculates a data frame of drawdowns.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate time series of VTI drawdowns
> clos_e <- log(na.omit(rutils::etf_env$price_s$VTI))
> draw_downs <- (clos_e - cummax(clos_e))
> # PerformanceAnalytics plot of VTI drawdowns
> re_turns <- rutils::diff_it(log(clos_e))
> PerformanceAnalytics::chart.Drawdown(re_turns,
+   ylab="", main="VTI Drawdowns")
> # PerformanceAnalytics table of VTI drawdowns
> PerformanceAnalytics::table.Drawdowns(re_turns)
> # dygraph plot of VTI drawdowns
> da_ta <- cbind(clos_e, draw_downs)
> col_names <- c("VTI", "Drawdowns")
> colnames(da_ta) <- col_names
> dygraphs::dygraph(da_ta, main="VTI Drawdowns") %>%
+   dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
+   dyAxis("y2", label=col_names[2], valueRange=c(min(da_ta[, "Drawdowns"]), 5), independentTicks=TRUE) %>%
+   dySeries(name=col_names[1], axis="y", col="blue") %>%
+   dySeries(name=col_names[2], axis="y2", col="red")
> # Plot VTI drawdowns
> plot_theme <- chart_theme()
> plot_theme$col$line.col <- c("blue")
> x11(width=6, height=5)
> quantmod::chart_Series(x=draw_downs, name="VTI Drawdowns", theme=plot_theme)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/drawdown_plot.png}
\begin{table}[ht]
\centering
\begingroup\tiny
\begin{tabular}{lllrrrr}
  \hline
From & Trough & To & Depth & Length & To Trough & Recovery \\ 
  \hline
2007-10-10 & 2009-03-09 & 2012-09-13 & -0.20 & 1243.00 & 355.00 & 888.00 \\ 
  2001-06-06 & 2002-10-09 & 2004-11-12 & -0.13 & 864.00 & 336.00 & 528.00 \\ 
  2020-02-20 & 2020-03-23 & 2020-08-24 & -0.09 & 130.00 & 23.00 & 107.00 \\ 
  2018-09-21 & 2018-12-24 & 2019-04-23 & -0.05 & 146.00 & 65.00 & 81.00 \\ 
  2015-06-24 & 2016-02-11 & 2016-07-08 & -0.04 & 263.00 & 161.00 & 102.00 \\ 
   \hline
\end{tabular}
\endgroup
\end{table}

  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\texttt{PerformanceSummary} Plots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{charts.PerformanceSummary()} from package \emph{PerformanceAnalytics} plots three charts: cumulative returns, return bars, and drawdowns, for time series of returns.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> data(managers)
> charts.PerformanceSummary(ham_1,
+   main="", lwd=2, ylog=TRUE)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
    \vspace{-3em}
      \includegraphics[width=0.45\paperwidth]{figure/performance_summary-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Loss Distribution of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The distribution of returns has a long left tail of negative returns representing the risk of loss.
      \vskip1ex
      The \emph{Value at Risk} ($\mathrm{VaR}$) is equal to the quantile of returns corresponding to a given confidence level $\alpha$.
      \vskip1ex
      The \emph{Conditional Value at Risk} ($\mathrm{CVaR}$) is equal to the average of negative returns less than the $\mathrm{VaR}$.
      \vskip1ex
      The function \texttt{hist()} calculates and plots a histogram, and returns its data \emph{invisibly}.
      \vskip1ex
      The function \texttt{density()} calculates a kernel estimate of the probability density for a sample of data.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # VTI percentage returns
> re_turns <- na.omit(rutils::etf_env$re_turns$VTI)
> conf_level <- 0.1
> va_r <- quantile(re_turns, conf_level)
> c_var <- mean(re_turns[re_turns < va_r])
> # Plot histogram of VTI returns
> histo_gram <- hist(re_turns, col="lightgrey",
+   xlab="returns", ylab="frequency", breaks=100,
+   xlim=c(-0.05, 0.01), freq=FALSE, main="VTI Returns Histogram")
> # Calculate density
> densi_ty <- density(re_turns, adjust=1.5)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/portf_var.png}
      \vspace{-2em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot density
> lines(densi_ty, lwd=3, col="blue")
> # Plot line for VaR
> abline(v=va_r, col="red", lwd=3)
> text(x=va_r, y=20, labels="VaR", lwd=2, srt=90, pos=2)
> # Plot polygon shading for CVaR
> var_max <- -0.06
> rang_e <- (densi_ty$x < va_r) &  (densi_ty$x > var_max)
> polygon(c(var_max, densi_ty$x[rang_e], va_r),
+   c(0, densi_ty$y[rang_e], 0), col=rgb(1, 0, 0,0.5), border=NA)
> text(x=va_r, y=3, labels="CVaR", lwd=2, pos=2)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Value at Risk (\protect\emph{VaR})}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Value at Risk} ($\mathrm{VaR}$) is equal to the quantile of returns corresponding to a given confidence level $\alpha$:
      \begin{displaymath}
        \alpha = \int_{-\infty}^{\mathrm{VaR}(\alpha)} \operatorname{f}(r) \, \mathrm{d}r
      \end{displaymath}
      Where $\operatorname{f}(r)$ is the probability density (distribution) of returns.
      \vskip1ex
      At a high confidence level, the value of $\mathrm{VaR}$ is subject to estimation error, and various numerical methods are used to approximate it.
      \vskip1ex
      The function \texttt{quantile()} calculates the sample quantiles.  It uses interpolation to improve the accuracy.  Information about the different interpolation methods can be found by typing \texttt{?quantile}.
      \vskip1ex
      The function \texttt{VaR()} from package \emph{PerformanceAnalytics} calculates the \emph{Value at Risk} using several different methods.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # VTI percentage returns
> re_turns <- na.omit(rutils::etf_env$re_turns$VTI)
> conf_level <- 0.02
> # Calculate VaR as quantile
> va_r <- quantile(re_turns, conf_level)
> # Or by sorting
> sort_ed <- sort(as.numeric(re_turns))
> in_dex <- round(conf_level*NROW(re_turns))
> va_r <- sort_ed[in_dex]
> # PerformanceAnalytics VaR
> PerformanceAnalytics::VaR(re_turns,
+   p=(1-conf_level), method="historical")
> all.equal(unname(va_r),
+   as.numeric(PerformanceAnalytics::VaR(re_turns,
+   p=(1-conf_level), method="historical")))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Conditional Value at Risk (\protect\emph{CVaR})}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Conditional Value at Risk} ($\mathrm{CVaR}$) is equal to the average of negative returns less than the $\mathrm{VaR}$:
      \begin{displaymath}
        \mathrm{CVaR} = \frac{1}{\alpha} \int_{0}^\alpha \mathrm{VaR}(p) \, \mathrm{d}p
      \end{displaymath}
      The \emph{Conditional Value at Risk} is also called the \emph{Expected Shortfall} (\emph{ES}), or the Expected Tail Loss (\emph{ETL}).
      \vskip1ex
      The function \texttt{ETL()} from package \emph{PerformanceAnalytics} calculates the \emph{Conditional Value at Risk} using several different methods.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate VaR as quantile
> va_r <- quantile(re_turns, conf_level)
> # Calculate CVaR as expected loss
> c_var <- mean(re_turns[re_turns < va_r])
> # Or by sorting
> sort_ed <- sort(as.numeric(re_turns))
> in_dex <- round(conf_level*NROW(re_turns))
> va_r <- sort_ed[in_dex]
> c_var <- mean(sort_ed[1:in_dex])
> # PerformanceAnalytics VaR
> PerformanceAnalytics::ETL(re_turns,
+   p=(1-conf_level), method="historical")
> all.equal(c_var,
+   as.numeric(PerformanceAnalytics::ETL(re_turns,
+   p=(1-conf_level), method="historical")))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk and Return Statistics}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{table.Stats()} from package \emph{PerformanceAnalytics} calculates a data frame of risk and return statistics of the return distributions.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate the risk-return statistics
> risk_ret <-
+   PerformanceAnalytics::table.Stats(rutils::etf_env$re_turns)
> class(risk_ret)
> # Transpose the data frame
> risk_ret <- as.data.frame(t(risk_ret))
> # Add Name column
> risk_ret$Name <- rownames(risk_ret)
> # Add Sharpe ratio column
> risk_ret$Sharpe <- risk_ret$"Arithmetic Mean"/risk_ret$Stdev
> # Sort on Sharpe ratio
> risk_ret <- risk_ret[order(risk_ret$Sharpe, decreasing=TRUE), ]
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}
\begin{tabular}{l|r|r|r}
\hline
  & Sharpe & Skewness & Kurtosis\\
\hline
USMV & 0.056 & -1.018 & 26.18\\
\hline
MTUM & 0.050 & -0.827 & 15.94\\
\hline
IEF & 0.049 & 0.018 & 3.02\\
\hline
QUAL & 0.046 & -0.690 & 18.27\\
\hline
VLUE & 0.041 & -1.191 & 21.31\\
\hline
XLP & 0.031 & -0.082 & 9.62\\
\hline
XLY & 0.028 & -0.384 & 7.86\\
\hline
GLD & 0.026 & -0.327 & 6.21\\
\hline
XLV & 0.025 & 0.076 & 10.81\\
\hline
IWB & 0.024 & -0.409 & 10.71\\
\hline
VTI & 0.024 & -0.399 & 11.63\\
\hline
IVW & 0.024 & -0.316 & 9.50\\
\hline
VYM & 0.024 & -0.708 & 14.67\\
\hline
XLU & 0.024 & 0.025 & 13.22\\
\hline
IWD & 0.024 & -0.505 & 12.86\\
\hline
VTV & 0.024 & -0.685 & 13.80\\
\hline
IVE & 0.023 & -0.492 & 10.16\\
\hline
TLT & 0.023 & -0.014 & 4.64\\
\hline
EEM & 0.022 & 0.000 & 15.09\\
\hline
IWF & 0.022 & -0.714 & 34.62\\
\hline
XLI & 0.022 & -0.400 & 7.78\\
\hline
XLB & 0.019 & -0.422 & 5.61\\
\hline
XLK & 0.018 & 0.088 & 7.58\\
\hline
VNQ & 0.015 & -0.555 & 17.52\\
\hline
VEU & 0.013 & -0.540 & 11.47\\
\hline
XLE & 0.011 & -0.573 & 13.96\\
\hline
XLF & 0.011 & -0.140 & 14.18\\
\hline
SVXY & 0.006 & -17.179 & 563.14\\
\hline
DBC & -0.008 & -0.414 & 3.05\\
\hline
USO & -0.029 & -1.197 & 15.75\\
\hline
VXX & -0.073 & 1.169 & 5.37\\
\hline
\end{tabular}

\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Investor Risk and Return Preferences}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Investors typically prefer larger \emph{odd moments} of the return distribution (mean, skewness), and smaller \emph{even moments} (variance, kurtosis).
      \vskip1ex
      But positive skewness is often associated with lower returns, which can be observed in the \emph{VIX} volatility ETFs, \emph{VXX} and \emph{SVXY}.
      \vskip1ex
      The \emph{VXX} ETF is long the \emph{VIX} index (effectively long an option), so it has positive skewness and small kurtosis, but negative returns (it's short market risk).
      \vskip1ex
      Since the \emph{VXX} is effectively long an option, it pays option premiums so it has negative returns most of the time, with isolated periods of positive returns when markets drop.
      \vskip1ex
      The \emph{SVXY} ETF is short the \emph{VIX} index, so it has negative skewness and large kurtosis, but positive returns (it's long market risk).
      \vskip1ex
      Since the \emph{SVXY} is effectively short an option, it earns option premiums so it has positive returns most of the time, but it suffers sharp losses when markets drop.
    \column{0.5\textwidth}
    \vspace{1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}
\begin{tabular}{l|r|r|r}
\hline
  & Sharpe & Skewness & Kurtosis\\
\hline
VXX & -0.073 & 1.17 & 5.37\\
\hline
SVXY & 0.006 & -17.18 & 563.14\\
\hline
\end{tabular}

\end{knitrout}
      \includegraphics[width=0.45\paperwidth]{figure/vix_vxx_svxy.png}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # dygraph plot of VTI drawdowns
> price_s <- na.omit(rutils::etf_env$price_s[, c("VXX", "SVXY")])
> price_s <- price_s["2017/"]
> col_names <- c("VXX", "SVXY")
> colnames(price_s) <- col_names
> dygraphs::dygraph(price_s, main="Prices of VXX and SVXY") %>%
+   dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
+   dyAxis("y2", label=col_names[2], independentTicks=TRUE) %>%
+   dySeries(name=col_names[1], axis="y", strokeWidth=2, col="blue") %>%
+   dySeries(name=col_names[2], axis="y2", strokeWidth=2, col="green") %>%
+   dyLegend(show="always", width=500)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Skewness and Return Tradeoff}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Similarly to the \emph{VXX} and \emph{SVXY}, for most other ETFs positive skewness is often associated with lower returns.
      \vskip1ex
      Some of the exceptions are bond ETFs (like \emph{IEF}), which have both non-negative skewness and positive returns.
      \vskip1ex
      Another exception are commodity ETFs (like \emph{USO} oil), which have both negative skewness and negative returns.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Remove VIX volatility ETF data
> risk_ret <- risk_ret[-match(c("VXX", "SVXY"), risk_ret$Name), ]
> # Plot scatterplot of Sharpe vs Skewness
> plot(Sharpe ~ Skewness, data=risk_ret,
+      ylim=1.1*range(risk_ret$Sharpe),
+      main="Sharpe vs Skewness")
> # Add labels
> text(x=risk_ret$Skewness, y=risk_ret$Sharpe,
+     labels=risk_ret$Name, pos=3, cex=0.8)
> # Plot scatterplot of Kurtosis vs Skewness
> x11(width=6, height=5)
> par(mar=c(4, 4, 2, 1), oma=c(0, 0, 0, 0))
> plot(Kurtosis ~ Skewness, data=risk_ret,
+      ylim=c(1, max(risk_ret$Kurtosis)),
+      main="Kurtosis vs Skewness")
> # Add labels
> text(x=risk_ret$Skewness, y=risk_ret$Kurtosis,
+     labels=risk_ret$Name, pos=1, cex=0.8)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/etf_skew_sharp.png}
      % \includegraphics[width=0.45\paperwidth]{figure/etf_skew_kurtosis.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Skewness and Return Tradeoff for ETFs and Stocks}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The ETFs or stocks can be sorted on their skewness to create high\_skew and low\_skew cohorts.
      \vskip1ex
      But the high\_skew cohort has better returns than the low\_skew cohort - contrary to the thesis that assets with positive skewness produce lower returns than those with a negative skewness.
      \vskip1ex
      The high and low volatility cohorts have very similar returns, contrary to expectations.  So do the high and low kurtosis cohorts.
    \column{0.5\textwidth}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ### Below is for ETFs
> # Sort on Sharpe ratio
> risk_ret <- risk_ret[order(risk_ret$Skewness, decreasing=TRUE), ]
> # Select high skew and low skew ETFs
> cut_off <- (NROW(risk_ret) %/% 2)
> high_skew <- risk_ret$Name[1:cut_off]
> low_skew <- risk_ret$Name[(cut_off+1):NROW(risk_ret)]
> # Calculate returns and log prices
> re_turns <- rutils::etf_env$re_turns
> re_turns <- zoo::na.locf(re_turns, na.rm=FALSE)
> re_turns[is.na(re_turns)] <- 0
> sum(is.na(re_turns))
> high_skew <- rowMeans(re_turns[, high_skew])
> low_skew <- rowMeans(re_turns[, low_skew])
> weal_th <- cbind(high_skew, low_skew)
> weal_th <- xts::xts(weal_th, index(re_turns))
> weal_th <- cumsum(weal_th)
> # dygraph plot of high skew and low skew ETFs
> col_names <- colnames(weal_th)
> dygraphs::dygraph(weal_th, main="Log Wealth of High and Low Skew ETFs") %>%
+   dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
+   dyAxis("y2", label=col_names[2], independentTicks=TRUE) %>%
+   dySeries(name=col_names[1], axis="y", strokeWidth=2, col="blue") %>%
+   dySeries(name=col_names[2], axis="y2", strokeWidth=2, col="green") %>%
+   dyLegend(show="always", width=500)
> 
> ### Below is for S&P500 constituent stocks
> # calc_mom() calculates the moments of returns
> calc_mom <- function(re_turns, mo_ment=3) {
+   re_turns <- na.omit(re_turns)
+   sum(((re_turns - mean(re_turns))/sd(re_turns))^mo_ment)/NROW(re_turns)
+ }  # end calc_mom
> # Calculate skew and kurtosis of VTI returns
> calc_mom(re_turns, mo_ment=3)
> calc_mom(re_turns, mo_ment=4)
> # Load the S&P500 constituent stock returns
> load(file="C:/Develop/lecture_slides/data/sp500_returns.RData")
> dim(re_turns)
> sum(is.na(re_turns))
> # re_turns <- re_turns["2000/"]
> skew_s <- sapply(re_turns, calc_mom, mo_ment=3)
> # skew_s <- sapply(re_turns, calc_mom, mo_ment=4)
> # skew_s <- sapply(re_turns, sd, na.rm=TRUE)
> skew_s <- sort(skew_s)
> name_s <- names(skew_s)
> n_rows <- NROW(name_s)
> # Select high skew and low skew ETFs
> cut_off <- (n_rows %/% 2)
> low_skew <- name_s[1:cut_off]
> high_skew <- name_s[(cut_off+1):n_rows]
> 
> # low_skew <- name_s[1:50]
> # high_skew <- name_s[(n_rows-51):n_rows]
> # Calculate returns and log prices
> low_skew <- rowMeans(re_turns[, low_skew], na.rm=TRUE)
> low_skew[1] <- 0
> high_skew <- rowMeans(re_turns[, high_skew], na.rm=TRUE)
> high_skew[1] <- 0
> weal_th <- cbind(high_skew, low_skew)
> weal_th <- xts::xts(weal_th, index(re_turns))
> weal_th <- cumsum(weal_th)
> # dygraph plot of high skew and low skew ETFs
> col_names <- colnames(weal_th)
> dygraphs::dygraph(weal_th, main="Log Wealth of High and Low Skew Stocks") %>%
+   dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
+   dyAxis("y2", label=col_names[2], independentTicks=TRUE) %>%
+   dySeries(name=col_names[1], axis="y", strokeWidth=2, col="blue") %>%
+   dySeries(name=col_names[2], axis="y2", strokeWidth=2, col="green") %>%
+   dyLegend(show="always", width=500)
> 
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk-adjusted Return Measures}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Sharpe} ratio measures the excess returns per unit of risk, and is equal to the excess returns (over a risk-free return $r_f$) divided by the standard deviation of the returns:
      \begin{displaymath}
        S_{r}=\frac{E[r-r_f]}{\sigma}
      \end{displaymath}
      The \emph{Sortino} ratio is equal to the excess returns divided by the \emph{downside deviation} $\sigma_{d}$ (standard deviation of returns that are less than a target rate of return $r_t$):
      \begin{displaymath}
        S_{r}=\frac{E[r-r_t]}{\sigma_{d}}
      \end{displaymath}
      The \emph{Calmar} ratio is equal to the excess returns divided by the maximum drawdown of the returns:
      \begin{displaymath}
        C_{r}=\frac{E[r-r_f]}{DD}
      \end{displaymath}
      \vskip1ex
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> re_turns <- rutils::etf_env$re_turns[, c("VTI", "IEF")]
> re_turns <- na.omit(re_turns)
> # Calculate the Sharpe ratio
> PerformanceAnalytics::SharpeRatio(re_turns)
> # Calculate the Sortino ratio
> PerformanceAnalytics::SortinoRatio(re_turns)
> # Calculate the Calmar ratio
> PerformanceAnalytics::CalmarRatio(re_turns)
> # Calculate the returns statistics
> tail(PerformanceAnalytics::table.Stats(re_turns), 4)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Feature Engineering}


%%%%%%%%%%%%%%%
\subsection{draft: Feature Engineering}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Feature engineering derives predictive data elements (features) from a large input data set.
      \vskip1ex
      Feature engineering reduces the size of the input data set to a smaller set of features with the highest predictive power.
      \vskip1ex
      The predictive features are then used as inputs into machine learning models.
      \vskip1ex
      \emph{Out-of-sample} features only depend on past data, while \emph{in-sample} features depend both on past and future data.
      \vskip1ex
      A \emph{trailing} data filter is an example of an \emph{out-of-sample} feature.
      \vskip1ex
      A \emph{centered} data filter is an example of an \emph{in-sample} feature.
      \vskip1ex
      Out-of-sample features are used in forecasting and scrubbing real-time (live) data.
      \vskip1ex
      In-sample features are used in data labeling and scrubbing historical data.
      \vskip1ex
      \emph{Principal Component Analysis} (\emph{PCA}) is a \emph{dimension reduction} technique used in multivariate feature engineering.
      \vskip1ex
      Feature engineering can be developed using \emph{domain knowledge} and analytical techniques.
      \vskip1ex
      Some features indicate trend, for example the moving average asset returns.
      \vskip1ex
      Other features indicate turning points when prices are too too rich or too cheap.
      \vskip1ex
      Features indicating turning points are often quasi-stationary time series which oscillate around zero and correspond to the extreme tops and bottoms of prices.
      \vskip1ex
      the process of using of the data to create features that make machine learning algorithms work. 
      If feature engineering is done correctly, it increases the predictive power of machine learning algorithms by creating features from raw data that help facilitate the machine learning process.
      \vskip1ex
      The \emph{data table} brackets \texttt{"[]"} operator can accept three arguments: \texttt{[i, j, by]}
      \begin{itemize}
        \item \texttt{i}: the row index to select,
        \item \texttt{j}: a list of columns or functions on columns,
        \item \texttt{by}: the columns of factors to aggregate over.
      \end{itemize}
      The \emph{data table} columns can be \emph{aggregated} over categories (factors) defined by one or more columns passed to the \texttt{"by"} argument.
      \vskip1ex
      The \texttt{"keyby"} argument is similar to \texttt{"by"}, but it sorts the output according to the categories used to group by.
      \vskip1ex
      Multiple \emph{data table} columns can be referenced by passing a list of names.
      \vskip1ex
      The dot \texttt{.()} operator is equivalent to the list function \texttt{list()}.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Number of flights from each airport
> data_table[, .N, by=origin]
> # Same, but add names to output
> data_table[, .(flights=.N), by=.(airport=origin)]
> # Number of AA flights from each airport
> data_table[carrier=="AA", .(flights=.N),
+      by=.(airport=origin)]
> # Number of flights from each airport and airline
> data_table[, .(flights=.N),
+      by=.(airport=origin, airline=carrier)]
> # Average aircraft_delay
> data_table[, mean(aircraft_delay)]
> # Average aircraft_delay from JFK
> data_table[origin=="JFK", mean(aircraft_delay)]
> # Average aircraft_delay from each airport
> data_table[, .(delay=mean(aircraft_delay)),
+      by=.(airport=origin)]
> # Average and max delays from each airport and month
> data_table[, .(mean_delay=mean(aircraft_delay), max_delay=max(aircraft_delay)),
+      by=.(airport=origin, month=month)]
> # Average and max delays from each airport and month
> data_table[, .(mean_delay=mean(aircraft_delay), max_delay=max(aircraft_delay)),
+      keyby=.(airport=origin, month=month)]
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Convolution Filtering of Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{filter()} applies a trailing linear filter to time series, vectors, and matrices, and returns a time series of class \texttt{"ts"}.
      \vskip1ex
      The function \texttt{filter()} with the argument \texttt{method="convolution"} calculates the \emph{convolution} of the vector $r_i$ with the filter $\varphi_i$:
      \begin{displaymath}
        f_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p}
      \end{displaymath}
      Where $f_i$ is the filtered output vector, and $\varphi_i$ are the filter coefficients.
      \vskip1ex
      \texttt{filter()} is very fast because it calculates the filter by calling compiled \texttt{C++} functions.
      \vskip1ex
      \texttt{filter()} with \texttt{method="convolution"} calls the function \texttt{stats:::C\_cfilter()} to calculate the \emph{convolution}.
      \vskip1ex
      Filtering can be performed even faster by directly calling the compiled function \texttt{stats:::C\_cfilter()}.
      \vskip1ex
      The function \texttt{roll::roll\_sum()} calculates the \emph{weighted} rolling sum (convolution) even faster than \texttt{stats:::C\_cfilter()}.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Extract time series of VTI log prices
> clos_e <- log(na.omit(rutils::etf_env$price_s$VTI))
> # Inspect the R code of the function filter()
> filter
> # Calculate EWMA weight_s
> look_back <- 21
> weight_s <- exp(-0.1*1:look_back)
> weight_s <- weight_s/sum(weight_s)
> # Calculate convolution using filter()
> filter_ed <- filter(clos_e, filter=weight_s,
+               method="convolution", sides=1)
> # filter() returns time series of class "ts"
> class(filter_ed)
> # Get information about C_cfilter()
> getAnywhere(C_cfilter)
> # Filter using C_cfilter() over past values (sides=1).
> filter_fast <- .Call(stats:::C_cfilter, clos_e, filter=weight_s,
+                sides=1, circular=FALSE)
> all.equal(as.numeric(filter_ed), filter_fast, check.attributes=FALSE)
> # Calculate EWMA prices using roll::roll_sum()
> weights_rev <- rev(weight_s)
> roll_ed <- roll::roll_sum(clos_e, width=look_back, weights=weights_rev, min_obs=1)
> all.equal(filter_fast[-(1:look_back)], as.numeric(roll_ed)[-(1:look_back)])
> # Benchmark speed of rolling calculations
> library(microbenchmark)
> summary(microbenchmark(
+   filter=filter(clos_e, filter=weight_s, method="convolution", sides=1),
+   filter_fast=.Call(stats:::C_cfilter, clos_e, filter=weight_s, sides=1, circular=FALSE),
+   roll=roll::roll_sum(clos_e, width=look_back, weights=weights_rev)
+   ), times=10)[, c(1, 4, 5)]
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Recursive Filtering of Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{filter()} with \texttt{method="recursive"} calls the function \texttt{stats:::C\_rfilter()} to calculate the \emph{recursive filter} as follows:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i
      \end{displaymath}
      Where $r_i$ is the filtered output vector, $\varphi_i$ are the filter coefficients, and $\xi_i$ are random \emph{innovations}.
      \vskip1ex
      The \emph{recursive} filter describes an \emph{AR(p)} process, which is a special case of an \emph{ARIMA} process.
      \vskip1ex
      The function \texttt{HighFreq::sim\_arima()} is very fast because it's written using the \texttt{C++} \emph{Armadillo} numerical library.
      \vskip1ex
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Simulate AR process using filter()
> n_rows <- NROW(clos_e)
> # Calculate ARIMA coefficients and innovations
> co_eff <- weight_s/4
> n_coeff <- NROW(co_eff)
> in_nov <- rnorm(n_rows)
> ari_ma <- filter(x=in_nov, filter=co_eff, method="recursive")
> # Get information about C_rfilter()
> getAnywhere(C_rfilter)
> # Filter using C_rfilter() compiled C++ function directly
> arima_fast <- .Call(stats:::C_rfilter, in_nov, co_eff,
+               double(n_coeff + n_rows))
> all.equal(as.numeric(ari_ma), arima_fast[-(1:n_coeff)], 
+     check.attributes=FALSE)
> # Filter using C++ code
> arima_fastest <- HighFreq::sim_arima(in_nov, rev(co_eff))
> all.equal(arima_fast[-(1:n_coeff)], drop(arima_fastest))
> # Benchmark speed of the three methods
> summary(microbenchmark(
+   filter=filter(x=in_nov, filter=co_eff, method="recursive"),
+   filter_fast=.Call(stats:::C_rfilter, in_nov, co_eff, double(n_coeff + n_rows)),
+   Rcpp=HighFreq::sim_arima(in_nov, rev(co_eff))
+   ), times=10)[, c(1, 4, 5)]
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Data Smoothing and The Bias-Variance Tradeoff}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Filtering through an averaging filter produces data \emph{smoothing}.
      \vskip1ex
      Smoothing real-time data with a trailing filter reduces its \emph{variance} but it increases its \emph{bias} because it introduces a time lag.
      \vskip1ex
      Smoothing historical data with a centered filter reduces its \emph{variance} but it introduces \emph{data snooping}.
      \vskip1ex
      In engineering, smoothing is called a \emph{low-pass filter}, since it eliminates high frequency signals, and it passes through low frequency signals.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate trailing EWMA prices using roll::roll_sum()
> look_back <- 21
> weight_s <- exp(-0.1*1:look_back)
> weight_s <- weight_s/sum(weight_s)
> weights_rev <- rev(weight_s)
> filter_ed <- roll::roll_sum(clos_e, width=NROW(weight_s), weights=weights_rev)
> # Copy warmup period
> filter_ed[1:look_back] <- clos_e[1:look_back]
> # Combine prices with smoothed prices
> price_s <- cbind(clos_e, filter_ed)
> colnames(price_s)[2] <- "VTI Smooth"
> # Calculate standard deviations of returns
> sapply(rutils::diff_it(price_s), sd)
> # Plot dygraph
> dygraphs::dygraph(price_s["2009"], main="VTI Prices and Trailing Smoothed Prices") %>%
+   dyOptions(colors=c("blue", "red"), strokeWidth=2)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_smooth.png}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate centered EWMA prices using roll::roll_sum()
> weight_s <- c(weights_rev, weight_s[-1])
> weight_s <- weight_s/sum(weight_s)
> filter_ed <- roll::roll_sum(clos_e, width=NROW(weight_s), weights=weight_s, online=FALSE)
> # Copy warmup period
> filter_ed[1:(2*look_back)] <- clos_e[1:(2*look_back)]
> # Center the data
> filter_ed <- rutils::lag_it(filter_ed, -(look_back-1), pad_zeros=FALSE)
> # Combine prices with smoothed prices
> price_s <- cbind(clos_e, filter_ed)
> colnames(price_s)[2] <- "VTI Smooth"
> # Calculate standard deviations of returns
> sapply(rutils::diff_it(price_s), sd)
> # Plot dygraph
> dygraphs::dygraph(price_s["2009"], main="VTI Prices and Centered Smoothed Prices") %>%
+   dyOptions(colors=c("blue", "red"), strokeWidth=2)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Plotting Filtered Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(rutils)  # Load package rutils
> library(ggplot2)  # Load ggplot2
> library(gridExtra)  # Load gridExtra
> # Coerce to zoo and merge the time series
> filter_ed <- cbind(clos_e, filter_ed)
> colnames(filter_ed) <- c("VTI", "VTI filtered")
> # Plot ggplot2
> autoplot(filter_ed["2008/2010"],
+     main="Filtered VTI", facets=NULL) +  # end autoplot
+ xlab("") + ylab("") +
+ theme(  # Modify plot theme
+     legend.position=c(0.1, 0.5),
+     plot.title=element_text(vjust=-2.0),
+     plot.margin=unit(c(-0.5, 0.0, -0.5, 0.0), "cm"),
+     plot.background=element_blank(),
+     axis.text.y=element_blank()
+     )  # end theme
> # end ggplot2
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/ggplot_vti.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelations of Smoothed Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Smoothing a time series of prices produces autocorrelations of their returns.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Open plot window
> x11(width=6, height=7)
> # Set plot parameters
> par(oma=c(1, 1, 0, 1), mar=c(1, 1, 1, 1), mgp=c(0, 0.5, 0),
+     cex.lab=0.8, cex.axis=0.8, cex.main=0.8, cex.sub=0.5)
> # Set two plot panels
> par(mfrow=c(2,1))
> # Plot ACF of VTI returns
> rutils::plot_acf(re_turns[, 1], lag=10, xlab="")
> title(main="ACF of VTI Returns", line=-1)
> # Plot ACF of smoothed VTI returns
> rutils::plot_acf(re_turns[, 2], lag=10, xlab="")
> title(main="ACF of Smoothed VTI Returns", line=-1)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_acf.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: \protect\emph{RSI} Price Technical Indicator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Relative Strength Index} (\emph{RSI}) is defined as the weighted average of prices over a rolling interval:
      \begin{displaymath}
        p^{RSI}_i = (1-\exp(-\lambda)) \sum_{j=0}^{\infty} \exp(-\lambda j) p_{i-j}
      \end{displaymath}
      Where the decay parameter $\lambda$ determines the rate of decay of the \emph{RSI} weights, with larger values of $\lambda$ producing faster decay, giving more weight to recent prices, and vice versa,
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Get close prices and calculate close-to-close returns
> # clos_e <- quantmod::Cl(rutils::etf_env$VTI)
> clos_e <- quantmod::Cl(HighFreq::SPY)
> colnames(clos_e) <- rutils::get_name(colnames(clos_e))
> re_turns <- TTR::ROC(clos_e)
> re_turns[1] <- 0
> # Calculate the RSI indicator
> r_si <- TTR::RSI(clos_e, 2)
> # Calculate the long (up) and short (dn) signals
> sig_up <- ifelse(r_si < 10, 1, 0)
> sig_dn <- ifelse(r_si > 90, -1, 0)
> # Lag signals by one period
> sig_up <- rutils::lag_it(sig_up, 1)
> sig_dn <- rutils::lag_it(sig_dn, 1)
> # Replace NA signals with zero position
> sig_up[is.na(sig_up)] <- 0
> sig_dn[is.na(sig_dn)] <- 0
> # Combine up and down signals into one
> sig_nals <- sig_up + sig_dn
> # Calculate cumulative returns
> eq_up <- exp(cumsum(sig_up*re_turns))
> eq_dn <- exp(cumsum(-1*sig_dn*re_turns))
> eq_all <- exp(cumsum(sig_nals*re_turns))
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/rsi_indic.png}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot daily cumulative returns in panels
> end_p <- endpoints(re_turns, on="days")
> plot.zoo(cbind(eq_all, eq_up, eq_dn)[end_p], lwd=c(2, 2, 2),
+   ylab=c("Total","Long","Short"), col=c("red","green","blue"),
+   main=paste("RSI(2) strategy for", colnames(clos_e), "from",
+        format(start(re_turns), "%B %Y"), "to",
+        format(end(re_turns), "%B %Y")))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{EWMA} Price Technical Indicator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Exponentially Weighted Moving Average Price} (\emph{EWMA}) is defined as the weighted average of prices over a rolling interval:
      {\scriptsize
      \begin{displaymath}
        p^{EWMA}_i = (1-\exp(-\lambda)) \sum_{j=0}^{\infty} \exp(-\lambda j) p_{i-j}
      \end{displaymath}
      }
      Where the decay parameter $\lambda$ determines the rate of decay of the \emph{EWMA} weights, with larger values of $\lambda$ producing faster decay, giving more weight to recent prices, and vice versa.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Extract log VTI prices
> clos_e <- log(na.omit(rutils::etf_env$price_s$VTI))
> n_rows <- NROW(clos_e)
> # Calculate EWMA weights
> look_back <- 21
> lamb_da <- 0.1
> weight_s <- exp(lamb_da*1:look_back)
> weight_s <- weight_s/sum(weight_s)
> # Calculate EWMA prices
> ew_ma <- roll::roll_sum(clos_e, width=look_back, weights=weight_s, min_obs=1)
> # Copy over NA values
> ew_ma <- zoo::na.locf(ew_ma, fromLast=TRUE)
> price_s <- cbind(clos_e, ew_ma)
> colnames(price_s) <- c("VTI", "VTI EWMA")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_ewma.png}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Dygraphs plot with custom line colors
> col_ors <- c("blue", "red")
> dygraphs::dygraph(price_s["2009"], main="VTI EWMA Prices") %>%
+   dyOptions(colors=col_ors, strokeWidth=2)
> # Plot EWMA prices with custom line colors
> x11(width=6, height=5)
> plot_theme <- chart_theme()
> plot_theme$col$line.col <- col_ors
> quantmod::chart_Series(price_s["2009"], theme=plot_theme,
+        lwd=2, name="VTI EWMA Prices")
> legend("bottomright", legend=colnames(price_s),
+  inset=0.1, bg="white", lty=1, lwd=6, cex=0.8,
+  col=plot_theme$col$line.col, bty="n")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Volume-Weighted Average Price Indicator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The Volume-Weighted Average Price (\emph{VWAP}) is defined as the sum of prices multiplied by trading volumes, divided by the sum of volumes:
      \begin{displaymath}
        p^{VWAP}_i = \frac{\sum_{j=0}^{n} v_j p_{i-j}}{\sum_{j=0}^{n} v_j}
      \end{displaymath}
      The \emph{VWAP} is often used as a technical indicator in trend following strategies.
      \vskip1ex
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate log OHLC prices and volumes
> sym_bol <- "VTI"
> oh_lc <- rutils::etf_env$VTI
> n_rows <- NROW(oh_lc)
> clos_e <- log(quantmod::Cl(oh_lc))
> vol_ume <- quantmod::Vo(oh_lc)
> # Calculate the VWAP prices
> look_back <- 21
> v_wap <- roll::roll_sum(clos_e*vol_ume, width=look_back, min_obs=1)
> volume_roll <- roll::roll_sum(vol_ume, width=look_back, min_obs=1)
> v_wap <- v_wap/volume_roll
> v_wap <- zoo::na.locf(v_wap, fromLast=TRUE)
> price_s <- cbind(clos_e, v_wap)
> colnames(price_s) <- c(sym_bol, paste(sym_bol, "VWAP"))
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_vwap.png}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Dygraphs plot with custom line colors
> col_ors <- c("blue", "red")
> dygraphs::dygraph(price_s["2009"], main="VTI VWAP Prices") %>%
+   dyOptions(colors=col_ors, strokeWidth=2)
> # Plot VWAP prices with custom line colors
> x11(width=6, height=5)
> plot_theme <- chart_theme()
> plot_theme$col$line.col <- col_ors
> quantmod::chart_Series(price_s["2009"], theme=plot_theme,
+        lwd=2, name="VTI VWAP Prices")
> legend("bottomright", legend=colnames(price_s),
+  inset=0.1, bg="white", lty=1, lwd=6, cex=0.8,
+  col=plot_theme$col$line.col, bty="n")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Smooth Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Asset returns are calculated by filtering prices through a \emph{differencing} filter.
      \vskip1ex
      The simplest \emph{differencing} filter is the filter with coefficients $(1, -1)$: $r_i = p_i - p_{i-1}$.
      \vskip1ex
      Differencing is a \emph{high-pass filter}, since it eliminates low frequency signals, and it passes through high frequency signals.
      \vskip1ex
      An alternative measure of returns is the difference between two moving averages of prices:
      $r_i = p^{fast}_i - p^{slow}_i$
      \vskip1ex
      The difference between moving averages is a \emph{mid-pass filter}, since it eliminates both high and low frequency signals, and it passes through medium frequency signals.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate two EWMA prices
> look_back <- 21
> lamb_da <- 0.1
> weight_s <- exp(lamb_da*1:look_back)
> weight_s <- weight_s/sum(weight_s)
> ewma_fast <- roll::roll_sum(clos_e, width=look_back, weights=weight_s, min_obs=1)
> lamb_da <- 0.05
> weight_s <- exp(lamb_da*1:look_back)
> weight_s <- weight_s/sum(weight_s)
> ewma_slow <- roll::roll_sum(clos_e, width=look_back, weights=weight_s, min_obs=1)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_returns.png}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate VTI returns
> re_turns <- (ewma_fast - ewma_slow)
> price_s <- cbind(clos_e, re_turns)
> colnames(price_s) <- c(sym_bol, paste(sym_bol, "Returns"))
> # Plot dygraph of VTI Returns
> col_names <- colnames(price_s)
> dygraphs::dygraph(price_s["2009"], main=paste(sym_bol, "EWMA Returns")) %>%
+   dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
+   dyAxis("y2", label=col_names[2], independentTicks=TRUE) %>%
+   dySeries(name=col_names[1], axis="y", label=col_names[1], strokeWidth=2, col="blue") %>%
+   dySeries(name=col_names[2], axis="y2", label=col_names[2], strokeWidth=2, col="red")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fractional Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The lag operator $L$ applies a lag (time shift) to a time series: $L(p_i) = p_{i-1}$.
      \vskip1ex
      The simple returns can then be expressed as equal to the returns operator $(1 - L)$ applied to the prices: $r_i = (1 - L) p_i$.
      \vskip1ex
      The simple returns can be generalized to the fractional returns by raising the returns operator to some power $\delta < 1$:
      {\scriptsize
      \begin{multline*}
        r_i = (1 - L)^\delta p_i = \\
        p_i - \delta L p_i + \frac{\delta(\delta-1)}{2!} L^2 p_i - \frac{\delta(\delta-1)(\delta-2)}{3!} L^3 p_i + \cdots = \\
        p_i - \delta p_{i-1} + \frac{\delta(\delta-1)}{2!} p_{i-2} - \frac{\delta(\delta-1)(\delta-2)}{3!} p_{i-3} + \cdots
      \end{multline*}
      }
      The fractional returns provide a tradeoff between simple returns (which are range-bound but with no memory) and prices (which have memory but are not range-bound). 
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_fracret.png}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate fractional weights
> del_ta <- 0.1
> weight_s <- (del_ta - 0:(look_back-2)) / 1:(look_back-1)
> weight_s <- (-1)^(1:(look_back-1))*cumprod(weight_s)
> weight_s <- c(1, weight_s)
> weight_s <- (weight_s - mean(weight_s))
> weight_s <- rev(weight_s)
> # Calculate fractional VTI returns
> re_turns <- roll::roll_sum(clos_e, width=look_back, weights=weight_s, min_obs=1, online=FALSE)
> price_s <- cbind(clos_e, re_turns)
> colnames(price_s) <- c(sym_bol, paste(sym_bol, "Returns"))
> # Plot dygraph of VTI Returns
> col_names <- colnames(price_s)
> dygraphs::dygraph(price_s["2009"], main=paste(sym_bol, "Fractional Returns")) %>%
+   dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
+   dyAxis("y2", label=col_names[2], independentTicks=TRUE) %>%
+   dySeries(name=col_names[1], axis="y", label=col_names[1], strokeWidth=2, col="blue") %>%
+   dySeries(name=col_names[2], axis="y2", label=col_names[2], strokeWidth=2, col="red")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Augmented Dickey-Fuller Test for Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The cumulative sum of a given process is called its \emph{integrated} process.
      \vskip1ex
      For example, asset prices follow an \emph{integrated} process with respect to asset returns: $p_n = {\sum_{i=1}^n r_i}$.
      \vskip1ex
      Integrated processes typically have a \emph{unit root} (they have unlimited range), even if their underlying difference process does not have a \emph{unit root} (has limited range).
      \vskip1ex
      Asset returns don't have a \emph{unit root} (they have limited range) while prices have a \emph{unit root} (they have unlimited range).
      \vskip1ex
      The \emph{Augmented Dickey-Fuller} \emph{ADF} test is designed to test the \emph{null hypothesis} that a time series has a \emph{unit root}.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate VTI log returns
> clos_e <- log(quantmod::Cl(rutils::etf_env$VTI))
> re_turns <- rutils::diff_it(clos_e)
> # Perform ADF test for prices
> tseries::adf.test(clos_e)
> # Perform ADF test for returns
> tseries::adf.test(re_turns)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Augmented Dickey-Fuller Test for Fractional Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The fractional returns for exponent values close to zero $\delta \approx 0$ resemble the asset price, while for values close to one $\delta \approx 1$ they resemble the standard returns.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate fractional VTI returns
> delta_s <- 0.1*c(1, 3, 5, 7, 9)
> re_turns <- lapply(delta_s, function(del_ta) {
+   weight_s <- (del_ta - 0:(look_back-2)) / 1:(look_back-1)
+   weight_s <- c(1, (-1)^(1:(look_back-1))*cumprod(weight_s))
+   weight_s <- rev(weight_s - mean(weight_s))
+   roll::roll_sum(clos_e, width=look_back, weights=weight_s, min_obs=1, online=FALSE)
+ })  # end lapply
> re_turns <- do.call(cbind, re_turns)
> re_turns <- cbind(clos_e, re_turns)
> colnames(re_turns) <- c("VTI", paste0("frac_", delta_s))
> # Calculate ADF test statistics
> adf_stats <- sapply(re_turns, function(x)
+   suppressWarnings(tseries::adf.test(x)$statistic)
+ )  # end sapply
> names(adf_stats) <- colnames(re_turns)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_fracrets.png}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot dygraph of fractional VTI returns
> color_s <- colorRampPalette(c("blue", "red"))(NCOL(re_turns))
> col_names <- colnames(re_turns)
> dy_graph <- dygraphs::dygraph(re_turns["2019"], main="Fractional Returns") %>%
+   dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
+   dySeries(name=col_names[1], axis="y", label=col_names[1], strokeWidth=2, col=color_s[1])
> for (i in 2:NROW(col_names))
+   dy_graph <- dy_graph %>%
+   dyAxis("y2", label=col_names[i], independentTicks=TRUE) %>%
+   dySeries(name=col_names[i], axis="y2", label=col_names[i], strokeWidth=2, col=color_s[i])
> dy_graph <- dy_graph %>% dyLegend(width=500)
> dy_graph
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Trading Volume Z-Scores}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The trailing \emph{volume z-score} is equal to the volume $v_i$ minus the trailing average volumes $\bar{v_i}$ divided by the volatility of the volumes $\sigma_i$:
      {\scriptsize
      \begin{displaymath}
        z_i = \frac{v_i - \bar{v_i}}{\sigma_i}
      \end{displaymath}
      }
      Trading volumes are typically higher when prices drop and they are also positively correlated with the return volatility.
      \vskip1ex
      The \emph{volume z-scores} are positively skewed because returns are negatively skewed.
      \vskip1ex
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate volume z-scores
> vol_ume <- quantmod::Vo(rutils::etf_env$VTI)
> look_back <- 21
> volume_mean <- roll::roll_mean(vol_ume, width=look_back, min_obs=1)
> volume_sd <- roll::roll_sd(rutils::diff_it(vol_ume), width=look_back, min_obs=1)
> volume_scores <- (vol_ume - volume_mean)/volume_sd
> # Plot histogram of volume z-scores
> x11(width=6, height=5)
> hist(volume_scores, breaks=1e2)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_volume_zscores.png}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot dygraph of volume z-scores of VTI prices
> price_s <- cbind(clos_e, volume_scores)
> colnames(price_s) <- c("VTI", "Z-scores")
> col_names <- colnames(price_s)
> dygraphs::dygraph(price_s["2009"], main="VTI Volume Z-Scores") %>%
+   dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
+   dyAxis("y2", label=col_names[2], independentTicks=TRUE) %>%
+   dySeries(name=col_names[1], axis="y", label=col_names[1], strokeWidth=2, col="blue") %>%
+   dySeries(name=col_names[2], axis="y2", label=col_names[2], strokeWidth=2, col="red")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Volatility Z-Scores}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The difference between high and low prices is a proxy for the spot volatility in a bar of data.
      \vskip1ex
      The \emph{volatility z-score} is equal to the spot volatility $v_i$ minus the trailing average volatility $\bar{v_i}$ divided by the standard deviation of the volatility $\sigma_i$:
      {\scriptsize
      \begin{displaymath}
        z_i = \frac{v_i - \bar{v_i}}{\sigma_i}
      \end{displaymath}
      }
      Volatility is typically higher when prices drop and it's also positively correlated with the trading volumes.
      \vskip1ex
      The \emph{volatility z-scores} are positively skewed because returns are negatively skewed.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Extract VTI log OHLC prices
> oh_lc <- log(rutils::etf_env$VTI)
> # Calculate volatility z-scores
> vol_at <- quantmod::Hi(oh_lc)-quantmod::Lo(oh_lc)
> look_back <- 21
> volat_mean <- roll::roll_mean(vol_at, width=look_back, min_obs=1)
> volat_sd <- roll::roll_sd(rutils::diff_it(vol_at), width=look_back, min_obs=1)
> volat_scores <- ifelse(is.na(volat_sd), 0, (vol_at - volat_mean)/volat_sd)
> # Plot histogram of volatility z-scores
> x11(width=6, height=5)
> hist(volat_scores, breaks=1e2)
> # Plot scatterplot of volume and volatility z-scores
> plot(as.numeric(volat_scores), as.numeric(volume_scores),
+      xlab="volatility z-score", ylab="volume z-score")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_volat_zscores.png}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot dygraph of VTI volatility z-scores
> clos_e <- quantmod::Cl(oh_lc)
> price_s <- cbind(clos_e, volat_scores)
> colnames(price_s) <- c("VTI", "Z-scores")
> col_names <- colnames(price_s)
> dygraphs::dygraph(price_s["2009"], main="VTI Volatility Z-Scores") %>%
+   dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
+   dyAxis("y2", label=col_names[2], independentTicks=TRUE) %>%
+   dySeries(name=col_names[1], axis="y", label=col_names[1], strokeWidth=2, col="blue") %>%
+   dySeries(name=col_names[2], axis="y2", label=col_names[2], strokeWidth=2, col="red")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Centered Price Z-scores}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An extreme local price is a price which differs significantly from neighboring prices.
      \vskip1ex
      Extreme prices can be identified in-sample using the centered \emph{price z-score} equal to the price difference with neighboring prices divided by the volatility of returns $\sigma_i$:
      {\scriptsize
      \begin{displaymath}
        z_i = \frac{2 p_i - p_{i-k} - p_{i+k}}{\sigma_i}
      \end{displaymath}
      }
      Where $p_{i-k}$ and $p_{i+k}$ are the lagged and advanced prices.
      \vskip1ex
      The lag parameter $k$ determines the scale of the extreme local prices, with smaller $k$ producing larger z-scores for more local price extremes.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate the centered volatility
> look_back <- 21
> half_back <- look_back %/% 2
> vol_at <- roll::roll_sd(re_turns, width=look_back, min_obs=1)
> vol_at <- rutils::lag_it(vol_at, lagg=(-half_back))
> # Calculate the z-scores of prices
> price_scores <- (2*clos_e - 
+   rutils::lag_it(clos_e, half_back, pad_zeros=FALSE) - 
+   rutils::lag_it(clos_e, -half_back, pad_zeros=FALSE))
> price_scores <- ifelse(vol_at > 0, price_scores/vol_at, 0)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_price_zscores.png}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot dygraph of z-scores of VTI prices
> price_s <- cbind(clos_e, price_scores)
> colnames(price_s) <- c("VTI", "Z-scores")
> col_names <- colnames(price_s)
> dygraphs::dygraph(price_s["2009"], main="VTI Price Z-Scores") %>%
+   dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
+   dyAxis("y2", label=col_names[2], independentTicks=TRUE) %>%
+   dySeries(name=col_names[1], axis="y", label=col_names[1], strokeWidth=2, col="blue") %>%
+   dySeries(name=col_names[2], axis="y2", label=col_names[2], strokeWidth=2, col="red")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Labeling the Tops and Bottoms of Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The local tops and bottoms of prices can be labeled approximately in-sample using the z-scores of prices and threshold values.
      \vskip1ex
      The labeled data can be used as a response or target variable in machine learning classifier models.
      \vskip1ex
      But it's not feasible to classify the prices out-of-sample exactly according to their in-sample labels.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate thresholds for labeling tops and bottoms
> threshold_s <- quantile(price_scores, c(0.1, 0.9))
> # Calculate the vectors of tops and bottoms
> top_s <- (price_scores > threshold_s[2])
> colnames(top_s) <- "tops"
> bottom_s <- (price_scores < threshold_s[1])
> colnames(bottom_s) <- "bottoms"
> # Backtest in-sample VTI strategy
> position_s <- rep(NA_integer_, NROW(re_turns))
> position_s[1] <- 0
> position_s[top_s] <- (-1)
> position_s[bottom_s] <- 1
> position_s <- zoo::na.locf(position_s)
> position_s <- rutils::lag_it(position_s)
> pnl_s <- cumsum(re_turns*position_s)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_labels.png}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot dygraph of in-sample VTI strategy
> price_s <- cbind(clos_e, pnl_s)
> colnames(price_s) <- c("VTI", "Strategy")
> col_names <- colnames(price_s)
> dygraphs::dygraph(price_s, main="VTI Strategy Using In-sample Labels") %>%
+   dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
+   dyAxis("y2", label=col_names[2], independentTicks=TRUE) %>%
+   dySeries(name=col_names[1], axis="y", label=col_names[1], strokeWidth=2, col="blue") %>%
+   dySeries(name=col_names[2], axis="y2", label=col_names[2], strokeWidth=2, col="red")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Regression Z-Scores}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The trailing \emph{z-score} $z_i$ of a price $p_i$ can be defined as the \emph{standardized residual} of the linear regression with respect to time $t_i$ or some other variable:
      {\scriptsize
      \begin{displaymath}
        z_i = \frac{p_i - (\alpha + \beta t_i)}{\sigma_i}
      \end{displaymath}
      }
      Where $\alpha$ and $\beta$ are the \emph{regression coefficients}, and $\sigma_i$ is the standard deviation of the residuals.
      \vskip1ex
      The regression \emph{z-scores} can be used as rich or cheap indicators, either relative to past prices, or relative to prices in a stock pair.
      \vskip1ex
      The regression residuals must be calculated in a loop, so it's much faster to calculate them using functions written in \texttt{C++} code.
      \vskip1ex
      The function \texttt{HighFreq::roll\_zscores()} calculates the residuals of a rolling regression.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate trailing price z-scores
> date_s <- matrix(as.numeric(zoo::index(clos_e)))
> look_back <- 21
> price_scores <- drop(HighFreq::roll_zscores(res_ponse=clos_e, de_sign=date_s, look_back=look_back))
> price_scores[1:look_back] <- 0
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/features_regr_zscores.png}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot dygraph of z-scores of VTI prices
> price_s <- cbind(clos_e, price_scores)
> colnames(price_s) <- c("VTI", "Z-scores")
> col_names <- colnames(price_s)
> dygraphs::dygraph(price_s["2009"], main="VTI Price Z-Scores") %>%
+   dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
+   dyAxis("y2", label=col_names[2], independentTicks=TRUE) %>%
+   dySeries(name=col_names[1], axis="y", label=col_names[1], strokeWidth=2, col="blue") %>%
+   dySeries(name=col_names[2], axis="y2", label=col_names[2], strokeWidth=2, col="red")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Hampel Filter for Outlier Detection}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Median Absolute Deviation} (\emph{MAD}) is a robust measure of dispersion (variability):
      {\scriptsize
      \begin{displaymath}
        \operatorname{MAD} = \operatorname{median}(\operatorname{abs}(p_i - \operatorname{median}(\mathbf{p})))
      \end{displaymath}
      }
      The \emph{Hampel filter} uses the \emph{MAD} dispersion measure to detect outliers in data.
      \vskip1ex
      The \emph{Hampel z-score} is equal to the deviation from the median divided by the \emph{MAD}:
      {\scriptsize
      \begin{displaymath}
        z_i = \frac{p_i - \operatorname{median}(\mathbf{p})}{\operatorname{MAD}}
      \end{displaymath}
      }
      A time series of \emph{z-scores} over past data can be calculated using a rolling look-back window.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Extract time series of VTI log prices
> clos_e <- log(na.omit(rutils::etf_env$price_s$VTI))
> # Define look-back window and a half window
> look_back <- 11
> # Calculate time series of medians
> medi_an <- roll::roll_median(clos_e, width=look_back)
> # medi_an <- TTR::runMedian(clos_e, n=look_back)
> # Calculate time series of MAD
> ma_d <- HighFreq::roll_var(clos_e, look_back=look_back, method="quantile")
> # ma_d <- TTR::runMAD(clos_e, n=look_back)
> # Calculate time series of z-scores
> z_scores <- (clos_e - medi_an)/ma_d
> z_scores[1:look_back, ] <- 0
> tail(z_scores, look_back)
> range(z_scores)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hampel_zscores.png}\\
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot prices and medians
> dygraphs::dygraph(cbind(clos_e, medi_an), main="VTI median") %>%
+   dyOptions(colors=c("black", "red"))
> # Plot histogram of z-scores
> histo_gram <- hist(z_scores, col="lightgrey",
+   xlab="z-scores", breaks=50, xlim=c(-4, 4),
+   ylab="frequency", freq=FALSE, main="Hampel Z-scores histogram")
> lines(density(z_scores, adjust=1.5), lwd=3, col="blue")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{One-sided and Two-sided Data Filters}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Filters calculated over past data are referred to as \emph{one-sided} filters, and they are appropriate for filtering real-time data.
      \vskip1ex
      Filters calculated over both past and future data are called \emph{two-sided} (centered) filters, and they are appropriate for filtering historical data.
      \vskip1ex
      The function \texttt{HighFreq::roll\_var()} with parameter \texttt{method="quantile"} calculates the rolling \emph{MAD} using a trailing look-back interval over past data.
      \vskip1ex
      The functions \texttt{TTR::runMedian()} and \texttt{TTR::runMAD()} calculate the rolling medians and \emph{MAD} using a trailing look-back interval over past data.
      \vskip1ex
      If the rolling medians and \emph{MAD} are advanced (shifted backward) in time, then they are calculated over both past and future data (centered).
      \vskip1ex
      The function \texttt{rutils::lag\_it()} with a negative \texttt{lagg} parameter value advances (shifts back) future data points to the present.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate one-sided Hampel z-scores
> medi_an <- roll::roll_median(clos_e, width=look_back)
> # medi_an <- TTR::runMedian(clos_e, n=look_back)
> ma_d <- HighFreq::roll_var(clos_e, look_back=look_back, method="quantile")
> # ma_d <- TTR::runMAD(clos_e, n=look_back)
> z_scores <- (clos_e - medi_an)/ma_d
> z_scores[1:look_back, ] <- 0
> tail(z_scores, look_back)
> range(z_scores)
> # Calculate two-sided Hampel z-scores
> half_back <- look_back %/% 2
> medi_an <- rutils::lag_it(medi_an, lagg=-half_back)
> ma_d <- rutils::lag_it(ma_d, lagg=-half_back)
> z_scores <- (clos_e - medi_an)/ma_d
> z_scores[1:look_back, ] <- 0
> tail(z_scores, look_back)
> range(z_scores)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: State Space Models}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{state space model} is a stochastic process for a \emph{state variable} $\theta$, which is subject to \emph{measurement error}.
      \vskip1ex
      The \emph{state variable} $\theta$ is latent (not directly observable), and its value is only measured by observing the \emph{measurement variable} $y_t$.
      \vskip1ex
      A simple \emph{state space model} can be written as a \emph{transition equation} and a \emph{measurement equation}:
      \begin{align*}
        \theta_t &= g_t \theta_{t-1} + w_t \\
        y_t &= f_t \theta_t + v_t
      \end{align*}
      Where $w_t$ and $v_t$ follow the normal distributions $\phi(0, \sigma_t^w)$ and $\phi(0, \sigma_t^v)$.
      \vskip1ex
      The system variables (matrices) $g_t$ and $f_t$ are deterministic functions of time.
      \vskip1ex
      If the \emph{time series} has zero \emph{expected} mean, then the \emph{EWMA} \emph{realized} variance estimator can be written approxiamtely as:
      $\sigma_i^2$ is the weighted \emph{realized} variance, equal to the weighted average of the point realized variance for period \texttt{i} and the past \emph{realized} variance.
      \vskip1ex
      The parameter $\lambda$ determines the rate of decay of the \emph{EWMA} weights, with smaller values of $\lambda$ producing faster decay, giving more weight to recent realized variance, and vice versa.
      \vskip1ex
      The function \texttt{stats:::C\_cfilter()} calculates the convolution of a vector or a time series with a filter of coefficients (weights).
      \vskip1ex
      The function \texttt{stats:::C\_cfilter()} is very fast because it's compiled \texttt{C++} code.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vol_vti_ewma.png}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate EWMA VTI variance using compiled C++ function
> look_back <- 51
> weight_s <- exp(-0.1*1:look_back)
> weight_s <- weight_s/sum(weight_s)
> vari_ance <- .Call(stats:::C_cfilter, re_turns^2, 
+   filter=weight_s, sides=1, circular=FALSE)
> vari_ance[1:(look_back-1)] <- vari_ance[look_back]
> # Plot EWMA volatility
> vari_ance <- xts:::xts(sqrt(vari_ance), order.by=index(re_turns))
> dygraphs::dygraph(vari_ance, main="VTI EWMA Volatility")
> quantmod::chart_Series(x_ts, name="VTI EWMA Volatility")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Estimating and Modeling Volatility and Skewness}


%%%%%%%%%%%%%%%
\subsection{Calculating the Rolling Variance of Asset Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The variance of asset returns exhibits \emph{heteroskedasticity}, i.e. it changes over time.
      \vskip1ex
      The rolling variance of returns is given by:
      {\scriptsize
      \begin{flalign*}
        \sigma_i^2 &= \frac{1}{k-1} \sum_{j=0}^{k-1} (r_{i-j}-\bar{r_i})^2 \\
        \bar{r_i} &= \frac{1}{k}{\sum_{j=0}^{k-1} r_{i-j}}
      \end{flalign*}
      }
      Where \texttt{k} is the \emph{look-back interval} equal to the number of data points
      for performing aggregations over the past.
      \vskip1ex
      It's also possible to calculate the rolling variance in \texttt{R} using vectorized functions, without using an \texttt{apply()} loop.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # VTI percentage returns
> re_turns <- na.omit(rutils::etf_env$re_turns$VTI)
> n_rows <- NROW(re_turns)
> # Define end points
> end_p <- 1:NROW(re_turns)
> # Start points are multi-period lag of end_p
> look_back <- 11
> start_p <- c(rep_len(0, look_back-1), end_p[1:(n_rows-look_back+1)])
> # Calculate rolling variance in sapply() loop - takes very long
> vari_ance <- sapply(1:n_rows, function(in_dex) {
+   ret_s <- re_turns[start_p[in_dex]:end_p[in_dex]]
+   sum((ret_s - mean(ret_s))^2)
+ }) / (look_back-1)  # end sapply
> # Use only vectorized functions
> cum_rets <- cumsum(re_turns)
> cum_rets <- (cum_rets -
+   c(rep_len(0, look_back), cum_rets[1:(n_rows-look_back)]))
> cum_rets2 <- cumsum(re_turns^2)
> cum_rets2 <- (cum_rets2 -
+   c(rep_len(0, look_back), cum_rets2[1:(n_rows-look_back)]))
> vari_ance2 <- (cum_rets2 - cum_rets^2/look_back)/(look_back-1)
> all.equal(vari_ance[-(1:look_back)], as.numeric(vari_ance2)[-(1:look_back)])
> # Same, using package rutils
> cum_rets <- rutils::roll_sum(re_turns, look_back=look_back, min_obs=1)
> cum_rets2 <- rutils::roll_sum(re_turns^2, look_back=look_back, min_obs=1)
> vari_ance2 <- (cum_rets2 - cum_rets^2/look_back)/(look_back-1)
> # Coerce vari_ance into xts
> tail(vari_ance)
> class(vari_ance)
> vari_ance <- xts(vari_ance, order.by=index(re_turns))
> colnames(vari_ance) <- "VTI.variance"
> head(vari_ance)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Calculating the Rolling Variance Using Package \protect\emph{roll}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{roll} contains functions for calculating \emph{weighted} rolling aggregations over \emph{vectors} and \emph{time series} objects:
      \begin{itemize}
        \item \texttt{roll\_sum()} for the \emph{weighted} rolling sum,
        \item \texttt{roll\_var()} for the \emph{weighted} rolling variance,
        \item \texttt{roll\_scale()} for the rolling scaling and centering of time series,
        \item \texttt{roll\_pcr()} for the rolling principal component regressions of time series.
      \end{itemize}
      The \emph{roll} functions are about \texttt{1,000} times faster than \texttt{apply()} loops!
      \vskip1ex
      The \emph{roll} functions are extremely fast because they perform calculations in \emph{parallel} in compiled \texttt{C++} code, using packages \emph{Rcpp} and \emph{RcppArmadillo}.
      \vskip1ex
      The \emph{roll} functions accept \emph{xts} time series, and they return \emph{xts}.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate rolling VTI variance using package roll
> library(roll)  # Load roll
> vari_ance <- roll::roll_var(re_turns, width=look_back)
> colnames(vari_ance) <- "VTI.variance"
> head(vari_ance)
> sum(is.na(vari_ance))
> vari_ance[1:(look_back-1)] <- 0
> # Benchmark calculation of rolling variance
> library(microbenchmark)
> summary(microbenchmark(
+   roll_sapply=sapply(2:n_rows, function(in_dex) {
+     ret_s <- re_turns[start_p[in_dex]:end_p[in_dex]]
+     sum((ret_s - mean(ret_s))^2)
+   }),
+   ro_ll=roll::roll_var(re_turns, width=look_back),
+   times=10))[, c(1, 4, 5)]
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling \protect\emph{EWMA} Realized Volatility Estimator}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Time-varying volatility can be more accurately estimated using an \emph{Exponentially Weighted Moving Average} (\emph{EWMA}) variance estimator.
      \vskip1ex
      If the \emph{time series} has zero \emph{expected} mean, then the \emph{EWMA} \emph{realized} variance estimator can be written approxiamtely as:
      {\scriptsize
      \begin{displaymath}
        \sigma_i^2 = (1-\lambda) {r_i}^2 + \lambda \sigma_{i-1}^2 = (1-\lambda) \sum_{j=0}^{\infty} \lambda^j {r_{i-j}}^2
      \end{displaymath}
      }
      $\sigma_i^2$ is the weighted \emph{realized} variance, equal to the weighted average of the point realized variance for period \texttt{i} and the past \emph{realized} variance.
      \vskip1ex
      The parameter $\lambda$ determines the rate of decay of the \emph{EWMA} weights, with smaller values of $\lambda$ producing faster decay, giving more weight to recent realized variance, and vice versa.
      \vskip1ex
      The function \texttt{stats:::C\_cfilter()} calculates the convolution of a vector or a time series with a filter of coefficients (weights).
      \vskip1ex
      The function \texttt{stats:::C\_cfilter()} is very fast because it's compiled \texttt{C++} code.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vol_vti_ewma.png}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate EWMA VTI variance using compiled C++ function
> look_back <- 51
> weight_s <- exp(-0.1*1:look_back)
> weight_s <- weight_s/sum(weight_s)
> vari_ance <- .Call(stats:::C_cfilter, re_turns^2, 
+   filter=weight_s, sides=1, circular=FALSE)
> vari_ance[1:(look_back-1)] <- vari_ance[look_back]
> # Plot EWMA volatility
> vari_ance <- xts:::xts(sqrt(vari_ance), order.by=index(re_turns))
> dygraphs::dygraph(vari_ance, main="VTI EWMA Volatility") %>%
+   dyOptions(colors="blue")
> quantmod::chart_Series(x_ts, name="VTI EWMA Volatility")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimating \protect\emph{EWMA} Variance Using Package \protect\emph{roll}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the \emph{time series} has non-zero \emph{expected} mean, then the rolling \emph{EWMA} variance is a vector given by the estimator:
      {\scriptsize
      \begin{flalign*}
        \sigma_i^2 &= \frac{1}{k-1} \sum_{j=0}^{k-1} {w_j (r_{i-j}-\bar{r_i})^2} \\
        \bar{r_i} &= \frac{1}{k}{\sum_{j=0}^{k-1} {w_j r_{i-j}}}
      \end{flalign*}
      }
      Where $w_j$ is the vector of weights:
      {\scriptsize
      \begin{displaymath}
        w_j = \frac{\lambda^j}{\sum_{j=0}^{k-1} \lambda^j}
      \end{displaymath}
      }
      The function \texttt{roll\_var()} from package \emph{roll} calculates the rolling \emph{EWMA} variance.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate rolling VTI variance using package roll
> library(roll)  # Load roll
> vari_ance <- roll::roll_var(re_turns,
+   weights=rev(weight_s), width=look_back)
> colnames(vari_ance) <- "VTI.variance"
> class(vari_ance)
> head(vari_ance)
> sum(is.na(vari_ance))
> vari_ance[1:(look_back-1)] <- 0
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Estimating Daily Volatility From Intraday Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The standard \emph{close-to-close} volatility $\sigma$ depends on the \emph{Close} prices $C_i$ from \emph{OHLC} data:
      {\scriptsize
      \begin{flalign*}
        \sigma^2 &= \frac{1}{n-1} \sum_{i=1}^{n} (r_i - \bar{r})^2 \\
        \bar{r} &= \frac{1}{n}{\sum_{i=0}^{n} r_i} \quad r_i = \log(\frac{C_i}{C_{i-1}})
      \end{flalign*}
      }
      But intraday time series of prices (for example \texttt{HighFreq::SPY} prices), can have large overnight jumps which inflate the volatility estimates.
      \vskip1ex
      So the overnight returns must be divided by the overnight time interval (in seconds), which produces per second returns.
      \vskip1ex
      The per second returns can be multiplied by \texttt{60} to scale them back up to per minute returns.
      \vskip1ex
      The function \texttt{zoo::index()} extracts the time index of a time series.
      \vskip1ex
      The function \texttt{zoo::.index()} extracts the time index expressed in the number of seconds.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(HighFreq)  # Load HighFreq
> # Minutely SPY returns (unit per minute) single day
> # Minutely SPY volatility (unit per minute)
> re_turns <- rutils::diff_it(log(SPY["2012-02-13", 4]))
> sd(re_turns)
> # SPY returns multiple days (includes overnight jumps)
> re_turns <- rutils::diff_it(log(SPY[, 4]))
> sd(re_turns)
> # Table of time intervals - 60 second is most frequent
> in_dex <- rutils::diff_it(.index(SPY))
> table(in_dex)
> # SPY returns divided by the overnight time intervals (unit per second)
> re_turns <- re_turns / in_dex
> re_turns[1] <- 0
> # Minutely SPY volatility scaled to unit per minute
> 60*sd(re_turns)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Range Volatility Estimators of \protect\emph{OHLC} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Range estimators of return volatility utilize the \texttt{high} and \texttt{low} prices, and therefore have lower standard errors than the standard \emph{close-to-close} estimator.
      \vskip1ex
      The \emph{Garman-Klass} estimator uses the \emph{low-to-high} price range, but it underestimates volatility because it doesn't account for \emph{close-to-open} price jumps:
      {\scriptsize
      \begin{displaymath}
        \sigma^2 = \frac{1}{n} \sum_{i=1}^{n} (0.5\log(\frac{H_i}{L_i})^2 - (2\log2-1)\log(\frac{C_i}{O_i})^2)
      \end{displaymath}
      }
      The \emph{Yang-Zhang} estimator accounts for \emph{close-to-open} price jumps and has the lowest standard error among unbiased estimators:
      {\scriptsize
      \begin{multline*}
        \hspace{-1em}\sigma^2 = \frac{1}{n-1} \sum_{i=1}^{n} (\log(\frac{O_i}{C_{i-1}})-\bar{r}_{co})^2 + \\
        0.134(\log(\frac{C_i}{O_i})-\bar{r}_{oc})^2 + \\
        \frac{0.866}{n} \sum_{i=1}^{n} (\log(\frac{H_i}{O_i})\log(\frac{H_i}{C_i}) + \log(\frac{L_i}{O_i})\log(\frac{L_i}{C_i}))
      \end{multline*}
      }
    \column{0.5\textwidth}
      The \emph{Yang-Zhang} (\emph{YZ}) and \emph{Garman-Klass-Yang-Zhang} (\emph{GKYZ}) estimators are unbiased and have up to seven times smaller standard errors than the standard close-to-close estimator.
      \vskip1ex
      But in practice, prices are not observed continuously, so the price range is underestimated, and so is the variance when using the \emph{YZ} and \emph{GKYZ} range estimators.
      \vskip1ex
      Therefore in practice the \emph{YZ} and \emph{GKYZ} range estimators underestimate the volatility, and their standard errors are reduced less than by the theoretical amount, for the same reason.
      \vskip1ex
      The \emph{Garman-Klass-Yang-Zhang} estimator is another very efficient and unbiased estimator, and also accounts for \emph{close-to-open} price jumps:
      {\scriptsize
      \begin{multline*}
        \sigma^2 = \frac{1}{n} \sum_{i=1}^{n} ((\log(\frac{O_i}{C_{i-1}})-\bar{r})^2 + \\
        0.5\log(\frac{H_i}{L_i})^2 - (2\log2-1)(\log(\frac{C_i}{O_i})^2))
      \end{multline*}
      }
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Calculating the Rolling Range Variance Using \protect\emph{HighFreq}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{HighFreq::calc\_var\_ohlc()} calculates the \emph{variance} of returns using several different range volatility estimators.
      \vskip1ex
      If the logarithms of the \emph{OHLC} prices are passed into \texttt{HighFreq::calc\_var\_ohlc()} then it calculates the variance of percentage returns, and if simple \emph{OHLC} prices are passed then it calculates the variance of dollar returns. 
      \vskip1ex
      The function \texttt{HighFreq::roll\_var\_ohlc()} calculates the \emph{rolling} variance of returns using several different range volatility estimators.
      \vskip1ex
      The functions \texttt{HighFreq::calc\_var\_ohlc()} and \texttt{HighFreq::roll\_var\_ohlc()} are very fast because they are written in \texttt{C++} code.
      \vskip1ex
      The function \texttt{TTR::volatility()} calculates the range volatility, but it's significantly slower than \texttt{HighFreq::calc\_var\_ohlc()}.
      \vskip1ex
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(HighFreq)  # Load HighFreq
> sp_y <- HighFreq::SPY["2009"]
> # Calculate daily SPY volatility using package HighFreq
> sqrt(6.5*60*HighFreq::calc_var_ohlc(log(sp_y), 
+   method="yang_zhang"))
> # Calculate daily SPY volatility from minutely prices using package TTR
> sqrt((6.5*60)*mean(na.omit(
+   TTR::volatility(sp_y, N=1, calc="yang.zhang"))^2))
> # Calculate rolling SPY variance using package HighFreq
> vari_ance <- HighFreq::roll_var_ohlc(log(sp_y), method="yang_zhang", 
+   look_back=look_back)
> # Plot range volatility
> vari_ance <- xts:::xts(sqrt(vari_ance), order.by=index(sp_y))
> dygraphs::dygraph(vari_ance["2009-02"], 
+   main="SPY Rolling Range Volatility") %>%
+   dyOptions(colors="blue")
> # Benchmark the speed of HighFreq vs TTR
> library(microbenchmark)
> summary(microbenchmark(
+   ttr=TTR::volatility(rutils::etf_env$VTI, N=1, calc="yang.zhang"),
+   highfreq=HighFreq::calc_var_ohlc(log(rutils::etf_env$VTI), method="yang_zhang"),
+   times=2))[, c(1, 4, 5)]
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{VXX Prices and the Rolling Volatility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{VXX} ETF invests in \emph{VIX} futures, so its price is tied to the level of the \emph{VIX} index, with higher \emph{VXX} prices corresponding to higher levels of the \emph{VIX} index. 
      \vskip1ex
      The rolling volatility of past returns moves in sympathy with the implied volatility and \emph{VXX} prices, but with a lag.
      \vskip1ex
      But \emph{VXX} prices exhibit a very strong downward trend which makes them hard to compare with the rolling volatility.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate VXX log prices
> vx_x <- na.omit(rutils::etf_env$price_s$VXX)
> date_s <- zoo::index(vx_x)
> look_back <- 41
> vx_x <- log(vx_x)
> # Calculate rolling VTI volatility
> clos_e <- get("VTI", rutils::etf_env)[date_s]
> clos_e <- log(clos_e)
> vol_at <- sqrt(HighFreq::roll_var_ohlc(oh_lc=clos_e, look_back=look_back, scal_e=FALSE))
> vol_at[1:look_back] <- vol_at[look_back+1]
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vxx_volat.png}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot dygraph of VXX and VTI volatility
> da_ta <- cbind(vx_x, vol_at)
> colnames(da_ta)[2] <- "VTI Volatility"
> col_names <- colnames(da_ta)
> cap_tion <- "VXX and VTI Volatility"
> dygraphs::dygraph(da_ta[, 1:2], main=cap_tion) %>%
+   dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
+   dyAxis("y2", label=col_names[2], independentTicks=TRUE) %>%
+   dySeries(name=col_names[1], axis="y", label=col_names[1], strokeWidth=1, col="blue") %>%
+   dySeries(name=col_names[2], axis="y2", label=col_names[2], strokeWidth=1, col="red")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Cointegration of VXX Prices and the Rolling Volatility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The rolling volatility of past returns moves in sympathy with the implied volatility and \emph{VXX} prices, but with a lag.
      \vskip1ex
      The parameter $\alpha$ is the weight of the squared realized returns in the variance.
      \vskip1ex
      Greater values of $\alpha$ produce a stronger feedback between the realized returns and variance, causing stronger variance spikes and higher kurtosis.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate VXX log prices
> vx_x <- na.omit(rutils::etf_env$price_s$VXX)
> date_s <- zoo::index(vx_x)
> look_back <- 41
> vx_x <- log(vx_x)
> vx_x <- (vx_x - roll::roll_mean(vx_x, width=look_back))
> vx_x[1:look_back] <- vx_x[look_back+1]
> # Calculate rolling VTI volatility
> clos_e <- get("VTI", rutils::etf_env)[date_s]
> clos_e <- log(clos_e)
> vol_at <- sqrt(HighFreq::roll_var_ohlc(oh_lc=clos_e, look_back=look_back, scal_e=FALSE))
> vol_at[1:look_back] <- vol_at[look_back+1]
> # Calculate regression coefficients of XLB ~ XLE
> be_ta <- drop(cov(vx_x, vol_at)/var(vol_at))
> al_pha <- drop(mean(vx_x) - be_ta*mean(vol_at))
> # Calculate regression residuals
> fit_ted <- (al_pha + be_ta*vol_at)
> residual_s <- (vx_x - fit_ted)
> # Perform ADF test on residuals
> tseries::adf.test(residual_s, k=1)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/garch_hist.png}
      \vspace{-2em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot dygraph of VXX and VTI volatility
> da_ta <- cbind(vx_x, vol_at)
> col_names <- colnames(da_ta)
> cap_tion <- "VXX and VTI Volatility"
> dygraphs::dygraph(da_ta[, 1:2], main=cap_tion) %>%
+   dyAxis("y", label=col_names[1], independentTicks=TRUE) %>%
+   dyAxis("y2", label=col_names[2], independentTicks=TRUE) %>%
+   dySeries(name=col_names[1], axis="y", label=col_names[1], strokeWidth=1, col="blue") %>%
+   dySeries(name=col_names[2], axis="y2", label=col_names[2], strokeWidth=1, col="red")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of Volatility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Variance calculated over non-overlapping intervals has very statistically significant autocorrelations.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # VTI percentage returns
> re_turns <- na.omit(rutils::etf_env$re_turns$VTI)
> # Calculate rolling VTI variance using package roll
> look_back <- 22
> vari_ance <- roll::roll_var(re_turns, width=look_back)
> vari_ance[1:(look_back-1)] <- 0
> colnames(vari_ance) <- "VTI.variance"
> # Number of look_backs that fit over re_turns
> n_rows <- NROW(re_turns)
> n_agg <- n_rows %/% look_back
> # Define end_p with beginning stub
> end_p <- c(0, n_rows-look_back*n_agg + (0:n_agg)*look_back)
> n_rows <- NROW(end_p)
> # Subset vari_ance to end_p
> vari_ance <- vari_ance[end_p]
> # Plot autocorrelation function
> rutils::plot_acf(vari_ance, lag=10, main="ACF of Variance")
> # Plot partial autocorrelation
> pacf(vari_ance, lag=10, main="PACF of Variance", ylab=NA)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/acf_var.png}\\
      \includegraphics[width=0.45\paperwidth]{figure/pacf_var.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Volatility Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{GARCH(1,1)} model is a volatility model defined by two coupled equations:
      \begin{flalign*}
        r_i &= \mu + \sigma_{i-1} \xi_i \\
        \sigma_i^2 &= \omega + \alpha r_i^2 + \beta \sigma_{i-1}^2
      \end{flalign*}
      Where $\sigma_i^2$ is the time-dependent variance, equal to the weighted average of the point \emph{realized} variance ${r_{i-1}}^2$, and the past variance $\sigma_{i-1}^2$.
      \vskip1ex
      The return process $r_i$ follows a normal distribution with time-dependent variance $\sigma_i^2$.
      \vskip1ex
      The parameter $\alpha$ is the weight associated with recent realized variance updates, and $\beta$ is the weight associated with the past variance.
      \vskip1ex
      The parameter $\omega$ is proportional to the long-term average level of variance, which is given by:
      \begin{displaymath}
        \sigma^2 = \frac{\omega}{1 - \alpha - \beta}
      \end{displaymath}
      The sum of $\alpha$ plus $\beta$ should be less than $1$, otherwise the volatility is explosive.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Define GARCH parameters
> om_ega <- 0.01 ; al_pha <- 0.2
> be_ta <- 0.79 ; n_rows <- 1000
> re_turns <- numeric(n_rows)
> vari_ance <- numeric(n_rows)
> vari_ance[1] <- om_ega/(1-al_pha-be_ta)
> re_turns[1] <- rnorm(1, sd=sqrt(vari_ance[1]))
> # Simulate GARCH model
> set.seed(1121)  # Reset random numbers
> for (i in 2:n_rows) {
+   re_turns[i] <- rnorm(n=1, sd=sqrt(vari_ance[i-1]))
+   vari_ance[i] <- om_ega + al_pha*re_turns[i]^2 +
+     be_ta*vari_ance[i-1]
+ }  # end for
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Volatility Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{GARCH(1,1)} volatility model exhibits sharp spikes in the volatility, followed by a quick decay of volatility.
      \vskip1ex
      But the decay of volatility in the \emph{GARCH} model is faster than what is observed in practice.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot GARCH cumulative returns
> plot(cumsum(re_turns/100), t="l",
+   lwd=2, col="blue", xlab="", ylab="",
+   main="GARCH cumulative returns")
> # Plot dygraphs GARCH volatility
> date_s <- seq.Date(from=Sys.Date()-n_rows+1,
+   to=Sys.Date(), length.out=n_rows)
> x_ts <- xts:::xts(cumsum(re_turns/100), order.by=date_s)
> dygraphs::dygraph(x_ts, main="GARCH cumulative returns")
> # Plot GARCH volatility
> plot(sqrt(vari_ance), t="l",
+   col="blue", xlab="", ylab="",
+   main="GARCH Volatility")
> # Plot dygraphsGARCH volatility
> vari_ance <- xts:::xts(sqrt(vari_ance), order.by=date_s)
> dygraphs::dygraph(vari_ance, main="GARCH Volatility")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/garch_returns.png}\\
      \includegraphics[width=0.45\paperwidth]{figure/garch_stdev.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Model Properties}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The parameter $\alpha$ is the weight of the squared realized returns in the variance.
      \vskip1ex
      Greater values of $\alpha$ produce a stronger feedback between the realized returns and variance, causing stronger variance spikes and higher kurtosis.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Define GARCH parameters
> om_ega <- 0.0001 ; al_pha <- 0.5
> be_ta <- 0.1 ; n_rows <- 10000
> re_turns <- numeric(n_rows)
> vari_ance <- numeric(n_rows)
> vari_ance[1] <- om_ega/(1-al_pha-be_ta)
> set.seed(1121)  # Reset random numbers
> re_turns[1] <- rnorm(1, sd=sqrt(vari_ance[1]))
> # Simulate GARCH model
> for (i in 2:n_rows) {
+   re_turns[i] <- rnorm(n=1, sd=sqrt(vari_ance[i-1]))
+   vari_ance[i] <- om_ega + al_pha*re_turns[i]^2 +
+     be_ta*vari_ance[i-1]
+ }  # end for
> # Calculate kurtosis of GARCH returns
> mean(((re_turns-mean(re_turns))/sd(re_turns))^4)
> # Perform Jarque-Bera test of normality
> tseries::jarque.bera.test(re_turns)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/garch_hist.png}
      \vspace{-2em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot histogram of GARCH returns
> histo_gram <- hist(re_turns, col="lightgrey",
+   xlab="returns", breaks=200, xlim=c(-0.05, 0.05),
+   ylab="frequency", freq=FALSE,
+   main="GARCH returns histogram")
> lines(density(re_turns, adjust=1.5),
+ lwd=3, col="blue")
> optim_fit <- MASS::fitdistr(re_turns,
+   densfun="t", df=2, lower=c(-1, 1e-7))
> lo_cation <- optim_fit$estimate[1]
> scal_e <- optim_fit$estimate[2]
> curve(expr=dt((x-lo_cation)/scal_e, df=2)/scal_e,
+   type="l", xlab="", ylab="", lwd=3,
+   col="red", add=TRUE)
> legend("topright", inset=0.05, bty="n",
+  leg=c("density", "t-distr w/ 2 dof"),
+  lwd=6, lty=1, col=c("blue", "red"))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Model Calibration}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{GARCH} models can be calibrated on returns using the \emph{maximum-likelihood} method, but it's a complex optimization procedure.
      \vskip1ex
      The package \emph{fGarch} contains functions for applying \emph{GARCH} models.
      \vskip1ex
      The function \texttt{garchFit()} calibrates a \emph{GARCH} model on a time series of returns.
      \vskip1ex
      The function \texttt{garchFit()} returns an \texttt{S4} object of class \emph{fGARCH}, with multiple slots containing the \emph{GARCH} model outputs and diagnostic information.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(fGarch)
> # Fit returns into GARCH
> garch_fit <- fGarch::garchFit(data=re_turns)
> # Fitted GARCH parameters
> round(garch_fit@fit$coef, 5)
> # Actual GARCH parameters
> round(c(mu=mean(re_turns), omega=om_ega,
+   alpha=al_pha, beta=be_ta), 5)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/garch_fGarch_fitted.png}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot GARCH fitted volatility
> plot(sqrt(garch_fit@fit$series$h), t="l",
+   col="blue", xlab="", ylab="",
+   main="GARCH Fitted Volatility")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{GARCH} Model Simulation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{garchSpec()} from package \emph{fGarch} specifies a \emph{GARCH} model.
      \vskip1ex
      The function \texttt{garchSim()} simulates a \emph{GARCH} model.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Specify GARCH model
> garch_spec <- fGarch::garchSpec(model=list(omega=om_ega, 
+   alpha=al_pha, beta=be_ta))
> # Simulate GARCH model
> garch_sim <- fGarch::garchSim(spec=garch_spec, n=n_rows)
> re_turns <- as.numeric(garch_sim)
> # Calculate kurtosis of GARCH returns
> moments::moment(re_turns, order=4) /
+   moments::moment(re_turns, order=2)^2
> # Perform Jarque-Bera test of normality
> tseries::jarque.bera.test(re_turns)
> # Plot histogram of GARCH returns
> histo_gram <- hist(re_turns, col="lightgrey",
+   xlab="returns", breaks=200, xlim=c(-0.05, 0.05),
+   ylab="frequency", freq=FALSE,
+   main="GARCH returns histogram")
> lines(density(re_turns, adjust=1.5),
+ lwd=3, col="blue")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/garch_fGarch_hist.png}
      \vspace{-2em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Fit t-distribution into GARCH returns
> optim_fit <- MASS::fitdistr(re_turns,
+   densfun="t", df=2, lower=c(-1, 1e-7))
> lo_cation <- optim_fit$estimate[1]
> scal_e <- optim_fit$estimate[2]
> curve(expr=dt((x-lo_cation)/scal_e, df=2)/scal_e,
+   type="l", xlab="", ylab="", lwd=3,
+   col="red", add=TRUE)
> legend("topright", inset=0.05, bty="n",
+  leg=c("density", "t-distr w/ 2 dof"),
+  lwd=6, lty=1, col=c("blue", "red"))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: \protect\emph{GARCH} Model Forecasting}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{GARCH(1,1)} model is a volatility forecasting model defined as:
      \begin{displaymath}
        \hspace{-0.5em}\hat\sigma_i^2 = \omega + \alpha {\Delta r_{i-1}}^2 + \beta \hat\sigma_{i-1}^2
      \end{displaymath}
      $\hat\sigma_i^2$ is the \emph{forecasted} variance, equal to the weighted average of the point \emph{realized} variance ${\Delta r_{i-1}}^2$, and the past variance forecast $\hat\sigma_{i-1}^2$.
      \vskip1ex
      The parameter $\omega$ is related to the long-term average level of variance, $\alpha$ is the weight associated with recent realized variance updates, and $\beta$ is the weight associated with the past variance forecasts.
      \vskip1ex
      In contrast to the \emph{EWMA} model, The \emph{GARCH} model doesn't depend on the realized variance in the current period \texttt{i}, but only on the past period $(i-1)$.
      \vskip1ex
      The rolling \emph{RcppRoll} functions are much faster than performing \texttt{apply()} loops, because they are \emph{compiled} functions (compiled from \texttt{C++} code).
      \vskip1ex
      But the rolling \emph{RcppRoll} functions are a little slower than using \emph{vectorized compiled} functions such as \texttt{cumsum()}.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(HighFreq)  # Load HighFreq
> # Calculate variance for each period
> vari_ance <- 252*(24*60*60)^2*
+   HighFreq::run_variance(oh_lc=rutils::etf_env$VTI)
> # Calculate EWMA VTI variance using RcppRoll
> library(RcppRoll)  # Load RcppRoll
> look_back <- 51
> weight_s <- exp(0.1*1:look_back)
> var_ewma <- RcppRoll::roll_mean(vari_ance,
+     align="right", n=look_back, weights=weight_s)
> var_ewma <- xts(var_ewma,
+     order.by=index(rutils::etf_env$VTI[-(1:(look_back-1)), ]))
> colnames(var_ewma) <- "VTI variance"
> # Plot EWMA variance with custom line colors
> x11(width=6, height=5)
> quantmod::chart_Series(rutils::etf_env$VTI["2010-01/2010-10"],
+    name="VTI EWMA variance with May 6, 2010 Flash Crash")
> # Add variance in extra panel
> add_TA(var_ewma["2010-01/2010-10"], col="black")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: old stuff about Estimating Volatility of Intraday Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{close-to-close} estimator depends on \emph{Close} prices specified over the aggregation intervals:
      \begin{displaymath}
        \hat\sigma^2 = \frac{1}{n-1} \sum_{i=1}^{n} (\log(\frac{C_i}{C_{i-1}})-\bar{r})^2
      \end{displaymath}
      \vspace{-1em}
      \begin{displaymath}
        \bar{r} = \frac{1}{n} \sum_{i=1}^{n} \log(\frac{C_i}{C_{i-1}})
      \end{displaymath}
      Volatility estimates for intraday time series depend both on the units of returns (per second, minute, day, etc.), and on the aggregation interval (secondly, minutely, daily, etc.)
      \vskip1ex
      A minutely time interval is equal to \texttt{60} seconds, a daily time interval is equal to \texttt{24*60*60 = 86,400} seconds.
      \vskip1ex
      For example, it's possible to measure returns in minutely intervals in units per second.
      \vskip1ex
      The estimated volatility is directly proportional to the measurement units.
      \vskip1ex
      For example, the volatility estimated from per minute returns is \texttt{60} times the volatility estimated from per second returns.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(HighFreq)  # Load HighFreq
> # Minutely SPY returns (unit per minute) single day
> re_turns <- rutils::diff_it(log(SPY["2012-02-13", 4]))
> # Minutely SPY volatility (unit per minute)
> sd(re_turns)
> # Divide minutely SPY returns by time intervals (unit per second)
> re_turns <- re_turns / rutils::diff_it(.index(SPY["2012-02-13"]))
> re_turns[1] <- 0
> # Minutely SPY volatility scaled to unit per minute
> 60*sd(re_turns)
> # SPY returns multiple days
> re_turns <- rutils::diff_it(log(SPY[, 4]))
> # Minutely SPY volatility (includes overnight jumps)
> sd(re_turns)
> # Table of intervals - 60 second is most frequent
> in_dex <- rutils::diff_it(.index(SPY))
> table(in_dex)
> # hist(in_dex)
> # SPY returns with overnight scaling (unit per second)
> re_turns <- re_turns / in_dex
> re_turns[1] <- 0
> # Minutely SPY volatility scaled to unit per minute
> 60*sd(re_turns)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Volatility as Function of Aggregation Interval}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The estimated volatility $\sigma$ scales as the \emph{power} of the length of the aggregation time interval $\Delta t$:
      \begin{displaymath}
        \frac{\sigma_t}{\sigma} = {\Delta t} ^ H
      \end{displaymath}
      Where \texttt{H} is the \emph{Hurst} exponent, $\sigma$ is the return volatility, and $\sigma_t$ is the volatility of the aggregated returns.
      \vskip1ex
      If returns follow \emph{Brownian motion} then the volatility scales as the \emph{square root} of the length of the aggregation interval (\texttt{H = 0.5}).
      \vskip1ex
      If returns are \emph{mean reverting} then the volatility scales slower than the \emph{square root} (\texttt{H < 0.5}).
      \vskip1ex
      If returns are \emph{trending} then the volatility scales faster than the \emph{square root} (\texttt{H > 0.5}).
      \vskip1ex
      The length of the daily time interval is often approximated to be equal to \texttt{390 = 6.5*60} minutes, since the exchange trading session is equal to \texttt{6.5} hours, and daily volatility is dominated by the trading session.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Minutely OHLC SPY prices aggregated to daily prices
> SPY_daily <- rutils::to_period(oh_lc=HighFreq::SPY, period="days")
> # Daily SPY volatility from daily returns
> sd(rutils::diff_it(log(SPY_daily[, 4])))
> # Minutely SPY returns (unit per minute)
> re_turns <- rutils::diff_it(log(SPY[, 4]))
> # Minutely SPY volatility scaled to daily interval
> sqrt(6.5*60)*sd(re_turns)
> # Minutely SPY returns with overnight scaling (unit per second)
> re_turns <- rutils::diff_it(log(SPY[, 4]))
> in_dex <- rutils::diff_it(.index(SPY))
> re_turns <- re_turns / in_dex
> re_turns[1] <- 0
> # Daily SPY volatility from minutely returns
> sqrt(6.5*60)*60*sd(re_turns)
> # Daily SPY volatility
> # Scale by extra time over weekends and holidays
> 24*60*60*sd(rutils::diff_it(log(SPY_daily[, 4]))[-1] /
+     rutils::diff_it(.index(SPY_daily))[-1])
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Hurst Exponent From Volatility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      For a single aggregation interval, the \emph{Hurst exponent} \texttt{H} is equal to:
      \begin{displaymath}
        H = \frac{\log{\sigma_t} - \log{\sigma}}{\log{\Delta t}}
      \end{displaymath}
      \vskip1ex
      For a vector of aggregation intervals, the \emph{Hurst exponent} \texttt{H} can be calculated by regressing the volatility against the aggregation intervals.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate SPY returns adjusted for overnight jumps
> clos_e <- log(as.numeric(Cl(HighFreq::SPY[, 4])))
> re_turns <- rutils::diff_it(clos_e) / 
+   rutils::diff_it(.index(HighFreq::SPY))
> re_turns[1] <- 0
> clos_e <- cumsum(re_turns)
> n_rows <- NROW(clos_e)
> # Calculate volatilities for vector of aggregation intervals
> interval_s <- seq.int(from=3, to=35, length.out=9)^2
> vol_s <- sapply(interval_s, function(inter_val) {
+   num_agg <- n_rows %/% inter_val
+   end_p <- c(0, n_rows - num_agg*inter_val + (0:num_agg)*inter_val)
+   # end_p <- rutils::calc_endpoints(clos_e, inter_val=inter_val)
+   sd(rutils::diff_it(clos_e[end_p]))
+ })  # end sapply
> # Calculate Hurst as regression slope using formula
> vol_log <- log(vol_s)
> inter_log <- log(interval_s)
> hurs_t <- cov(vol_log, inter_log)/var(inter_log)
> # Or using function lm()
> mod_el <- lm(vol_log ~ inter_log)
> coef(mod_el)[2]
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hurst_vol.png}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate Hurst from single data point
> (last(vol_log) - log(sd(re_turns)))/last(inter_log)
> # Plot the volatilities
> x11(width=6, height=5)
> par(mar=c(4, 4, 2, 1), oma=c(1, 1, 1, 1))
> plot(vol_log ~ inter_log, lwd=6, col="red",
+      xlab="aggregation intervals (log)", ylab="volatility (log)",
+      main="Hurst Exponent for SPY From Volatilities")
> abline(mod_el, lwd=3, col="blue")
> text(inter_log[2], vol_log[NROW(vol_log)-1], 
+      paste0("Hurst = ", round(hurs_t, 4)))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rescaled Range Analysis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The range $R_{\Delta t}$ of prices $p_t$ over an interval $\Delta t$, is the difference between the highest attained price minus the lowest:
      \begin{displaymath}
        R_t = \max_{\Delta t}{[p_{\tau}]} - \min_{\Delta t}{[p_{\tau}]}
      \end{displaymath}
      The \emph{Rescaled Range} $RS_{\Delta t}$ is equal to the range $R_{\Delta t}$ divided by the standard deviation of the price differences $\sigma_t$: $RS_{\Delta t} = R_t / \sigma_t$.
      \vskip1ex
      The \emph{Rescaled Range} $RS_{\Delta t}$ for a time series of prices is calculated by:
      \begin{itemize}
        \item Dividing the time series into non-overlapping intervals of length $\Delta t$,
        \item Calculating the \emph{rescaled range} $RS_{\Delta t}$ for each interval,
        \item Calculating the average of the \emph{rescaled ranges} $RS_{\Delta t}$ for all the intervals.
      \end{itemize}
      \emph{Rescaled Range Analysis} (R/S) consists of calculating the average \emph{rescaled range} $RS_{\Delta t}$ as a function of the length of the aggregation interval $\Delta t$.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate the rescaled range
> inter_val <- 500
> n_rows <- NROW(clos_e); num_agg <- n_rows %/% inter_val
> end_p <- c(0, n_rows - num_agg*inter_val + (0:num_agg)*inter_val)
> # Or
> # end_p <- rutils::calc_endpoints(clos_e, inter_val=inter_val)
> r_s <- sapply(2:NROW(end_p), function(ep) {
+   in_dex <- end_p[ep-1]:end_p[ep]
+   diff(range(clos_e[in_dex]))/sd(re_turns[in_dex])
+ })  # end sapply
> mean(r_s)
> # Calculate Hurst from single data point
> log(mean(r_s))/log(inter_val)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Hurst Exponent From Rescaled Range}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Rescaled Range} $RS_{\Delta t}$ is proportional to the length of the aggregation interval $\Delta t$ raised to the power of the \emph{Hurst exponent} \texttt{H}:
      \begin{displaymath}
        RS_{\Delta t} \propto {\Delta t}^H
      \end{displaymath}
      The \emph{Hurst exponents} calculated from the \emph{rescaled range} and the \emph{volatility} are similar because they both measure the dependence of returns over time, but they're not exactly equal because they use different methods to estimate price dispersion.
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate rescaled range for vector of aggregation intervals
> n_rows <- NROW(clos_e)
> r_s <- sapply(interval_s, function(inter_val) {
+ # Calculate end points
+   num_agg <- n_rows %/% inter_val
+   end_p <- c(0, n_rows - num_agg*inter_val + (0:num_agg)*inter_val)
+ # Calculate rescaled ranges
+   r_s <- sapply(2:NROW(end_p), function(ep) {
+     in_dex <- end_p[ep-1]:end_p[ep]
+     diff(range(clos_e[in_dex]))/sd(re_turns[in_dex])
+   })  # end sapply
+   mean(na.omit(r_s))
+ })  # end sapply
> # Calculate Hurst as regression slope using formula
> rs_log <- log(r_s)
> inter_log <- log(interval_s)
> hurs_t <- cov(rs_log, inter_log)/var(inter_log)
> # Or using function lm()
> mod_el <- lm(rs_log ~ inter_log)
> coef(mod_el)[2]
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/hurst_reg.png}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> plot(rs_log ~ inter_log, lwd=6, col="red",
+      xlab="aggregation intervals (log)",
+      ylab="rescaled range (log)",
+      main="Rescaled Range Analysis for SPY")
> abline(mod_el, lwd=3, col="blue")
> text(inter_log[2], rs_log[NROW(rs_log)-1], 
+      paste0("Hurst = ", round(hurs_t, 4)))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Comparing Range Volatility}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The range volatility estimators have much lower variability (standard errors) than the standard \emph{Close-to-Close} estimator.
      \vskip1ex
      Is the above correct?  Because the plot shows otherwise.
      \vskip1ex
      The range volatility estimators follow the standard \emph{Close-to-Close} estimator, except in intervals of high intra-period volatility.
      \vskip1ex
      During the May 6, 2010 \emph{flash crash}, range volatility spiked more than the \emph{Close-to-Close} volatility.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(HighFreq)  # Load HighFreq
> oh_lc <- log(rutils::etf_env$VTI)
> # Calculate variance
> var_close <- HighFreq::run_variance(oh_lc=oh_lc,
+   method="close")
> var_yang_zhang <- HighFreq::run_variance(oh_lc=oh_lc)
> std_dev <- 24*60*60*sqrt(252*cbind(var_close, var_yang_zhang))
> colnames(std_dev) <- c("close std_dev", "Yang-Zhang")
> # Plot the time series of volatility
> plot_theme <- chart_theme()
> plot_theme$col$line.col <- c("black", "red")
> quantmod::chart_Series(std_dev["2011-07/2011-12"],
+   theme=plot_theme, name="Standard Deviations: Close and YZ")
> legend("top", legend=colnames(std_dev),
+  bg="white", lty=1, lwd=6, inset=0.1, cex=0.8,
+  col=plot_theme$col$line.col, bty="n")
> # Plot volatility around 2010 flash crash
> quantmod::chart_Series(std_dev["2010-04/2010-06"],
+   theme=plot_theme, name="Volatility Around 2010 Flash Crash")
> legend("top", legend=colnames(std_dev),
+  bg="white", lty=1, lwd=6, inset=0.1, cex=0.8,
+  col=plot_theme$col$line.col, bty="n")
> # Plot density of volatility distributions
> plot(density(std_dev[, 1]), xlab="", ylab="",
+   main="Density of Volatility Distributions",
+   xlim=c(-0.05, range(std_dev[, 1])[2]/3), type="l", lwd=2, col="blue")
> lines(density(std_dev[, 2]), col='red', lwd=2)
> legend("top", legend=c("Close-to-Close", "Yang-Zhang"),
+  bg="white", lty=1, lwd=6, inset=0.1, cex=0.8,
+  col=plot_theme$col$line.col, bty="n")
> # ? range volatility estimator has lower standard error ?
> c(sd(var_close)/mean(var_close), sd(var_yang_zhang)/mean(var_yang_zhang))
> foo <- std_dev[var_close<range(var_close)[2]/3, ]
> c(sd(foo[, 1])/mean(foo[, 1]), sd(foo[, 2])/mean(foo[, 2]))
> plot(density(foo[, 1]), xlab="", ylab="",
+   main="Mixture of Normal Returns",
+   xlim=c(-0.05, range(foo[, 1])[2]/2), type="l", lwd=2, col="blue")
> lines(density(foo[, 2]), col='red', lwd=2)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/vol_close_yz.png}\\
      \includegraphics[width=0.45\paperwidth]{figure/vol_density.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: \protect\emph{Log-range} Volatility Proxies}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
    % wippp
      To-do: plot time series of \emph{intra-day range} volatility estimator and standard close-to-close volatility estimator.  Emphasize flash-crash of 2010.
      \vskip1ex
      An alternative range volatility estimator can be created by calculating the logarithm of the range, (as opposed to the range percentage, or the logarithm of the price ratios).
      \vskip1ex
      To-do: plot scatterplot of \emph{intra-day range} volatility estimator and standard close-to-close volatility estimator.
      \vskip1ex
      Emphasize the two are different: the intra-day range volatility estimator captures volatility events which aren't captured by close-to-close volatility estimator, and vice versa.
      \begin{displaymath}
        \hat\sigma^2 = \frac{1}{n} \sum_{i=1}^{n} \log(\frac{H_i - L_i}{H_i + L_i})^2
      \end{displaymath}
      The range logarithm fits better into the normal distribution than the range percentage.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> oh_lc <- rutils::etf_env$VTI
> re_turns <- log((oh_lc[, 2] - oh_lc[, 3]) / (oh_lc[, 2] + oh_lc[, 3]))
> foo <- rutils::diff_it(log(oh_lc[, 4]))
> plot(as.numeric(foo)^2, as.numeric(re_turns)^2)
> bar <- lm(re_turns ~ foo)
> summary(bar)
> 
> 
> # Perform normality tests
> shapiro.test(coredata(re_turns))
> tseries::jarque.bera.test(re_turns)
> # Fit VTI returns using MASS::fitdistr()
> optim_fit <- MASS::fitdistr(re_turns,
+             densfun="t", df=2)
> optim_fit$estimate; optim_fit$sd
> # Calculate moments of standardized returns
> sapply(3:4, moments::moment,
+   x=(re_turns - mean(re_turns))/sd(re_turns))
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/log_range.png}
      \vspace{-2em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot histogram of VTI returns
> col_ors <- c("lightgray", "blue", "green", "red")
> PerformanceAnalytics::chart.Histogram(re_turns,
+   main="", xlim=c(-7, -3), col=col_ors[1:3],
+   methods = c("add.density", "add.normal"))
> curve(expr=dt((x-optim_fit$estimate[1])/
+   optim_fit$estimate[2], df=2)/optim_fit$estimate[2],
+ type="l", xlab="", ylab="", lwd=2,
+ col=col_ors[4], add=TRUE)
> # Add title and legend
> title(main="VTI logarithm of range",
+ cex.main=1.3, line=-1)
> legend("topright", inset=0.05,
+   legend=c("density", "normal", "t-distr"),
+   lwd=6, lty=1, col=col_ors[2:4], bty="n")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Autocorrelations of Alternative \protect\emph{Range} Estimators}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The logarithm of the range exhibits very significant autocorrelations, unlike the range percentage.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # VTI range variance partial autocorrelations
> pacf(re_turns^2, lag=10, xlab=NA, ylab=NA,
+      main="PACF of VTI log range")
> quantmod::chart_Series(re_turns^2, name="VTI log of range squared")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/pacf_log_range.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{depr: Standard Errors of Volatility Estimators Using Bootstrap}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The standard errors of estimators can be calculated using a \emph{bootstrap} simulation.
      \vskip1ex
      The \emph{bootstrap} procedure generates new data by randomly sampling with replacement from the observed data set.
      \vskip1ex
      The \emph{bootstrapped} data is then used to recalculate the estimator many times, producing a vector of values.
      \vskip1ex
      The \emph{bootstrapped} estimator values can then be used to calculate the probability distribution of the estimator and its standard error.
      \vskip1ex
      Bootstrapping doesn't provide accurate estimates for estimators that are sensitive to the ordering and correlations in the data.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Standard errors of variance estimators using bootstrap
> boot_data <- sapply(1:1e2, function(x) {
+   # Create random OHLC
+   oh_lc <- HighFreq::random_ohlc()
+   # Calculate variance estimate
+   c(var=var(oh_lc[, 4]),
+     yang_zhang=HighFreq::calc_variance(
+ oh_lc, method="yang_zhang", scal_e=FALSE))
+ })  # end sapply
> # Analyze bootstrapped variance
> boot_data <- t(boot_data)
> head(boot_data)
> colMeans(boot_data)
> apply(boot_data, MARGIN=2, sd) /
+   colMeans(boot_data)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Autocorrelations of \protect\emph{Close-to-Close} and \protect\emph{Range} Variances}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The standard \emph{Close-to-Close} estimator exhibits very significant autocorrelations, but the \emph{range} estimators are not autocorrelated.
      \vskip1ex
      That is because the time series of squared intra-period ranges is not autocorrelated.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Close variance estimator partial autocorrelations
> pacf(var_close, lag=10, xlab=NA, ylab=NA)
> title(main="VTI close variance partial autocorrelations")
> 
> # Range variance estimator partial autocorrelations
> pacf(var_yang_zhang, lag=10, xlab=NA, ylab=NA)
> title(main="VTI YZ variance partial autocorrelations")
> 
> # Squared range partial autocorrelations
> re_turns <- log(rutils::etf_env$VTI[,2] /
+             rutils::etf_env$VTI[,3])
> pacf(re_turns^2, lag=10, xlab=NA, ylab=NA)
> title(main="VTI squared range partial autocorrelations")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/var_pacf.png}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Performing Aggregations Over Time Series}


%%%%%%%%%%%%%%%
\subsection{Defining Look-back Time Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A time \emph{period} is the time between two neighboring points in time.
      \vskip1ex
      A time \emph{interval} is the time spanned by one or more time \emph{periods}.
      \vskip1ex
      A \emph{look-back interval} is a time \emph{interval} for performing aggregations over the past, starting from a \emph{start point} and ending at an \emph{end point}.
      \vskip1ex
      The \emph{start points} are the \emph{end points} lagged by the \emph{look-back interval}.
      \vskip1ex
      The look-back \emph{intervals} may or may not \emph{overlap} with their neighboring intervals.
    \column{0.5\textwidth}
      A \emph{rolling aggregation} is specified by \emph{end points} at each point in time.
      \vskip1ex
      An example of a rolling aggregation are moving average prices.
      \vskip1ex
      An \emph{interval aggregation} is specified by \emph{end points} separated by many time \emph{periods}.
      \vskip1ex
      Examples of interval aggregations are monthly asset returns, or trailing 12-month asset returns calculated every month.
  \end{columns}
    \vspace{-2em}
    \includegraphics[width=0.9\paperwidth]{figure/intervals_overlapping.png}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining \protect\emph{Rolling} Look-back Time Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A \emph{rolling aggregation} is specified by \emph{end points} at each point in time.
      \vskip1ex
      The first \emph{end point} is equal to zero $0$.
      \vskip1ex
      The \emph{start points} are the \emph{end points} lagged by the \emph{look-back interval}.
      \vskip1ex
      An example of a rolling aggregation are moving average prices.
    \column{0.5\textwidth}
\vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> oh_lc <- rutils::etf_env$VTI
> # Number of data points
> n_rows <- NROW(oh_lc["2018-06/"])
> # Define end_p at each point in time
> end_p <- 0:n_rows
> # Number of data points in look_back interval
> look_back <- 22
> # start_p are end_p lagged by look_back
> start_p <- c(rep_len(0, look_back - 1),
+     end_p[1:(NROW(end_p)- look_back + 1)])
> head(start_p, 33)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
    \vspace{-2em}
    \includegraphics[width=0.9\paperwidth]{figure/intervals_rolling.png}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining Equally Spaced \protect\emph{end points} of a Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The neighboring \emph{end points} may be separated by a fixed number of periods, equal to \texttt{n\_points}.
      \vskip1ex
      If the total number of data points is not an integer multiple of \texttt{n\_points}, then a stub interval must be added either at the beginning or at the end of the \emph{end points}.
      \vskip1ex
      The function \texttt{xts::endpoints()} extracts the indices of the last observations in each calendar period of an \emph{xts} series.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Number of data points
> clos_e <- quantmod::Cl(oh_lc["2018/"])
> n_rows <- NROW(clos_e)
> # Number of periods between endpoints
> n_points <- 21
> # Number of n_points that fit over n_rows
> n_agg <- n_rows %/% n_points
> # If n_rows==n_points*n_agg then whole number
> end_p <- (0:n_agg)*n_points
> # Stub interval at beginning
> end_p <- c(0, n_rows-n_points*n_agg + (0:n_agg)*n_points)
> # Else stub interval at end
> end_p <- c((0:n_agg)*n_points, n_rows)
> # Or use xts::endpoints()
> end_p <- xts::endpoints(clos_e, on="months")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/intervals_end_points.png}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot data and endpoints as vertical lines
> plot.xts(clos_e, col="blue", lwd=2, xlab="", ylab="",
+    main="Prices with Endpoints as Vertical Lines")
> addEventLines(xts(rep("endpoint", NROW(end_p)-1), index(clos_e)[end_p]),
+         col="red", lwd=2, pos=4)
> # Or
> plot_theme <- chart_theme()
> plot_theme$col$line.col <- "blue"
> quantmod::chart_Series(clos_e, theme=plot_theme,
+   name="prices with endpoints as vertical lines")
> abline(v=end_p, col="red", lwd=2)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining \protect\emph{Overlapping} Look-back Time Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Overlapping} time intervals can be defined if the \emph{start points} are equal to the \emph{end points} lagged by the \emph{look-back interval}.
      \vskip1ex
      An example of an overlapping interval aggregation are trailing 12-month asset returns calculated every month.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Number of data points
> n_rows <- NROW(rutils::etf_env$VTI["2019/"])
> # Number of n_points that fit over n_rows
> n_points <- 21
> n_agg <- n_rows %/% n_points
> # Stub interval at beginning
> end_p <- c(0, n_rows-n_points*n_agg + (0:n_agg)*n_points)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      The length of the \emph{look-back interval} can be defined either as the number of data points, or as the number of \emph{end points} to look back over.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # look_back defined as number of data points
> look_back <- 252
> # start_p are end_p lagged by look_back
> start_p <- (end_p - look_back + 1)
> start_p <- ifelse(start_p < 0, 0, start_p)
> # look_back defined as number of end_p
> look_back <- 12
> start_p <- c(rep_len(0, look_back-1),
+     end_p[1:(NROW(end_p)- look_back + 1)])
> # Bind start_p with end_p
> cbind(start_p, end_p)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
    \vspace{-2em}
    \includegraphics[width=0.9\paperwidth]{figure/intervals_overlapping.png}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining \protect\emph{Non-overlapping} Look-back Time Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Non-overlapping} time intervals can be defined if \emph{start points} are equal to the previous \emph{end points}.
      \vskip1ex
      In that case the look-back \emph{intervals} are non-overlapping and \emph{contiguous} (each \emph{start point} is the \emph{end point} of the previous interval).
      \vskip1ex
      If the \emph{start points} are defined as the previous \emph{end points} plus $1$, then the \emph{intervals} are \emph{exclusive}.
      \vskip1ex
      \emph{Exclusive intervals} are used for calculating \emph{out-of-sample} aggregations over future intervals.
    \column{0.5\textwidth}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Number of data points
> n_rows <- NROW(rutils::etf_env$VTI["2019/"])
> # Number of data points per interval
> n_points <- 21
> # Number of n_pointss that fit over n_rows
> n_agg <- n_rows %/% n_points
> # Define end_p with beginning stub
> end_p <- c(0, n_rows-n_points*n_agg + (0:n_agg)*n_points)
> # Define contiguous start_p
> start_p <- c(0, end_p[1:(NROW(end_p)-1)])
> # Define exclusive start_p
> start_p <- c(0, end_p[1:(NROW(end_p)-1)]+1)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
    \vspace{-2em}
    \includegraphics[width=0.9\paperwidth]{figure/intervals_non_overlapping.png}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Using \texttt{sapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Aggregations performed over time series can be extremely slow if done improperly, therefore it's very important to find the fastest methods of performing aggregations.
      \vskip1ex
      The \texttt{sapply()} functional allows performing aggregations over the look-back \emph{intervals}.
      \vskip1ex
      The \texttt{sapply()} functional by default returns a vector or matrix, not an \emph{xts} series.
      \vskip1ex
      The vector or matrix returned by \texttt{sapply()} therefore needs to be coerced into an \emph{xts} series.
      \vskip1ex
      The variable \texttt{look\_back} is the size of the look-back interval, equal to the number of data points used for applying the aggregation function (including the current point).
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Extract time series of VTI log prices
> clos_e <- log(na.omit(rutils::etf_env$price_s$VTI))
> end_p <- 0:NROW(clos_e)  # End points at each point
> n_rows <- NROW(end_p)
> look_back <- 22  # Number of data points per look-back interval
> # start_p are multi-period lag of end_p
> start_p <- c(rep_len(0, look_back - 1),
+     end_p[1:(n_rows - look_back + 1)])
> # Define list of look-back intervals for aggregations over past
> look_backs <- lapply(2:n_rows, function(in_dex) {
+     start_p[in_dex]:end_p[in_dex]
+ })  # end lapply
> # Define aggregation function
> agg_regate <- function(x_ts) c(max=max(x_ts), min=min(x_ts))
> # Perform aggregations over look_backs list
> agg_s <- sapply(look_backs,
+     function(look_back) agg_regate(clos_e[look_back])
+ )  # end sapply
> # Coerce agg_s into matrix and transpose it
> if (is.vector(agg_s))
+   agg_s <- t(agg_s)
> agg_s <- t(agg_s)
> # Coerce agg_s into xts series
> agg_s <- xts(agg_s, order.by=index(clos_e[end_p]))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Using \texttt{lapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{lapply()} functional allows performing aggregations over the look-back \emph{intervals}.
      \vskip1ex
      The \texttt{lapply()} functional by default returns a list, not an \emph{xts} series.
      \vskip1ex
      If \texttt{lapply()} returns a list of \emph{xts} series, then this list can be collapsed into a single \emph{xts} series using the function \texttt{do\_call\_rbind()} from package \emph{rutils}.
      \vskip1ex
      The function \texttt{chart\_Series()} from package \emph{quantmod} can produce a variety of time series plots.
      \vskip1ex
      \texttt{chart\_Series()} plots can be modified by modifying \emph{plot objects} or \emph{theme objects}.
      \vskip1ex
      A plot \emph{theme object} is a list containing parameters that determine the plot appearance (colors, size, fonts).
      \vskip1ex
      The function \texttt{chart\_theme()} returns the theme object.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Perform aggregations over look_backs list
> agg_s <- lapply(look_backs,
+     function(look_back) agg_regate(clos_e[look_back])
+ )  # end lapply
> # rbind list into single xts or matrix
> agg_s <- rutils::do_call(rbind, agg_s)
> # Convert into xts
> agg_s <- xts::xts(agg_s, order.by=index(clos_e))
> agg_s <- cbind(agg_s, clos_e)
> # Plot aggregations with custom line colors
> plot_theme <- chart_theme()
> plot_theme$col$line.col <- c("black", "red", "green")
> x11(width=6, height=5)
> quantmod::chart_Series(agg_s, theme=plot_theme,
+        name="price aggregations")
> legend("top", legend=colnames(agg_s),
+   bg="white", lty=1, lwd=6,
+   col=plot_theme$col$line.col, bty="n")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Defining Functionals for Rolling Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The functional \texttt{roll\_agg()} performs rolling aggregations of its function argument \texttt{FUN}, over an \emph{xts} series (\texttt{x\_ts}), and a look-back interval (\texttt{look\_back}).
      \vskip1ex
      The argument \texttt{FUN} is an aggregation function over a subset of \texttt{x\_ts} series.
      \vskip1ex
      The dots \texttt{"..."} argument is passed into \texttt{FUN} as additional arguments.
      \vskip1ex
      The argument \texttt{look\_back} is equal to the number of periods of \texttt{x\_ts} series which are passed to the aggregation function \texttt{FUN}.
      \vskip1ex
      The functional \texttt{roll\_agg()} calls \texttt{lapply()}, which loops over the length of series \texttt{x\_ts}.
      \vskip1ex
      Note that two different intervals may be used with \texttt{roll\_agg()}.
      \vskip1ex
      The first interval is the argument \texttt{look\_back}.
      \vskip1ex
      A second interval may be one of the variables bound to the dots \texttt{"..."} argument, and passed to the aggregation function \texttt{FUN} (for example, an \emph{EWMA} window).
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Define functional for rolling aggregations
> roll_agg <- function(x_ts, look_back, FUN, ...) {
+ # Define end points at every period
+   end_p <- 0:NROW(x_ts)
+   n_rows <- NROW(end_p)
+ # Define starting points as lag of end_p
+   start_p <- c(rep_len(0, look_back - 1),
+     end_p[1:(n_rows- look_back + 1)])
+ # Perform aggregations over look_backs list
+   agg_s <- lapply(2:n_rows, function(in_dex)
+     FUN(x_ts[start_p[in_dex]:end_p[in_dex]], ...)
+   )  # end lapply
+ # rbind list into single xts or matrix
+   agg_s <- rutils::do_call(rbind, agg_s)
+ # Coerce agg_s into xts series
+   if (!is.xts(agg_s))
+     agg_s <- xts(agg_s, order.by=index(x_ts))
+   agg_s
+ }  # end roll_agg
> # Define aggregation function
> agg_regate <- function(x_ts)
+   c(max=max(x_ts), min=min(x_ts))
> # Perform aggregations over rolling interval
> agg_s <- roll_agg(clos_e, look_back=look_back, FUN=agg_regate)
> class(agg_s)
> dim(agg_s)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Benchmarking Speed of Rolling Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The speed of rolling aggregations using \texttt{apply()} loops can be greatly increased by simplifying the aggregation function
      \vskip1ex
      For example, an aggregation function that returns a vector is over \texttt{13} times faster than a function that returns an \emph{xts} object.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Define aggregation function that returns a vector
> agg_vector <- function(x_ts)
+   c(max=max(x_ts), min=min(x_ts))
> # Define aggregation function that returns an xts
> agg_xts <- function(x_ts)
+   xts(t(c(max=max(x_ts), min=min(x_ts))), order.by=end(x_ts))
> # Benchmark the speed of aggregation functions
> library(microbenchmark)
> summary(microbenchmark(
+   agg_vector=roll_agg(clos_e, look_back=look_back, FUN=agg_vector),
+   agg_xts=roll_agg(clos_e, look_back=look_back, FUN=agg_xts),
+   times=10))[, c(1, 4, 5)]
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Benchmarking Functionals for Rolling Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.45\textwidth}
      Several packages contain functionals designed for performing rolling aggregations:
      \begin{itemize}
        \item \texttt{rollapply.zoo()} from package \emph{zoo},
        \item \texttt{rollapply.xts()} from package \emph{xts},
        \item \texttt{apply.rolling()} from package \emph{PerformanceAnalytics},
      \end{itemize}
      These functionals don't require specifying the \emph{end points}, and instead calculate the \emph{end points} from the rolling interval width.
      \vskip1ex
      These functionals can only apply functions that return a single value, not a vector.
      \vskip1ex
      These functionals return an \emph{xts} series with leading \texttt{NA} values at points before the rolling interval can fit over the data.
      \vskip1ex
      The argument \texttt{align="right"} of \texttt{rollapply()} determines that aggregations are taken from the past.
      \vskip1ex
      The functional \texttt{rollapply.xts} is the fastest, about as fast as performing an \texttt{lapply()} loop directly.
    \column{0.55\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Define aggregation function that returns a single value
> agg_regate <- function(x_ts)  max(x_ts)
> # Perform aggregations over a rolling interval
> agg_s <- xts:::rollapply.xts(clos_e, width=look_back,
+               FUN=agg_regate, align="right")
> # Perform aggregations over a rolling interval
> library(PerformanceAnalytics)  # Load package PerformanceAnalytics
> agg_s <- apply.rolling(clos_e, width=look_back, FUN=agg_regate)
> # Benchmark the speed of the functionals
> library(microbenchmark)
> summary(microbenchmark(
+   roll_agg=roll_agg(clos_e, look_back=look_back, FUN=max),
+   roll_xts=xts:::rollapply.xts(clos_e, width=look_back, FUN=max, align="right"),
+   apply_rolling=apply.rolling(clos_e, width=look_back, FUN=max),
+   times=10))[, c(1, 4, 5)]
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}

%%%%%%%%%%%%%%%
\subsection{Rolling Aggregations Using \protect\emph{Vectorized} Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The generic functions \texttt{cumsum()}, \texttt{cummax()}, and \texttt{cummin()} return the cumulative sums, minima, and maxima of \emph{vectors} and \emph{time series} objects.
      \vskip1ex
      The methods for these functions are implemented as \emph{vectorized compiled} functions, and are therefore much faster than \texttt{apply()} loops.
      \vskip1ex
      The \texttt{cumsum()} function can be used to efficiently calculate the rolling sum of an an \emph{xts} series.
      \vskip1ex
      Using the function \texttt{cumsum()} is over \texttt{25} times faster than using \texttt{apply()} loops.
      \vskip1ex
      But rolling volatilities and higher moments can't be easily calculated using \texttt{cumsum()}.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Rolling sum using cumsum()
> roll_sum <- function(x_ts, look_back) {
+   cum_sum <- cumsum(na.omit(x_ts))
+   out_put <- cum_sum - rutils::lag_it(x=cum_sum, lagg=look_back)
+   out_put[1:look_back, ] <- cum_sum[1:look_back, ]
+   colnames(out_put) <- paste0(colnames(x_ts), "_stdev")
+   out_put
+ }  # end roll_sum
> agg_s <- roll_sum(clos_e, look_back=look_back)
> # Perform rolling aggregations using lapply loop
> agg_s <- lapply(2:n_rows, function(in_dex)
+     sum(clos_e[start_p[in_dex]:end_p[in_dex]])
+ )  # end lapply
> # rbind list into single xts or matrix
> agg_s <- rutils::do_call(rbind, agg_s)
> head(agg_s)
> tail(agg_s)
> # Benchmark the speed of both methods
> library(microbenchmark)
> summary(microbenchmark(
+   roll_sum=roll_sum(clos_e, look_back=look_back),
+   s_apply=sapply(look_backs,
+     function(look_back) sum(clos_e[look_back])),
+   times=10))[, c(1, 4, 5)]
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Filtering Time Series Using Function \texttt{filter()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{filter()} applies a linear filter to time series, vectors, and matrices, and returns a time series of class \texttt{"ts"}.
      \vskip1ex
      The function \texttt{filter()} with the argument \texttt{method="convolution"} calculates the \emph{convolution} of the vector $r_i$ with the filter $\varphi_i$:
      \begin{displaymath}
        f_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p}
      \end{displaymath}
      Where $f_i$ is the filtered output vector, and $\varphi_i$ are the filter coefficients.
      \vskip1ex
      \texttt{filter()} with \texttt{method="recursive"} calculates a \emph{recursive} filter over the vector of random \emph{innovations} $\xi_i$ as follows:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i
      \end{displaymath}
      Where $r_i$ is the filtered output vector, and $\varphi_i$ are the filter coefficients.
      \vskip1ex
      The \emph{recursive} filter describes an \emph{AR(p)} process, which is a special case of an \emph{ARIMA} process.
      \vskip1ex
      \texttt{filter()} is very fast because it calculates the filter by calling compiled \texttt{C++} functions.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Extract time series of VTI log prices
> clos_e <- log(na.omit(rutils::etf_env$price_s$VTI))
> # Calculate EWMA prices using filter()
> look_back <- 21
> weight_s <- exp(-0.1*1:look_back)
> weight_s <- weight_s/sum(weight_s)
> filter_ed <- stats::filter(clos_e, filter=weight_s,
+                    method="convolution", sides=1)
> filter_ed <- as.numeric(filter_ed)
> # filter() returns time series of class "ts"
> class(filter_ed)
> # Filter using compiled C++ function directly
> getAnywhere(C_cfilter)
> str(stats:::C_cfilter)
> filter_fast <- .Call(stats:::C_cfilter, clos_e, 
+                filter=weight_s, sides=1, circular=FALSE)
> all.equal(as.numeric(filter_ed), filter_fast, check.attributes=FALSE)
> # Calculate EWMA prices using roll::roll_sum()
> weights_rev <- rev(weight_s)
> roll_ed <- roll::roll_sum(clos_e, width=look_back, weights=weights_rev, min_obs=1)
> all.equal(filter_ed[-(1:look_back)],
+     as.numeric(roll_ed)[-(1:look_back)],
+     check.attributes=FALSE)
> # Benchmark speed of rolling calculations
> library(microbenchmark)
> summary(microbenchmark(
+   filter=filter(clos_e, filter=weight_s, method="convolution", sides=1),
+   filter_fast=.Call(stats:::C_cfilter, clos_e, filter=weight_s, sides=1, circular=FALSE),
+   cum_sum=cumsum(clos_e),
+   roll=roll::roll_sum(clos_e, width=look_back, weights=weights_rev)
+   ), times=10)[, c(1, 4, 5)]
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Using Package \protect\emph{TTR}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{TTR} contains functions for calculating rolling aggregations over \emph{vectors} and \emph{time series} objects:
      \begin{itemize}
        \item \texttt{runSum()} for rolling sums,
        \item \texttt{runMin()} and \texttt{runMax()} for rolling minima and maxima,
        \item \texttt{runSD()} for rolling standard deviations,
        \item \texttt{runMedian()} and \texttt{runMAD()} for rolling medians and Median Absolute Deviations (\emph{MAD}),
        \item \texttt{runCor()} for rolling correlations,
      \end{itemize}
      The rolling \emph{TTR} functions are much faster than performing \texttt{apply()} loops, because they are \emph{compiled} functions (compiled from \texttt{C++} or \texttt{Fortran} code).
      \vskip1ex
      But the rolling \emph{TTR} functions are a little slower than using \emph{vectorized compiled} functions such as \texttt{cumsum()}.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate the rolling maximum and minimum over a vector of data
> roll_maxminr <- function(vec_tor, look_back) {
+   n_rows <- NROW(vec_tor)
+   max_min <- matrix(numeric(2*n_rows), nc=2)
+   # Loop over periods
+   for (it in 1:n_rows) {
+     sub_vec <- vec_tor[max(1, it-look_back+1):it]
+     max_min[it, 1] <- max(sub_vec)
+     max_min[it, 2] <- min(sub_vec)
+   }  # end for
+   return(max_min)
+ }  # end roll_maxminr
> max_minr <- roll_maxminr(clos_e, look_back)
> max_minr <- xts::xts(max_minr, index(clos_e))
> library(TTR)  # Load package TTR
> max_min <- cbind(TTR::runMax(x=clos_e, n=look_back),
+            TTR::runMin(x=clos_e, n=look_back))
> all.equal(max_min[-(1:look_back), ], max_minr[-(1:look_back), ], check.attributes=FALSE)
> # Benchmark the speed of TTR::runMax
> library(microbenchmark)
> summary(microbenchmark(
+   pure_r=roll_maxminr(clos_e, look_back),
+   ttr=TTR::runMax(clos_e, n=look_back),
+   times=10))[, c(1, 4, 5)]
> # Benchmark the speed of TTR::runSum
> summary(microbenchmark(
+   vector_r=cumsum(coredata(clos_e)),
+   rutils=rutils::roll_sum(clos_e, look_back=look_back),
+   ttr=TTR::runSum(clos_e, n=look_back),
+   times=10))[, c(1, 4, 5)]
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling \protect\emph{Weighted} Aggregations Using Package \protect\emph{roll}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{roll} contains functions for calculating \emph{weighted} rolling aggregations over \emph{vectors} and \emph{time series} objects:
      \begin{itemize}
        \item \texttt{roll\_sum()}, \texttt{roll\_max()}, \texttt{roll\_mean()}, and \texttt{roll\_median()} for \emph{weighted} rolling sums, maximums, means, and medians,
        \item \texttt{roll\_var()} for \emph{weighted} rolling variance,
        \item \texttt{roll\_scale()} for rolling scaling and centering of time series,
        \item \texttt{roll\_lm()} for rolling regression,
        \item \texttt{roll\_pcr()} for rolling principal component regressions of time series,
      \end{itemize}
      The \emph{roll} functions are about \texttt{1,000} times faster than \texttt{apply()} loops!
      \vskip1ex
      The \emph{roll} functions are extremely fast because they perform calculations in \emph{parallel} in compiled \texttt{C++} code, using packages \emph{Rcpp} and \emph{RcppArmadillo}.
      \vskip1ex
      The \emph{roll} functions accept \emph{xts} time series, and they return \emph{xts}.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate rolling VTI variance using package roll
> library(roll)  # Load roll
> re_turns <- na.omit(rutils::etf_env$re_turns[, "VTI"])
> look_back <- 22
> # Calculate rolling sum using RcppRoll
> sum_roll <- roll::roll_sum(re_turns, width=look_back, min_obs=1)
> # Calculate rolling sum using rutils
> sum_rutils <- rutils::roll_sum(re_turns, look_back=look_back)
> all.equal(sum_roll[-(1:look_back), ], 
+     sum_rutils[-(1:look_back), ], check.attributes=FALSE)
> # Benchmark speed of rolling calculations
> library(microbenchmark)
> summary(microbenchmark(
+   cum_sum=cumsum(re_turns),
+   roll=roll::roll_sum(re_turns, width=look_back),
+   RcppRoll=RcppRoll::roll_sum(re_turns, n=look_back),
+   rutils=rutils::roll_sum(re_turns, look_back=look_back),
+   times=10))[, c(1, 4, 5)]
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Rolling \protect\emph{Weighted} Aggregations Using Package \protect\emph{RcppRoll}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{RcppRoll} contains functions for calculating \emph{weighted} rolling aggregations over \emph{vectors} and \emph{time series} objects:
      \begin{itemize}
        \item \texttt{roll\_sum()} for \emph{weighted} rolling sums,
        \item \texttt{roll\_min()} and \texttt{roll\_max()} for \emph{weighted} rolling minima and maxima,
        \item \texttt{roll\_sd()} for \emph{weighted} rolling standard deviations,
        \item \texttt{roll\_median()} for \emph{weighted} rolling medians,
      \end{itemize}
      The \emph{RcppRoll} functions accept \emph{xts} objects, but they return matrices, not \emph{xts} objects.
      \vskip1ex
      The rolling \emph{RcppRoll} functions are much faster than performing \texttt{apply()} loops, because they are \emph{compiled} functions (compiled from \texttt{C++} code).
      \vskip1ex
      But the rolling \emph{RcppRoll} functions are a little slower than using \emph{vectorized compiled} functions such as \texttt{cumsum()}.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(RcppRoll)  # Load package RcppRoll
> # Calculate rolling sum using RcppRoll
> sum_roll <- RcppRoll::roll_sum(re_turns, align="right", n=look_back)
> # Calculate rolling sum using rutils
> sum_rutils <- rutils::roll_sum(re_turns, look_back=look_back)
> all.equal(sum_roll, coredata(sum_rutils[-(1:(look_back-1))]), 
+     check.attributes=FALSE)
> # Benchmark speed of rolling calculations
> library(microbenchmark)
> summary(microbenchmark(
+   cum_sum=cumsum(re_turns),
+   RcppRoll=RcppRoll::roll_sum(re_turns, n=look_back),
+   rutils=rutils::roll_sum(re_turns, look_back=look_back),
+   times=10))[, c(1, 4, 5)]
> # Calculate EWMA prices using RcppRoll
> clos_e <- quantmod::Cl(rutils::etf_env$VTI)
> weight_s <- exp(0.1*1:look_back)
> prices_ewma <- RcppRoll::roll_mean(clos_e,
+ align="right", n=look_back, weights=weight_s)
> prices_ewma <- cbind(clos_e,
+   rbind(coredata(clos_e[1:(look_back-1), ]), prices_ewma))
> colnames(prices_ewma) <- c("VTI", "VTI EWMA")
> # Plot an interactive dygraph plot
> dygraphs::dygraph(prices_ewma)
> # Or static plot of EWMA prices with custom line colors
> x11(width=6, height=5)
> plot_theme <- chart_theme()
> plot_theme$col$line.col <- c("black", "red")
> quantmod::chart_Series(prices_ewma, theme=plot_theme, name="EWMA prices")
> legend("top", legend=colnames(prices_ewma),
+  bg="white", lty=1, lwd=6,
+  col=plot_theme$col$line.col, bty="n")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Using Package \protect\emph{caTools}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{caTools} contains functions for calculating rolling interval aggregations over a \texttt{vector} of data:
      \begin{itemize}
        \item \texttt{runmin()} and \texttt{runmax()} for rolling minima and maxima,
        \item \texttt{runsd()} for rolling standard deviations,
        \item \texttt{runmad()} for rolling Median Absolute Deviations (\emph{MAD}),
        \item \texttt{runquantile()} for rolling quantiles,
      \end{itemize}
      Time series need to be coerced to \emph{vectors} before they are passed to \emph{caTools} functions.
      \vskip1ex
      The rolling \emph{caTools} functions are very fast because they are \emph{compiled} functions (compiled from \texttt{C++} code).
      \vskip1ex
      The argument \texttt{"endrule"} determines how the end values of the data are treated.
      \vskip1ex
      The argument \texttt{"align"} determines whether the interval is centered (default), left-aligned or right-aligned, with \texttt{align="center"} the fastest option.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(caTools)  # Load package "caTools"
> # Get documentation for package "caTools"
> packageDescription("caTools")  # Get short description
> help(package="caTools")  # Load help page
> data(package="caTools")  # List all datasets in "caTools"
> ls("package:caTools")  # List all objects in "caTools"
> detach("package:caTools")  # Remove caTools from search path
> # Median filter
> look_back <- 2
> clos_e <- quantmod::Cl(HighFreq::SPY["2012-02-01/2012-04-01"])
> med_ian <- runmed(x=clos_e, k=look_back)
> # Vector of rolling volatility
> sigma_r <- runsd(x=clos_e, k=look_back,
+           endrule="constant", align="center")
> # Vector of rolling quantiles
> quan_tiles <- runquantile(x=clos_e, k=look_back,
+   probs=0.9, endrule="constant", align="center")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Using \protect\emph{RcppArmadillo}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{RcppArmadillo} functions for calculating rolling aggregations are often the fastest.
      \begin{lstlisting}[language=R,basicstyle=\tiny\ttfamily\bfseries,backgroundcolor=\color{anti_flashwhite},showstringspaces=FALSE]
// Rcpp header with information for C++ compiler
#include <RcppArmadillo.h> // include C++ header file from Armadillo library
using namespace arma; // use C++ namespace from Armadillo library
// declare dependency on RcppArmadillo
// [[Rcpp::depends(RcppArmadillo)]]

// export the function roll_maxmin() to R
// [[Rcpp::export]]
arma::mat roll_maxmin(const arma::vec& vec_tor,
                      const arma::uword& look_back) {
  arma::uword n_rows = vec_tor.size();
  arma::mat max_min(n_rows, 2);
  arma::vec sub_vec;
  // startup period
  max_min(0, 0) = vec_tor[0];
  max_min(0, 1) = vec_tor[0];
  for (uword it = 1; it < look_back; it++) {
    sub_vec = vec_tor.subvec(0, it);
    max_min(it, 0) = sub_vec.max();
    max_min(it, 1) = sub_vec.min();
  }  // end for
  // remaining periods
  for (uword it = look_back; it < n_rows; it++) {
    sub_vec = vec_tor.subvec(it- look_back + 1, it);
    max_min(it, 0) = sub_vec.max();
    max_min(it, 1) = sub_vec.min();
  }  // end for
  return max_min;
}  // end roll_maxmin
    \end{lstlisting}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/rolling_maxmin.png}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Compile Rcpp functions
> Rcpp::sourceCpp(file="C:/Develop/R/Rcpp/roll_maxmin.cpp")
> max_minarma <- roll_maxmin(clos_e, look_back)
> max_minarma <- xts::xts(max_minr, index(clos_e))
> max_min <- cbind(TTR::runMax(x=clos_e, n=look_back),
+            TTR::runMin(x=clos_e, n=look_back))
> all.equal(max_min[-(1:look_back), ], max_minarma[-(1:look_back), ], check.attributes=FALSE)
> # Benchmark the speed of TTR::runMax
> library(microbenchmark)
> summary(microbenchmark(
+   arma=roll_maxmin(clos_e, look_back),
+   ttr=TTR::runMax(clos_e, n=look_back),
+   times=10))[, c(1, 4, 5)]
> # Dygraphs plot with max_min lines
> da_ta <- cbind(clos_e, max_minarma)
> colnames(da_ta)[2:3] <- c("max", "min")
> col_ors <- c("blue", "red", "green")
> dygraphs::dygraph(da_ta, main=paste(colnames(clos_e), "max and min lines")) %>%
+   dyOptions(colors=col_ors)
> # Standard plot with max_min lines
> plot_theme <- chart_theme()
> plot_theme$col$line.col <- col_ors
> quantmod::chart_Series(da_ta["2008/2009"], theme=plot_theme,
+   name=paste(colnames(clos_e), "max and min lines"))
> legend(x="topright", title=NULL, legend=colnames(da_ta),
+  inset=0.1, cex=0.9, bg="white", bty="n",
+  lwd=6, lty=1, col=col_ors)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Determining Calendar \protect\emph{end points} of \protect\emph{xts} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{xts::endpoints()} extracts the indices of the last observations in each calendar period of an \emph{xts} series.
      \vskip1ex
      For example:\\ \-\ \texttt{endpoints(x, on="hours")}\\
      extracts the indices of the last observations in each hour.
      \vskip1ex
      The \emph{end points} calculated by \texttt{endpoints()} aren't always equally spaced, and aren't the same as those calculated from fixed intervals.
      \vskip1ex
      For example, the last observations in each day aren't equally spaced due to weekends and holidays.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Indices of last observations in each hour
> end_p <- xts::endpoints(clos_e, on="hours")
> head(end_p)
> # extract the last observations in each hour
> head(clos_e[end_p, ])
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Non-overlapping Aggregations Using \texttt{sapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{apply()} functionals allow for applying a function over intervals of an \emph{xts} series defined by a vector of \emph{end points}.
      \vskip1ex
      The \texttt{sapply()} functional by default returns a vector or matrix, not an \emph{xts} series.
      \vskip1ex
      The vector or matrix returned by \texttt{sapply()} therefore needs to be coerced into an \emph{xts} series.
      \vskip1ex
      The function \texttt{chart\_Series()} from package \emph{quantmod} can produce a variety of time series plots.
      \vskip1ex
      \texttt{chart\_Series()} plots can be modified by modifying \emph{plot objects} or \emph{theme objects}.
      \vskip1ex
      A plot \emph{theme object} is a list containing parameters that determine the plot appearance (colors, size, fonts).
      \vskip1ex
      The function \texttt{chart\_theme()} returns the theme object.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Extract time series of VTI log prices
> clos_e <- log(na.omit(rutils::etf_env$price_s$VTI))
> # Number of data points
> n_rows <- NROW(clos_e)
> # Number of data points per interval
> look_back <- 22
> # Number of look_backs that fit over n_rows
> n_agg <- n_rows %/% look_back
> # Define end_p with beginning stub
> end_p <- c(0, n_rows-look_back*n_agg + (0:n_agg)*look_back)
> # Define contiguous start_p
> start_p <- c(0, end_p[1:(NROW(end_p)-1)])
> # Define list of look-back intervals for aggregations over past
> look_backs <- lapply(2:NROW(end_p), function(in_dex) {
+     start_p[in_dex]:end_p[in_dex]
+ })  # end lapply
> look_backs[[1]]
> look_backs[[2]]
> # Perform sapply() loop over look_backs list
> agg_s <- sapply(look_backs, function(look_back) {
+   x_ts <- clos_e[look_back]
+   c(max=max(x_ts), min=min(x_ts))
+ })  # end sapply
> # Coerce agg_s into matrix and transpose it
> if (is.vector(agg_s))
+   agg_s <- t(agg_s)
> agg_s <- t(agg_s)
> # Coerce agg_s into xts series
> agg_s <- xts(agg_s, order.by=index(clos_e[end_p]))
> head(agg_s)
> # Plot aggregations with custom line colors
> plot_theme <- chart_theme()
> plot_theme$col$line.col <- c("red", "green")
> quantmod::chart_Series(agg_s, theme=plot_theme,
+        name="price aggregations")
> legend("top", legend=colnames(agg_s),
+   bg="white", lty=1, lwd=6,
+   col=plot_theme$col$line.col, bty="n")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Non-overlapping Aggregations Using \texttt{lapply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \texttt{apply()} functionals allow for applying a function over intervals of an \emph{xts} series defined by a vector of \emph{end points}.
      \vskip1ex
      The \texttt{lapply()} functional by default returns a list, not an \emph{xts} series.
      \vskip1ex
      If \texttt{lapply()} returns a list of \emph{xts} series, then this list can be collapsed into a single \emph{xts} series using the function \texttt{do\_call\_rbind()} from package \emph{rutils}.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Perform lapply() loop over look_backs list
> agg_s <- lapply(look_backs, function(look_back) {
+   x_ts <- clos_e[look_back]
+   c(max=max(x_ts), min=min(x_ts))
+ })  # end lapply
> # rbind list into single xts or matrix
> agg_s <- rutils::do_call(rbind, agg_s)
> # Coerce agg_s into xts series
> agg_s <- xts(agg_s, order.by=index(clos_e[end_p]))
> head(agg_s)
> # Plot aggregations with custom line colors
> plot_theme <- chart_theme()
> plot_theme$col$line.col <- c("red", "green")
> quantmod::chart_Series(agg_s, theme=plot_theme, name="price aggregations")
> legend("top", legend=colnames(agg_s),
+   bg="white", lty=1, lwd=6,
+   col=plot_theme$col$line.col, bty="n")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Interval Aggregations Using \texttt{period.apply()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The functional \texttt{period.apply()} from package \emph{xts} performs \emph{aggregations} over non-overlapping intervals of an \emph{xts} series defined by a vector of \emph{end points}.
      \vskip1ex
      Internally \texttt{period.apply()} performs an \texttt{sapply()} loop, and is therefore about as fast as an \texttt{sapply()} loop.
      \vskip1ex
      The package \emph{xts} also has several specialized functionals for aggregating data over \emph{end points}:
      \begin{itemize}
        \item \texttt{period.sum()} calculate the sum for each period,
        \item \texttt{period.max()} calculate the maximum for each period,
        \item \texttt{period.min()} calculate the minimum for each period,
        \item \texttt{period.prod()} calculate the product for each period,
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Define functional for rolling aggregations over end_p
> roll_agg <- function(x_ts, end_p, FUN, ...) {
+   n_rows <- NROW(end_p)
+ # start_p are single-period lag of end_p
+   start_p <- c(1, end_p[1:(n_rows-1)])
+ # Perform aggregations over look_backs list
+   agg_s <- lapply(look_backs,
+     function(look_back) FUN(x_ts[look_back], ...))  # end lapply
+ # rbind list into single xts or matrix
+   agg_s <- rutils::do_call(rbind, agg_s)
+   if (!is.xts(agg_s))
+     agg_s <-  # Coerce agg_s into xts series
+     xts(agg_s, order.by=index(x_ts[end_p]))
+   agg_s
+ }  # end roll_agg
> # Apply sum() over end_p
> agg_s <- roll_agg(clos_e, end_p=end_p, FUN=sum)
> agg_s <- period.apply(clos_e, INDEX=end_p, FUN=sum)
> # Benchmark the speed of aggregation functions
> summary(microbenchmark(
+   roll_agg=roll_agg(clos_e, end_p=end_p, FUN=sum),
+   period_apply=period.apply(clos_e, INDEX=end_p, FUN=sum),
+   times=10))[, c(1, 4, 5)]
> agg_s <- period.sum(clos_e, INDEX=end_p)
> head(agg_s)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Aggregations of \protect\emph{xts} Over Calendar Periods}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{xts} has convenience wrapper functionals for \texttt{period.apply()}, that apply functions over calendar periods:
      \begin{itemize}
        \item \texttt{apply.daily()} applies functions over daily periods,
        \item \texttt{apply.weekly()} applies functions over weekly periods,
        \item \texttt{apply.monthly()} applies functions over monthly periods,
        \item \texttt{apply.quarterly()} applies functions over quarterly periods,
        \item \texttt{apply.yearly()} applies functions over yearly periods,
      \end{itemize}
      These functionals don't require specifying a vector of \emph{end points}, because they determine the \emph{end points} from the calendar periods.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Load package HighFreq
> library(HighFreq)
> # Extract closing minutely prices
> clos_e <- quantmod::Cl(rutils::etf_env$VTI["2019"])
> # Apply "mean" over daily periods
> agg_s <- apply.daily(clos_e, FUN=sum)
> head(agg_s)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Aggregations Over Overlapping Intervals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The functional \texttt{period.apply()} performs aggregations over \emph{non-overlapping} intervals.
      \vskip1ex
      But it's often necessary to perform aggregations over \emph{overlapping} intervals, defined by a vector of \emph{end points} and a \emph{look-back interval}.
      \vskip1ex
      The \emph{start points} are defined as the \emph{end points} lagged by the interval width (number of periods in the \emph{look-back interval}).
      \vskip1ex
      Each point in time has an associated \emph{look-back interval}, which starts at a certain number of periods in the past (\emph{start\_point}) and ends at that point (\emph{end\_point}).
      \vskip1ex
      The variable \texttt{look\_back} is equal to the number of end points in the \emph{look-back interval}, while (\texttt{look\_back - 1}) is equal to the number of intervals in the look-back.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Define end_p with beginning stub
> n_points <- 5
> n_rows <- NROW(clos_e)
> n_agg <- n_rows %/% n_points
> end_p <- c(0, n_rows-n_points*n_agg + (0:n_agg)*n_points)
> # Number of data points in look_back interval
> look_back <- 22
> # start_p are end_p lagged by look_back
> start_p <- (end_p - look_back + 1)
> start_p <- ifelse(start_p < 0, 0, start_p)
> # Perform lapply() loop over look_backs list
> agg_s <- lapply(2:NROW(end_p), function(in_dex) {
+ x_ts <- clos_e[start_p[in_dex]:end_p[in_dex]]
+ c(max=max(x_ts), min=min(x_ts))
+ })  # end lapply
> # rbind list into single xts or matrix
> agg_s <- rutils::do_call(rbind, agg_s)
> # Coerce agg_s into xts series
> agg_s <- xts(agg_s, order.by=index(clos_e[end_p]))
> # Plot aggregations with custom line colors
> plot_theme <- chart_theme()
> plot_theme$col$line.col <- c("red", "green")
> quantmod::chart_Series(agg_s, theme=plot_theme,
+        name="price aggregations")
> legend("top", legend=colnames(agg_s),
+   bg="white", lty=1, lwd=6,
+   col=plot_theme$col$line.col, bty="n")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Extending Interval Aggregations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Interval aggregations produce values only at the \emph{end points}, but they can be carried forward in time using the function \texttt{na.locf.xts()} from package \emph{xts}.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> agg_s <- cbind(clos_e, agg_s)
> tail(agg_s)
> agg_s <- na.omit(xts:::na.locf.xts(agg_s))
> # Plot aggregations with custom line colors
> plot_theme <- chart_theme()
> plot_theme$col$line.col <- c("black", "red", "green")
> quantmod::chart_Series(agg_s, theme=plot_theme, name="price aggregations")
> legend("top", legend=colnames(agg_s),
+   bg="white", lty=1, lwd=6,
+   col=plot_theme$col$line.col, bty="n")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/agg_interval_carryfwd.png}\\
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Interval Aggregations of \protect\emph{zoo} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The method \texttt{aggregate.zoo()} performs aggregations of \emph{zoo} series over non-overlapping intervals defined by a vector of aggregation groups (minutes, hours, days, etc.).
      \vskip1ex
      For example, \texttt{aggregate.zoo()} can calculate the average monthly returns.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Create zoo time series of random returns
> date_s <- Sys.Date() + 0:365
> zoo_series <- zoo(rnorm(NROW(date_s)), order.by=date_s)
> # Create monthly dates
> dates_agg <- as.Date(as.yearmon(index(zoo_series)))
> # Perform monthly mean aggregation
> zoo_agg <- aggregate(zoo_series, by=dates_agg, FUN=mean)
> # Merge with original zoo - union of dates
> zoo_agg <- cbind(zoo_series, zoo_agg)
> # Replace NA's using locf
> zoo_agg <- na.locf(zoo_agg, na.rm=FALSE)
> # Extract aggregated zoo
> zoo_agg <- zoo_agg[index(zoo_series), 2]
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/zoo_agg-1}
      \vspace{-7em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot original and aggregated cumulative returns
> plot(cumsum(zoo_series), xlab="", ylab="")
> lines(cumsum(zoo_agg), lwd=2, col="red")
> # Add legend
> legend("topright", inset=0.05, cex=0.8, bty="n",
+  title="Aggregated Prices",
+  leg=c("orig prices", "agg prices"),
+  lwd=2, bg="white", col=c("black", "red"))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Interpolating \protect\emph{zoo} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{zoo} has two functions for replacing \texttt{NA} values using interpolation:
      \begin{itemize}
        \item \texttt{na.approx()} performs linear interpolation,
        \item \texttt{na.spline()} performs spline interpolation,
      \end{itemize}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Perform monthly mean aggregation
> zoo_agg <- aggregate(zoo_series, by=dates_agg, FUN=mean)
> # Merge with original zoo - union of dates
> zoo_agg <- cbind(zoo_series, zoo_agg)
> # Replace NA's using linear interpolation
> zoo_agg <- na.approx(zoo_agg)
> # Extract interpolated zoo
> zoo_agg <- zoo_agg[index(zoo_series), 2]
> # Plot original and interpolated zoo
> plot(cumsum(zoo_series), xlab="", ylab="")
> lines(cumsum(zoo_agg), lwd=2, col="red")
> # Add legend
> legend("topright", inset=0.05, cex=0.8, title="Interpolated Prices",
+  leg=c("orig prices", "interpol prices"), lwd=2, bg="white",
+  col=c("black", "red"), bty="n")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/zoo_interpol-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Performing Rolling Aggregations Over \protect\emph{zoo} Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{zoo} has several functions for rolling calculations:
      \begin{itemize}
        \item \texttt{rollapply()} performing aggregations over a rolling (sliding) interval,
        \item \texttt{rollmean()} calculating rolling means,
        \item \texttt{rollmedian()} calculating rolling median,
        \item \texttt{rollmax()} calculating rolling max,
      \end{itemize}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # "mean" aggregation over interval with width=11
> zoo_mean <- rollapply(zoo_series, width=11,
+                 FUN=mean, align="right")
> # Merge with original zoo - union of dates
> zoo_mean <- cbind(zoo_series, zoo_mean)
> # Replace NA's using na.locf
> zoo_mean <- na.locf(zoo_mean, na.rm=FALSE, fromLast=TRUE)
> # Extract mean zoo
> zoo_mean <- zoo_mean[index(zoo_series), 2]
> # Plot original and interpolated zoo
> plot(cumsum(zoo_series), xlab="", ylab="")
> lines(cumsum(zoo_mean), lwd=2, col="red")
> # Add legend
> legend("topright", inset=0.05, cex=0.8, title="Mean Prices",
+  leg=c("orig prices", "mean prices"), lwd=2, bg="white",
+  col=c("black", "red"), bty="n")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/zoo_roll-1}
      \vspace{-3em}
      The argument \texttt{align="right"} determines that aggregations are taken from the past,
  \end{columns}
\end{block}

\end{frame}


\end{document}
