# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' Apply a lag to a single-column \emph{time series} or a \emph{vector} 
#' using \code{RcppArmadillo}.
#' 
#' @param \code{tseries} A single-column \emph{time series} or a
#'   \emph{vector}.
#'
#' @param \code{lagg} An \emph{integer} equal to the number of periods to lag.
#'   (The default is \code{lagg = 1}.)
#'
#' @param \code{pad_zeros} \emph{Boolean} argument: Should the output be padded
#'   with zeros? (The default is \code{pad_zeros = TRUE}.)
#'
#' @return A column \emph{vector} with the same number of elements as the input
#'   time series.
#'
#' @details
#'   The function \code{lag_vec()} applies a lag to the input \emph{time
#'   series} \code{tseries} by shifting its elements by the number equal to the
#'   argument \code{lagg}.  For positive \code{lagg} values, the elements are
#'   shifted forward in time (down), and for negative \code{lagg} values they
#'   are shifted backward (up).
#'   
#'   The output \emph{vector} is padded with either zeros (the default), or
#'   with data from \code{tseries}, so that it has the same number of element
#'   as \code{tseries}.
#'   If the \code{lagg} is positive, then the first element is copied and added
#'   upfront.
#'   If the \code{lagg} is negative, then the last element is copied and added
#'   to the end.
#'   
#'   As a rule, if \code{tseries} contains returns data, then the output
#'   \emph{matrix} should be padded with zeros, to avoid data snooping.
#'   If \code{tseries} contains prices, then the output \emph{matrix} should
#'   be padded with the prices.
#'
#' @examples
#' \dontrun{
#' # Create a vector of random returns
#' returns <- rnorm(1e6)
#' # Compare lag_vec() with rutils::lagit()
#' all.equal(drop(HighFreq::lag_vec(returns)), 
#'   rutils::lagit(returns))
#' # Compare the speed of RcppArmadillo with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::lag_vec(returns),
#'   Rcode=rutils::lagit(returns),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' 
#' @export
lag_vec <- function(tseries, lagg = 1L, pad_zeros = TRUE) {
    .Call('_HighFreq_lag_vec', PACKAGE = 'HighFreq', tseries, lagg, pad_zeros)
}

#' Apply a lag to the rows of a \emph{time series} or a \emph{matrix} using
#' \code{RcppArmadillo}.
#' 
#' @param \code{tseries} A \emph{time series} or a \emph{matrix}.
#' 
#' @param \code{lagg} An \emph{integer} equal to the number of periods to lag
#'   (the default is \code{lagg = 1}).
#'
#' @param \code{pad_zeros} \emph{Boolean} argument: Should the output be padded
#'   with zeros? (The default is \code{pad_zeros = TRUE}.)
#'   
#' @return A \emph{matrix} with the same dimensions as the input argument
#'   \code{tseries}.
#'
#' @details
#'   The function \code{lagit()} applies a lag to the input \emph{matrix} by
#'   shifting its rows by the number equal to the argument \code{lagg}. For
#'   positive \code{lagg} values, the rows are shifted \emph{forward} (down),
#'   and for negative \code{lagg} values they are shifted \emph{backward} (up).
#'   
#'   The output \emph{matrix} is padded with either zeros (the default), or
#'   with rows of data from \code{tseries}, so that it has the same dimensions
#'   as \code{tseries}.
#'   If the \code{lagg} is positive, then the first row is copied and added
#'   upfront.
#'   If the \code{lagg} is negative, then the last row is copied and added
#'   to the end.
#'   
#'   As a rule, if \code{tseries} contains returns data, then the output
#'   \emph{matrix} should be padded with zeros, to avoid data snooping.
#'   If \code{tseries} contains prices, then the output \emph{matrix} should
#'   be padded with the prices.
#'
#' @examples
#' \dontrun{
#' # Create a matrix of random returns
#' returns <- matrix(rnorm(5e6), nc=5)
#' # Compare lagit() with rutils::lagit()
#' all.equal(HighFreq::lagit(returns), rutils::lagit(returns))
#' # Compare the speed of RcppArmadillo with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::lagit(returns),
#'   Rcode=rutils::lagit(returns),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' 
#' @export
lagit <- function(tseries, lagg = 1L, pad_zeros = TRUE) {
    .Call('_HighFreq_lagit', PACKAGE = 'HighFreq', tseries, lagg, pad_zeros)
}

#' Calculate the differences between the neighboring elements of a
#' single-column \emph{time series} or a \emph{vector}.
#' 
#' @param \code{tseries} A single-column \emph{time series} or a \emph{vector}.
#' 
#' @param \code{lagg} An \emph{integer} equal to the number of time periods to
#'   lag when calculating the differences (the default is \code{lagg = 1}).
#'   
#' @param \code{pad_zeros} \emph{Boolean} argument: Should the output
#'   \emph{vector} be padded (extended) with zeros, in order to return a
#'   \emph{vector} of the same length as the input? (the default is
#'   \code{pad_zeros = TRUE})
#'
#' @return A column \emph{vector} containing the differences between the
#'   elements of the input vector.
#'
#' @details
#'   The function \code{diff_vec()} calculates the differences between the
#'   input \emph{time series} or \emph{vector} and its lagged version.
#'   
#'   The argument \code{lagg} specifies the number of lags.  For example, if
#'   \code{lagg=3} then the differences will be taken between each element
#'   minus the element three time periods before it (in the past).  The default
#'   is \code{lagg = 1}.
#' 
#'   The argument \code{pad_zeros} specifies whether the output \emph{vector}
#'   should be padded (extended) with zeros at the front, in order to
#'   return a \emph{vector} of the same length as the input.  The default is
#'   \code{pad_zeros = TRUE}. The padding operation can be time-consuming,
#'   because it requires the copying of data.
#'   
#'   The function \code{diff_vec()} is implemented in \code{RcppArmadillo}
#'   \code{C++} code, which makes it several times faster than \code{R} code.
#'
#' @examples
#' \dontrun{
#' # Create a vector of random returns
#' returns <- rnorm(1e6)
#' # Compare diff_vec() with rutils::diffit()
#' all.equal(drop(HighFreq::diff_vec(returns, lagg=3, pad=TRUE)),
#'   rutils::diffit(returns, lagg=3))
#' # Compare the speed of RcppArmadillo with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::diff_vec(returns, lagg=3, pad=TRUE),
#'   Rcode=rutils::diffit(returns, lagg=3),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' 
#' @export
diff_vec <- function(tseries, lagg = 1L, pad_zeros = TRUE) {
    .Call('_HighFreq_diff_vec', PACKAGE = 'HighFreq', tseries, lagg, pad_zeros)
}

#' Calculate the row differences of a \emph{time series} or a \emph{matrix}
#' using \emph{RcppArmadillo}.
#' 
#' @param \code{tseries} A \emph{time series} or a \emph{matrix}.
#' 
#' @param \code{lagg} An \emph{integer} equal to the number of rows (time
#'   periods) to lag when calculating the differences (the default is
#'   \code{lagg = 1}).
#'   
#' @param \code{pad_zeros} \emph{Boolean} argument: Should the output
#'   \emph{matrix} be padded (extended) with zeros, in order to return a
#'   \emph{matrix} with the same number of rows as the input? (the default is
#'   \code{pad_zeros = TRUE})
#'
#' @return A \emph{matrix} containing the differences between the rows of the
#'   input \emph{matrix} \code{tseries}.
#'
#' @details
#'   The function \code{diffit()} calculates the differences between the rows
#'   of the input \emph{matrix} \code{tseries} and its lagged version.
#'   
#'   The argument \code{lagg} specifies the number of lags applied to the rows
#'   of the lagged version of \code{tseries}. 
#'   For positive \code{lagg} values, the lagged version of \code{tseries} has
#'   its rows shifted \emph{forward} (down) by the number equal to \code{lagg}
#'   rows. For negative \code{lagg} values, the lagged version of
#'   \code{tseries} has its rows shifted \emph{backward} (up) by the number
#'   equal to \code{-lagg} rows.
#'   For example, if \code{lagg=3} then the lagged version will have its rows
#'   shifted down by \code{3} rows, and the differences will be taken between
#'   each row minus the row three time periods before it (in the past). The
#'   default is \code{lagg = 1}.
#' 
#'   The argument \code{pad_zeros} specifies whether the output \emph{matrix}
#'   should be padded (extended) with the rows of the initial (warmup) period
#'   at the front, in order to return a \emph{matrix} with the same number of
#'   rows as the input \code{tseries}.  The default is \code{pad_zeros = TRUE}.
#'   The padding operation can be time-consuming, because it requires the
#'   copying of data.
#'   
#'   The function \code{diffit()} is implemented in \code{RcppArmadillo}
#'   \code{C++} code, which makes it much faster than \code{R} code.
#'
#' @examples
#' \dontrun{
#' # Create a matrix of random data
#' datav <- matrix(sample(15), nc=3)
#' # Calculate differences with lagged rows
#' HighFreq::diffit(datav, lagg=2)
#' # Calculate differences with advanced rows
#' HighFreq::diffit(datav, lagg=-2)
#' # Compare HighFreq::diffit() with rutils::diffit()
#' all.equal(HighFreq::diffit(datav, lagg=2), 
#'   rutils::diffit(datav, lagg=2), 
#'   check.attributes=FALSE)
#' # Compare the speed of RcppArmadillo with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::diffit(datav, lagg=2),
#'   Rcode=rutils::diffit(datav, lagg=2),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' 
#' @export
diffit <- function(tseries, lagg = 1L, pad_zeros = TRUE) {
    .Call('_HighFreq_diffit', PACKAGE = 'HighFreq', tseries, lagg, pad_zeros)
}

#' Calculate a vector of end points that divides a vector into equal intervals.
#'
#' @param \code{length} An \emph{integer} equal to the length of the vector to
#'   be divided into equal intervals.
#'   
#' @param \code{step} The number of elements in each interval between
#'   neighboring end points.
#' 
#' @param \code{stub} An \emph{integer} value equal to the first end point for
#'   calculating the end points.
#'
#' @return A vector of equally spaced \emph{integers} representing the end
#'   points.
#'
#' @details
#'   The end points are a vector of integers which divide a vector of length
#'   equal to \code{length} into equally spaced intervals. If a whole number of
#'   intervals doesn't fit over the vector, then \code{calc_endpoints()} adds a
#'   stub interval at the end.
#'
#'   The first end point is equal to the argument \code{step}, unless the
#'   argument \code{stub} is provided, and then it becomes the first end point.
#'
#'   For example, consider the end points for a vector of length \code{20}
#'   divided into intervals of length \code{step=5}: \code{0, 5, 10, 15, 20}.
#'   In order for all the differences between neighboring end points to be
#'   equal to \code{5}, the first end point is set equal to \code{0}. But
#'   \code{0} doesn't correspond to any vector element, so
#'   \code{calc_endpoints()} doesn't include it and it only retains the
#'   non-zero end points equal to: \code{5, 10, 15, 20}. 
#'
#'   Since indexing in \code{C++} code starts at \code{0}, then
#'   \code{calc_endpoints()} shifts the end points by \code{-1} and returns the
#'   vector equal to \code{4, 9, 14, 19}.
#'
#'   If \code{stub = 1} then the first end point is equal to \code{1} and the
#'   end points are equal to: \code{1, 6, 11, 16, 20}.
#'   The extra stub interval at the end is equal to \code{4 = 20 - 16}.
#'   And \code{calc_endpoints()} returns \code{0, 5, 10, 15, 19}. The first
#'   value is equal to \code{0} which is the index of the first element in
#'   \code{C++} code.
#'
#'   If \code{stub = 2} then the first end point is equal to \code{2}, with an
#'   extra stub interval at the end, and the end points are equal to: \code{2,
#'   7, 12, 17, 20}.
#'   And \code{calc_endpoints()} returns \code{1, 6, 11, 16, 19}.
#'
#'   The function \code{calc_endpoints()} is similar to the function
#'   \code{rutils::calc_endpoints()} from package
#'   \href{https://github.com/algoquant/rutils}{rutils}.
#'   
#'   But the end points are shifted by \code{-1} compared to \code{R} code
#'   because indexing starts at \code{0} in \code{C++} code, while it starts at
#'   \code{1} in \code{R} code. So if \code{calc_endpoints()} is used in
#'   \code{R} code then \code{1} should be added to it.
#'   
#'   This works in \code{R} code because the vector element corresponding to
#'   index \code{0} is empty.  For example, the \code{R} code: \code{(4:1)[c(0,
#'   1)]} produces \code{4}.  So in \code{R} we can select vector elements
#'   using the end points starting at zero.
#'   
#'   In \code{C++} the end points must be shifted by \code{-1} compared to
#'   \code{R} code, because indexing starts at \code{0}: \code{-1, 4, 9, 14,
#'   19}.  But there is no vector element corresponding to index \code{-1}. So
#'   in \code{C++} we cannot select vector elements using the end points
#'   starting at \code{-1}. The solution is to drop the first placeholder end
#'   point.
#'   
#' @examples
#' # Calculate end points without a stub interval
#' HighFreq::calc_endpoints(length=20, step=5)
#' # Calculate end points with a final stub interval
#' HighFreq::calc_endpoints(length=23, step=5)
#' # Calculate end points with initial and final stub intervals
#' HighFreq::calc_endpoints(length=20, step=5, stub=2)
#' # Calculate end points with initial and final stub intervals
#' HighFreq::calc_endpoints(length=20, step=5, stub=24)
#'
#' @export
calc_endpoints <- function(length, step = 1L, stub = 0L) {
    .Call('_HighFreq_calc_endpoints', PACKAGE = 'HighFreq', length, step, stub)
}

#' Calculate a vector of start points by lagging (shifting) a vector of end
#' points.
#'
#' @param \code{endp} An \emph{integer} vector of end points.
#'   
#' @param \code{look_back} The length of the look-back interval, equal to the
#'   lag (shift) applied to the end points.
#'   
#' @return An \emph{integer} vector with the same number of elements as the
#'   vector \code{endp}.
#'
#' @details
#'   The start points are equal to the values of the vector \code{endp} lagged
#'   (shifted) by an amount equal to \code{look_back}.  In addition, an extra
#'   value of \code{1} is added to them, to avoid data overlaps.  The lag
#'   operation requires appending a beginning warmup interval containing zeros,
#'   so that the vector of start points has the same length as the \code{endp}.
#'   
#'   For example, consider the end points for a vector of length \code{25}
#'   divided into equal intervals of length \code{5}: \code{4, 9, 14, 19, 24}.
#'   (In \code{C++} the vector indexing starts at \code{0} not \code{1}, so
#'   it's shifted by \code{-1}.)
#'   Then the start points for \code{look_back = 2} are equal to: \code{0, 0, 
#'   5, 10, 15}.  The differences between the end points minus the
#'   corresponding start points are equal to \code{9}, except for the warmup
#'   interval.
#'   
#' @examples
#' # Calculate end points
#' endp <- HighFreq::calc_endpoints(25, 5)
#' # Calculate start points corresponding to the end points
#' startp <- HighFreq::calc_startpoints(endp, 2)
#'
#' @export
calc_startpoints <- function(endp, look_back) {
    .Call('_HighFreq_calc_startpoints', PACKAGE = 'HighFreq', endp, look_back)
}

#' Multiply in place (without copying) the columns or rows of a \emph{matrix}
#' times a \emph{vector}, element-wise.
#' 
#' @param \code{vector} A \emph{vector}.
#' 
#' @param \code{matrix} A \emph{matrix}.
#' 
#' @param \code{by_col} A \emph{Boolean} argument: if \code{TRUE} then multiply
#'   the columns, otherwise multiply the rows (the default is
#'   \code{by_col = TRUE}.)
#' 
#' @return A single \emph{integer} value, equal to either the number of
#'   \emph{matrix} columns or the number of rows.
#' 
#' @details
#'   The function \code{mult_vec_mat()} multiplies the columns or rows of a
#'   \emph{matrix} times a \emph{vector}, element-wise.
#'
#'   If the number of \emph{vector} elements is equal to the number of matrix
#'   columns, then it multiplies the columns by the \emph{vector}, and returns
#'   the number of columns. If the number of \emph{vector} elements is equal to
#'   the number of rows, then it multiplies the rows, and returns the number of
#'   rows.
#'
#'   If the \emph{matrix} is square and if \code{by_col} is \code{TRUE} then it
#'   multiplies the columns, otherwise it multiplies the rows.
#'   
#'   It accepts \emph{pointers} to the \emph{matrix} and \emph{vector}, and
#'   replaces the old \emph{matrix} values with the new values.
#'   It performs the calculation in place, without copying the \emph{matrix} in
#'   memory (which greatly increases the computation speed).
#'   It performs an implicit loop over the \emph{matrix} rows and columns using
#'   the \emph{Armadillo} operators \code{each_row()} and \code{each_col()},
#'   instead of performing explicit \code{for()} loops (both methods are
#'   equally fast).
#'
#'   The function \code{mult_vec_mat()} uses \code{RcppArmadillo} \code{C++}
#'   code, so when multiplying large \emph{matrix} columns it's several times
#'   faster than vectorized \code{R} code, and it's even much faster compared
#'   to \code{R} when multiplying the \emph{matrix} rows.
#'   
#' @examples
#' \dontrun{
#' # Multiply matrix columns using R
#' matrixv <- matrix(round(runif(25e4), 2), nc=5e2)
#' vectorv <- round(runif(5e2), 2)
#' prod_uct <- vectorv*matrixv
#' # Multiply the matrix in place
#' HighFreq::mult_vec_mat(vectorv, matrixv)
#' all.equal(prod_uct, matrixv)
#' # Compare the speed of Rcpp with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'     Rcpp=HighFreq::mult_vec_mat(vectorv, matrixv),
#'     Rcode=vectorv*matrixv,
#'     times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' 
#' # Multiply matrix rows using R
#' matrixv <- matrix(round(runif(25e4), 2), nc=5e2)
#' vectorv <- round(runif(5e2), 2)
#' prod_uct <- t(vectorv*t(matrixv))
#' # Multiply the matrix in place
#' HighFreq::mult_vec_mat(vectorv, matrixv, by_col=FALSE)
#' all.equal(prod_uct, matrixv)
#' # Compare the speed of Rcpp with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'     Rcpp=HighFreq::mult_vec_mat(vectorv, matrixv, by_col=FALSE),
#'     Rcode=t(vectorv*t(matrixv)),
#'     times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' 
#' @export
mult_vec_mat <- function(vector, matrix, by_col = TRUE) {
    .Call('_HighFreq_mult_vec_mat', PACKAGE = 'HighFreq', vector, matrix, by_col)
}

#' Calculate the eigen decomposition of the covariance \emph{matrix} of returns
#' data using \code{RcppArmadillo}.
#' 
#' @param \code{tseries} A \emph{time series} or \emph{matrix} of returns
#'   data.
#'
#' @return A list with two elements: a \emph{vector} of eigenvalues 
#'   (named "values"), and a \emph{matrix} of eigenvectors (named
#'   "vectors").
#'
#' @details
#'   The function \code{calc_eigen()} first calculates the covariance
#'   \emph{matrix} of \code{tseries}, and then calculates the eigen
#'   decomposition of the covariance \emph{matrix}.
#'
#' @examples
#' \dontrun{
#' # Create matrix of random data
#' datav <- matrix(rnorm(5e6), nc=5)
#' # Calculate eigen decomposition
#' eigend <- HighFreq::calc_eigen(scale(datav, scale=FALSE))
#' # Calculate PCA
#' pcad <- prcomp(datav)
#' # Compare PCA with eigen decomposition
#' all.equal(pcad$sdev^2, drop(eigend$values))
#' all.equal(abs(unname(pcad$rotation)), abs(eigend$vectors))
#' # Compare the speed of Rcpp with R code
#' summary(microbenchmark(
#'   Rcpp=HighFreq::calc_eigen(datav),
#'   Rcode=prcomp(datav),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' 
#' @export
calc_eigen <- function(tseries) {
    .Call('_HighFreq_calc_eigen', PACKAGE = 'HighFreq', tseries)
}

#' Calculate the shrinkage inverse of a \emph{matrix} of data using Singular
#' Value Decomposition (\emph{SVD}).
#' 
#' @param \code{tseries} A \emph{time series} or \emph{matrix} of data.
#' 
#' @param \code{eigen_thresh} A \emph{numeric} threshold level for discarding
#'   small singular values in order to regularize the inverse of the
#'   matrix \code{tseries} (the default is \code{0.01}).
#'   
#' @param \code{eigen_max} An \emph{integer} equal to the number of singular
#'   values used for calculating the shrinkage inverse of the matrix
#'   \code{tseries} (the default is \code{eigen_max = 0} - equivalent to
#'   \code{eigen_max} equal to the number of columns of \code{tseries}).
#'
#' @return A \emph{matrix} equal to the shrinkage inverse of the matrix
#'   \code{tseries}.
#'
#' @details
#'   The function \code{calc_inv()} calculates the shrinkage inverse of the
#'   matrix \code{tseries} using Singular Value Decomposition (\emph{SVD}).
#'   
#'   The function \code{calc_inv()} first performs Singular Value Decomposition
#'   (\emph{SVD}) of the matrix \code{tseries}.  
#'   The \emph{SVD} of a matrix \eqn{\strong{A}} is defined as the
#'   factorization:
#'   \deqn{
#'     \strong{A} = \strong{U}  \, \Sigma  \, \strong{V}^T
#'   }
#'   Where \eqn{\strong{U}} and \eqn{\strong{V}} are the left and right
#'   \emph{singular matrices}, and \eqn{\Sigma} is a diagonal matrix of
#'   \emph{singular values} \eqn{\Sigma = \{\sigma_i\}}.
#'   
#'   The inverse \eqn{\strong{A}^{-1}} of the matrix \eqn{\strong{A}} can be
#'   calculated from the \emph{SVD} matrices as:
#'   \deqn{
#'     \strong{A}^{-1} = \strong{V} \, \Sigma^{-1} \, \strong{U}^T
#'   }
#'   
#'   The \emph{regularized inverse} of the matrix \eqn{\strong{A}} is given by:
#'   \deqn{
#'     \strong{A}^{-1} = \strong{V}_n \, \Sigma_n^{-1} \, \strong{U}_n^T
#'   }
#'   Where \eqn{\strong{U}_n}, \eqn{\strong{V}_n} and \eqn{\Sigma_n} are the
#'   \emph{SVD} matrices with the rows and columns corresponding to zero
#'   \emph{singular values} removed.
#'   
#'   The function \code{calc_inv()} applies regularization by discarding the
#'   smallest singular values \eqn{\sigma_i} that are less than the threshold
#'   level \code{eigen_thresh} times the sum of all the singular values:
#'   \deqn{\sigma_i < eigen\_thresh \cdot (\sum{\sigma_i})}
#'   
#'   It then discards additional singular values so that only the largest
#'   \code{eigen_max} singular values remain.  
#'   It calculates the shrinkage inverse from the \emph{SVD} matrices using
#'   only the largest singular values up to \code{eigen_max}.  For example, if
#'   \code{eigen_max = 3} then it only uses the \code{3} largest singular
#'   values. This has the effect of dimension shrinkage.
#'   
#'   If the matrix \code{tseries} has a large number of small singular values,
#'   then the number of remaining singular values may be less than
#'   \code{eigen_max}.
#'   
#' @examples
#' \dontrun{
#' # Calculate ETF returns
#' returns <- na.omit(rutils::etfenv$returns)
#' # Calculate covariance matrix
#' covmat <- cov(returns)
#' # Calculate shrinkage inverse using RcppArmadillo
#' inverse <- HighFreq::calc_inv(covmat, eigen_max=3)
#' # Calculate shrinkage inverse from SVD in R
#' svdec <- svd(covmat)
#' eigen_max <- 1:3
#' inverser <-  svdec$v[, eigen_max] %*% (t(svdec$u[, eigen_max]) / svdec$d[eigen_max])
#' # Compare RcppArmadillo with R
#' all.equal(inverse, inverser)
#' }
#' 
#' @export
calc_inv <- function(tseries, eigen_thresh = 0.01, eigen_max = 0L) {
    .Call('_HighFreq_calc_inv', PACKAGE = 'HighFreq', tseries, eigen_thresh, eigen_max)
}

#' Scale (standardize) the columns of a \emph{matrix} of data using
#' \code{RcppArmadillo}.
#' 
#' @param \code{tseries} A \emph{time series} or \emph{matrix} of data.
#' 
#' @param \code{use_median} A \emph{Boolean} argument: if \code{TRUE} then the 
#'   centrality (central tendency) is calculated as the \emph{median} and the 
#'   dispersion is calculated as the \emph{median absolute deviation}
#'   (\emph{MAD}).
#'   If \code{use_median = FALSE} then the centrality is calculated as the
#'   \emph{mean} and the dispersion is calculated as the \emph{standard
#'   deviation} (the default is \code{FALSE})
#'
#' @return A \emph{matrix} with the same dimensions as the input
#'   argument \code{tseries}.
#'
#' @details
#'   The function \code{calc_scaled()} scales (standardizes) the columns of the
#'   \code{tseries} argument using \code{RcppArmadillo}.
#'
#'   If the argument \code{use_median} is \code{FALSE} (the default), then it
#'   performs the same calculation as the standard \code{R} function
#'   \code{scale()}, and it calculates the centrality (central tendency) as the
#'   \emph{mean} and the dispersion as the \emph{standard deviation}.
#'
#'   If the argument \code{use_median} is \code{TRUE}, then it calculates the
#'   centrality as the \emph{median} and the dispersion as the \emph{median
#'   absolute deviation} (\emph{MAD}).
#'
#'   If the number of rows of \code{tseries} is less than \code{3} then it
#'   returns \code{tseries} unscaled.
#'   
#'   The function \code{calc_scaled()} uses \code{RcppArmadillo} \code{C++}
#'   code and is about \emph{5} times faster than function \code{scale()}, for
#'   a \emph{matrix} with \emph{1,000} rows and \emph{20} columns.
#'   
#' @examples
#' \dontrun{
#' # Create a matrix of random data
#' returns <- matrix(rnorm(20000), nc=20)
#' scaled <- calc_scaled(tseries=returns, use_median=FALSE)
#' scaled2 <- scale(returns)
#' all.equal(scaled, scaled2, check.attributes=FALSE)
#' # Compare the speed of Rcpp with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=calc_scaled(tseries=returns, use_median=FALSE),
#'   Rcode=scale(returns),
#'   times=100))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' 
#' @export
calc_scaled <- function(tseries, use_median = FALSE) {
    .Call('_HighFreq_calc_scaled', PACKAGE = 'HighFreq', tseries, use_median)
}

#' Calculate the ranks of the elements of a single-column \emph{time series} or
#' a \emph{vector} using \code{RcppArmadillo}.
#' 
#' @param \code{tseries} A single-column \emph{time series} or a \emph{vector}.
#'
#' @return An \emph{integer vector} with the ranks of the elements of the
#'   \code{tseries}.
#'
#' @details
#'   The function \code{calc_ranks()} calculates the ranks of the elements of a
#'   single-column \emph{time series} or a \emph{vector}. It uses the
#'   \code{RcppArmadillo} function \code{arma::sort_index()}. The function
#'   \code{arma::sort_index()} calculates the permutation index to sort a given
#'   vector into ascending order.
#'   
#'   Applying the function \code{arma::sort_index()} twice:
#'   \code{arma::sort_index(arma::sort_index())}, calculates the \emph{reverse}
#'   permutation index to sort the vector from ascending order back into its
#'   original unsorted order.
#'   The permutation index produced by:
#'   \code{arma::sort_index(arma::sort_index())} is the \emph{reverse} of the
#'   permutation index produced by: \code{arma::sort_index()}.
#'   
#'   The ranks of the elements are equal to the \emph{reverse} permutation
#'   index.
#'   The function \code{calc_ranks()} calculates the \emph{reverse} permutation
#'   index.
#'
#' @examples
#' \dontrun{
#' # Create a vector of random data
#' datav <- round(runif(7), 2)
#' # Calculate the ranks of the elements in two ways
#' all.equal(rank(datav), drop(HighFreq::calc_ranks(datav)))
#' # Create a time series of random data
#' datav <- xts::xts(runif(7), seq.Date(Sys.Date(), by=1, length.out=7))
#' # Calculate the ranks of the elements in two ways
#' all.equal(rank(coredata(datav)), drop(HighFreq::calc_ranks(datav)))
#' # Compare the speed of RcppArmadillo with R code
#' datav <- runif(7)
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=calc_ranks(datav),
#'   Rcode=rank(datav),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' 
#' @export
calc_ranks <- function(tseries) {
    .Call('_HighFreq_calc_ranks', PACKAGE = 'HighFreq', tseries)
}

#' Aggregate a time series of data into a single bar of \emph{OHLC} data.
#'
#' @export
#' @param \code{tseries} A \emph{time series} or a \emph{matrix} with multiple
#'   columns of data.
#'
#' @return A \emph{matrix} containing a single row, with the \emph{open},
#'   \emph{high}, \emph{low}, and \emph{close} values, and also the total
#'   \emph{volume} (if provided as either the second or fifth column of
#'   \code{tseries}).
#'
#' @details
#'   The function \code{agg_ohlc()} aggregates a time series of data into a
#'   single bar of \emph{OHLC} data. It can accept either a single column of
#'   data or four columns of \emph{OHLC} data.
#'   It can also accept an additional column containing the trading volume.
#'   
#' The function \code{agg_ohlc()} calculates the \emph{open} value as equal to
#' the \emph{open} value of the first row of \code{tseries}.
#'   The \emph{high} value as the maximum of the \emph{high} column of
#'   \code{tseries}.
#'   The \emph{low} value as the minimum of the \emph{low} column of
#'   \code{tseries}.
#'   The \emph{close} value as the \emph{close} of the last row of
#'   \code{tseries}.
#'   The \emph{volume} value as the sum of the \emph{volume} column of
#'   \code{tseries}.
#'
#'   For a single column of data, the \emph{open}, \emph{high}, \emph{low}, and
#'   \emph{close} values are all the same.
#'
#' @examples
#' \dontrun{
#' # Define matrix of OHLC data
#' ohlc <- coredata(rutils::etfenv$VTI[, 1:5])
#' # Aggregate to single row matrix
#' ohlcagg <- HighFreq::agg_ohlc(ohlc)
#' # Compare with calculation in R
#' all.equal(drop(ohlcagg),
#'   c(ohlc[1, 1], max(ohlc[, 2]), min(ohlc[, 3]), ohlc[NROW(ohlc), 4], sum(ohlc[, 5])), 
#'   check.attributes=FALSE)
#' }
#' 
#' @export
agg_ohlc <- function(tseries) {
    .Call('_HighFreq_agg_ohlc', PACKAGE = 'HighFreq', tseries)
}

#' Count the number of consecutive \code{TRUE} elements in a Boolean vector,
#' and reset the count to zero after every \code{FALSE} element.
#' 
#' @param \code{tseries} A \emph{Boolean vector} of data.
#'
#' @return An \emph{integer vector} of the same length as the argument
#'   \code{tseries}.
#'
#' @details
#'   The function \code{roll_count()} calculates the number of consecutive
#'   \code{TRUE} elements in a Boolean vector, and it resets the count to zero
#'   after every \code{FALSE} element.
#'   
#'   For example, the Boolean vector {\code{FALSE}, \code{TRUE}, \code{TRUE},
#'   \code{FALSE}, \code{FALSE}, \code{TRUE}, \code{TRUE}, \code{TRUE},
#'   \code{TRUE}, \code{TRUE}, \code{FALSE}}, is translated into {\code{0},
#'   \code{1}, \code{2}, \code{0}, \code{0}, \code{1}, \code{2}, \code{3},
#'   \code{4}, \code{5}, \code{0}}.
#'   
#' @examples
#' \dontrun{
#' # Calculate the number of consecutive TRUE elements
#' drop(HighFreq::roll_count(c(FALSE, TRUE, TRUE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE, TRUE, FALSE)))
#' }
#' @export
roll_count <- function(tseries) {
    .Call('_HighFreq_roll_count', PACKAGE = 'HighFreq', tseries)
}

#' Aggregate a time series to an \emph{OHLC} time series with lower
#' periodicity.
#'
#' Given a time series of prices at a higher periodicity (say seconds), it
#' calculates the \emph{OHLC} prices at a lower periodicity (say minutes).
#'
#' @param \code{tseries} A \emph{time series} or a \emph{matrix} with multiple
#'   columns of data.
#'   
#' @param \emph{endp} An \emph{integer vector} of end points.
#'
#' @return A \emph{matrix} with \emph{OHLC} data, with the number of rows equal
#'   to the number of \emph{endp} minus one.
#'   
#' @details
#'   The function \code{roll_ohlc()} performs a loop over the end points
#'   \emph{endp}, along the rows of the data \code{tseries}. At each end point,
#'   it selects the past rows of the data \code{tseries}, starting at the first
#'   bar after the previous end point, and then calls the function
#'   \code{agg_ohlc()} on the selected data \code{tseries} to calculate the
#'   aggregations.
#'   
#'   The function \code{roll_ohlc()} can accept either a single column of data
#'   or four columns of \emph{OHLC} data.
#'   It can also accept an additional column containing the trading volume.
#'
#'   The function \code{roll_ohlc()} performs a similar aggregation as the
#'   function \code{to.period()} from package
#'   \href{https://cran.r-project.org/web/packages/xts/index.html}{xts}.
#'
#' @examples
#' \dontrun{
#' # Define matrix of OHLC data
#' ohlc <- rutils::etfenv$VTI[, 1:5]
#' # Define end points at 25 day intervals
#' endp <- HighFreq::calc_endpoints(NROW(ohlc), step=25)
#' # Aggregate over endp:
#' ohlcagg <- HighFreq::roll_ohlc(tseries=ohlc, endp=endp)
#' # Compare with xts::to.period()
#' ohlcagg_xts <- .Call("toPeriod", ohlc, as.integer(endp+1), TRUE, NCOL(ohlc), FALSE, FALSE, colnames(ohlc), PACKAGE="xts")
#' all.equal(ohlcagg, coredata(ohlcagg_xts), check.attributes=FALSE)
#' }
#' 
#' @export
roll_ohlc <- function(tseries, endp) {
    .Call('_HighFreq_roll_ohlc', PACKAGE = 'HighFreq', tseries, endp)
}

#' Calculate the rolling sums over a single-column \emph{time series} or a
#' single-column \emph{matrix} using \emph{Rcpp}.
#' 
#' @param \code{tseries} A single-column \emph{time series} or a single-column
#'   \emph{matrix}.
#' 
#' @param \code{look_back} The length of the look-back interval, equal to the
#'   number of elements of data used for calculating the sum.
#'
#' @return A single-column \emph{matrix} of the same length as the argument
#'   \code{tseries}.
#'
#' @details
#'   The function \code{roll_vec()} calculates a single-column \emph{matrix} of
#'   rolling sums, over a single-column \emph{matrix} of data, using fast
#'   \emph{Rcpp} \code{C++} code.  The function \code{roll_vec()} is several
#'   times faster than \code{rutils::roll_sum()} which uses vectorized \code{R}
#'   code.
#'
#' @examples
#' \dontrun{
#' # Define a single-column matrix of returns
#' returns <- zoo::coredata(na.omit(rutils::etfenv$returns$VTI))
#' # Calculate rolling sums over 11-period look-back intervals
#' sum_rolling <- HighFreq::roll_vec(returns, look_back=11)
#' # Compare HighFreq::roll_vec() with rutils::roll_sum()
#' all.equal(HighFreq::roll_vec(returns, look_back=11), 
#'          rutils::roll_sum(returns, look_back=11), 
#'          check.attributes=FALSE)
#' # Compare the speed of Rcpp with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::roll_vec(returns, look_back=11),
#'   Rcode=rutils::roll_sum(returns, look_back=11),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' 
#' @export
roll_vec <- function(tseries, look_back) {
    .Call('_HighFreq_roll_vec', PACKAGE = 'HighFreq', tseries, look_back)
}

#' Calculate the rolling weighted sums over a single-column \emph{time series}
#' or a single-column \emph{matrix} using \code{RcppArmadillo}.
#' 
#' @param \code{tseries} A single-column \emph{time series} or a single-column
#'   \emph{matrix}.
#' 
#' @param \code{weights} A single-column \emph{matrix} of weights.
#'
#' @return A single-column \emph{matrix} of the same length as the argument
#'   \code{tseries}.
#'
#' @details
#'   The function \code{roll_vecw()} calculates the rolling weighted sums of a
#'   single-column \emph{matrix} over its past values (a convolution with the
#'   single-column \emph{matrix} of weights), using \code{RcppArmadillo}. It
#'   performs a similar calculation as the standard \code{R} function
#'   \cr\code{stats::filter(x=series, filter=weights, method="convolution",
#'   sides=1)}, but it's over \code{6} times faster, and it doesn't produce any
#'   \code{NA} values.
#'   
#' @examples
#' \dontrun{
#' # First example
#' # Define a single-column matrix of returns
#' returns <- zoo::coredata(na.omit(rutils::etfenv$returns$VTI))
#' # Create simple weights
#' weights <- matrix(c(1, rep(0, 10)))
#' # Calculate rolling weighted sums
#' weighted <- HighFreq::roll_vecw(tseries=returns, weights=weights)
#' # Compare with original
#' all.equal(zoo::coredata(returns), weighted, check.attributes=FALSE)
#' # Second example
#' # Create exponentially decaying weights
#' weights <- matrix(exp(-0.2*1:11))
#' weights <- weights/sum(weights)
#' # Calculate rolling weighted sums
#' weighted <- HighFreq::roll_vecw(tseries=returns, weights=weights)
#' # Calculate rolling weighted sums using filter()
#' filtered <- stats::filter(x=returns, filter=weights, method="convolution", sides=1)
#' # Compare both methods
#' all.equal(filtered[-(1:11)], weighted[-(1:11)], check.attributes=FALSE)
#' # Compare the speed of Rcpp with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::roll_vecw(tseries=returns, weights=weights),
#'   Rcode=stats::filter(x=returns, filter=weights, method="convolution", sides=1),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' 
#' @export
roll_vecw <- function(tseries, weights) {
    .Call('_HighFreq_roll_vecw', PACKAGE = 'HighFreq', tseries, weights)
}

#' Calculate the rolling convolutions (weighted sums) of a \emph{time series}
#' with a single-column \emph{matrix} of weights.
#' 
#' @param \code{tseries} A \emph{time series} or a \emph{matrix} of data.
#' 
#' @param \code{weights} A single-column \emph{matrix} of weights.
#'
#' @return A \emph{matrix} with the same dimensions as the input
#'   argument \code{tseries}.
#'
#' @details
#'   The function \code{roll_conv()} calculates the convolutions of the
#'   \emph{matrix} columns with a single-column \emph{matrix} of weights.  It
#'   performs a loop over the \emph{matrix} rows and multiplies the past
#'   (higher) values by the weights.  It calculates the rolling weighted sums
#'   of the past values.
#'   
#'   The function \code{roll_conv()} uses the \code{RcppArmadillo} function
#'   \code{arma::conv2()}. It performs a similar calculation to the standard
#'   \code{R} function \cr\code{filter(x=tseries, filter=weights,
#'   method="convolution", sides=1)}, but it's over \code{6} times faster, and
#'   it doesn't produce any leading \code{NA} values.
#'   
#' @examples
#' \dontrun{
#' # First example
#' # Calculate a time series of returns
#' returns <- na.omit(rutils::etfenv$returns[, c("IEF", "VTI")])
#' # Create simple weights equal to a 1 value plus zeros
#' weights <- matrix(c(1, rep(0, 10)), nc=1)
#' # Calculate rolling weighted sums
#' weighted <- HighFreq::roll_conv(returns, weights)
#' # Compare with original
#' all.equal(coredata(returns), weighted, check.attributes=FALSE)
#' # Second example
#' # Calculate exponentially decaying weights
#' weights <- exp(-0.2*(1:11))
#' weights <- matrix(weights/sum(weights), nc=1)
#' # Calculate rolling weighted sums
#' weighted <- HighFreq::roll_conv(returns, weights)
#' # Calculate rolling weighted sums using filter()
#' filtered <- filter(x=returns, filter=weights, method="convolution", sides=1)
#' # Compare both methods
#' all.equal(filtered[-(1:11), ], weighted[-(1:11), ], check.attributes=FALSE)
#' }
#' 
#' @export
roll_conv <- function(tseries, weights) {
    .Call('_HighFreq_roll_conv', PACKAGE = 'HighFreq', tseries, weights)
}

#' Calculate the rolling sums over a \emph{time series} or a \emph{matrix}
#' using \emph{Rcpp}.
#' 
#' @param \code{tseries} A \emph{time series} or a \emph{matrix}.
#' 
#' @param \code{look_back} The length of the look-back interval, equal to the
#'   number of data points included in calculating the rolling sum (the default
#'   is \code{look_back = 1}).
#'   
#' @return A \emph{matrix} with the same dimensions as the input argument
#'   \code{tseries}.
#'
#' @details
#'   The function \code{roll_sum()} calculates the rolling sums over the
#'   columns of the data \code{tseries}.
#'   
#'   The function \code{roll_sum()} returns a \emph{matrix} with the same
#'   dimensions as the input argument \code{tseries}.
#' 
#'   The function \code{roll_sum()} uses the fast \code{RcppArmadillo} function
#'   \code{arma::cumsum()}, without explicit loops.
#'   The function \code{roll_sum()} is several times faster than
#'   \code{rutils::roll_sum()} which uses vectorized \code{R} code.
#'   
#' @examples
#' \dontrun{
#' # Calculate historical returns
#' returns <- na.omit(rutils::etfenv$returns[, c("VTI", "IEF")])
#' # Define parameters
#' look_back <- 22
#' # Calculate rolling sums and compare with rutils::roll_sum()
#' c_sum <- HighFreq::roll_sum(returns, look_back)
#' r_sum <- rutils::roll_sum(returns, look_back)
#' all.equal(c_sum, coredata(r_sum), check.attributes=FALSE)
#' # Calculate rolling sums using R code
#' r_sum <- apply(zoo::coredata(returns), 2, cumsum)
#' lag_sum <- rbind(matrix(numeric(2*look_back), nc=2), r_sum[1:(NROW(r_sum) - look_back), ])
#' r_sum <- (r_sum - lag_sum)
#' all.equal(c_sum, r_sum, check.attributes=FALSE)
#' }
#' 
#' @export
roll_sum <- function(tseries, look_back = 1L) {
    .Call('_HighFreq_roll_sum', PACKAGE = 'HighFreq', tseries, look_back)
}

#' Calculate the rolling sums at the end points of a \emph{time series} or a
#' \emph{matrix}.
#' 
#' @param \code{tseries} A \emph{time series} or a \emph{matrix}.
#' 
#' @param \code{startp} An \emph{integer} vector of start points (the default
#'   is \code{startp = 0}).
#' 
#' @param \code{endp} An \emph{integer} vector of end points (the default is
#'   \code{endp = 0}).
#' 
#' @param \code{step} The number of time periods between the end points (the
#'   default is \code{step = 1}).
#'
#' @param \code{look_back} The number of end points in the look-back interval
#'   (the default is \code{look_back = 1}).
#'   
#' @param \code{stub} An \emph{integer} value equal to the first end point for
#'   calculating the end points.
#'
#' @return A \emph{matrix} with the same number of columns as the input time
#'   series \code{tseries}, and the number of rows equal to the number of end
#'   points.
#'   
#' @details
#'   The function \code{roll_sumep()} calculates the rolling sums at the end
#'   points of the \emph{time series} \code{tseries}.
#'   
#'   The function \code{roll_sumep()} is implemented in \code{RcppArmadillo}
#'   \code{C++} code, which makes it several times faster than \code{R} code.
#'   
#' @examples
#' \dontrun{
#' # Calculate historical returns
#' returns <- na.omit(rutils::etfenv$returns[, c("VTI", "IEF")])
#' # Define end points at 25 day intervals
#' endp <- HighFreq::calc_endpoints(NROW(returns), step=25)
#' # Define start points as 75 day lag of end points
#' startp <- HighFreq::calc_startpoints(endp, look_back=3)
#' # Calculate rolling sums using Rcpp
#' c_sum <- HighFreq::roll_sumep(returns, startp=startp, endp=endp)
#' # Calculate rolling sums using R code
#' r_sum <- sapply(1:NROW(endp), function(ep) {
#' colSums(returns[(startp[ep]+1):(endp[ep]+1), ])
#'   })  # end sapply
#' r_sum <- t(r_sum)
#' all.equal(c_sum, r_sum, check.attributes=FALSE)
#' }
#' 
#' @export
roll_sumep <- function(tseries, startp = 0L, endp = 0L, step = 1L, look_back = 1L, stub = 0L) {
    .Call('_HighFreq_roll_sumep', PACKAGE = 'HighFreq', tseries, startp, endp, step, look_back, stub)
}

#' Calculate the rolling weighted sums over a \emph{time series} or a
#' \emph{matrix} using \emph{Rcpp}.
#' 
#' @param \code{tseries} A \emph{time series} or a \emph{matrix}.
#' 
#' @param \code{endp} An \emph{integer} vector of end points (the default is
#'   \code{endp = NULL}).
#'   
#' @param \code{look_back} The length of the look-back interval, equal to the
#'   number of data points included in calculating the rolling sum (the default
#'   is \code{look_back = 1}).
#'   
#' @param \code{stub} An \emph{integer} value equal to the first end point for
#'   calculating the end points (the default is \code{stub = NULL}).
#' 
#' @param \code{weights} A single-column \emph{matrix} of weights (the default
#'   is \code{weights = NULL}).
#'
#' @return A \emph{matrix} with the same dimensions as the input argument
#'   \code{tseries}.
#'
#' @details
#'   The function \code{roll_wsum()} calculates the rolling weighted sums over
#'   the columns of the data \code{tseries}.
#' 
#'   The function \code{roll_wsum()} calculates the rolling weighted sums as
#'   convolutions of the columns of \code{tseries} with the \emph{column
#'   vector} of weights using the \code{RcppArmadillo} function
#'   \code{arma::conv2()}.  It performs a similar calculation to the standard
#'   \code{R} function \cr\code{stats::filter(x=returns, filter=weights,
#'   method="convolution", sides=1)}, but it can be many times faster, and it
#'   doesn't produce any leading \code{NA} values.
#'   
#'   The function \code{roll_wsum()} returns a \emph{matrix} with the same
#'   dimensions as the input argument \code{tseries}.
#' 
#'   The arguments \code{weights}, \code{endp}, and \code{stub} are
#'   optional.
#'   
#'   If the argument \code{weights} is not supplied, then simple sums are
#'   calculated, not weighted sums.
#'   
#'   If either the \code{stub} or \code{endp} arguments are supplied,
#'   then the rolling sums are calculated at the end points. 
#'   
#'   If only the argument \code{stub} is supplied, then the end points are
#'   calculated from the \code{stub} and \code{look_back} arguments. The first
#'   end point is equal to \code{stub} and the end points are spaced
#'   \code{look_back} periods apart.
#'   
#'   If the arguments \code{weights}, \code{endp}, and \code{stub} are
#'   not supplied, then the sums are calculated over a number of data points
#'   equal to \code{look_back}.
#'   
#'   The function \code{roll_wsum()} is also several times faster than
#'   \code{rutils::roll_sum()} which uses vectorized \code{R} code.
#'   
#'   Technical note:
#'   The function \code{roll_wsum()} has arguments with default values equal to
#'   \code{NULL}, which are implemented in \code{Rcpp} code.
#'   
#' @examples
#' \dontrun{
#' # First example
#' # Calculate historical returns
#' returns <- na.omit(rutils::etfenv$returns[, c("VTI", "IEF")])
#' # Define parameters
#' look_back <- 22
#' # Calculate rolling sums and compare with rutils::roll_sum()
#' c_sum <- HighFreq::roll_sum(returns, look_back)
#' r_sum <- rutils::roll_sum(returns, look_back)
#' all.equal(c_sum, coredata(r_sum), check.attributes=FALSE)
#' # Calculate rolling sums using R code
#' r_sum <- apply(zoo::coredata(returns), 2, cumsum)
#' lag_sum <- rbind(matrix(numeric(2*look_back), nc=2), r_sum[1:(NROW(r_sum) - look_back), ])
#' r_sum <- (r_sum - lag_sum)
#' all.equal(c_sum, r_sum, check.attributes=FALSE)
#' 
#' # Calculate rolling sums at end points
#' stu_b <- 21
#' c_sum <- HighFreq::roll_wsum(returns, look_back, stub=stu_b)
#' endp <- (stu_b + look_back*(0:(NROW(returns) %/% look_back)))
#' endp <- endp[endp < NROW(returns)]
#' r_sum <- apply(zoo::coredata(returns), 2, cumsum)
#' r_sum <- r_sum[endp+1, ]
#' lag_sum <- rbind(numeric(2), r_sum[1:(NROW(r_sum) - 1), ])
#' r_sum <- (r_sum - lag_sum)
#' all.equal(c_sum, r_sum, check.attributes=FALSE)
#' 
#' # Calculate rolling sums at end points - pass in endp
#' c_sum <- HighFreq::roll_wsum(returns, endp=endp)
#' all.equal(c_sum, r_sum, check.attributes=FALSE)
#' 
#' # Create exponentially decaying weights
#' weights <- exp(-0.2*(1:11))
#' weights <- matrix(weights/sum(weights), nc=1)
#' # Calculate rolling weighted sum
#' c_sum <- HighFreq::roll_wsum(returns, weights=weights)
#' # Calculate rolling weighted sum using filter()
#' filtered <- filter(x=returns, filter=weights, method="convolution", sides=1)
#' all.equal(c_sum[-(1:11), ], filtered[-(1:11), ], check.attributes=FALSE)
#' 
#' # Calculate rolling weighted sums at end points
#' c_sum <- HighFreq::roll_wsum(returns, endp=endp, weights=weights)
#' all.equal(c_sum, filtered[endp+1, ], check.attributes=FALSE)
#' 
#' # Create simple weights equal to a 1 value plus zeros
#' weights <- matrix(c(1, rep(0, 10)), nc=1)
#' # Calculate rolling weighted sum
#' weighted <- HighFreq::roll_wsum(returns, weights=weights)
#' # Compare with original
#' all.equal(coredata(returns), weighted, check.attributes=FALSE)
#' }
#' 
#' @export
roll_wsum <- function(tseries, endp = NULL, look_back = 1L, stub = NULL, weights = NULL) {
    .Call('_HighFreq_roll_wsum', PACKAGE = 'HighFreq', tseries, endp, look_back, stub, weights)
}

#' Calculate the running weighted means of streaming \emph{time series} data.
#' 
#' @param \code{tseries} A \emph{time series} or a \emph{matrix}.
#' 
#' @param \code{lambda} A \emph{numeric} decay factor to multiply past
#'   estimates.
#'   
#' @param \code{weights} A single-column \emph{matrix} of weights.
#'
#' @return A \emph{matrix} with the same dimensions as the input argument
#'   \code{tseries}.
#'
#' @details
#'   The function \code{run_mean()} calculates the running weighted means of
#'   the streaming \emph{time series} data \eqn{p_t} by recursively weighing
#'   present and past values using the decay factor \eqn{\lambda}:
#'   \deqn{
#'     \mu^w_t = (1-\lambda) w_t + \lambda \mu^w_{t-1}
#'   }
#'   \deqn{
#'     \mu^p_t = (1-\lambda) w_t p_t + \lambda \mu^p_{t-1}
#'   }
#'   Where \eqn{p_t} is the streaming data, \eqn{w_t} are the streaming
#'   weights, \eqn{\mu^w_t} are the running mean weights, and \eqn{\mu^p_t} are
#'   the running mean products of the data and the weights. 
#'   
#'   The running mean weighted value \eqn{\mu_t} is equal to the ratio of the
#'   data and weights products, divided by the mean weights:
#'   \deqn{
#'     \mu_t = \frac{\mu^p_t}{\mu^w_t}
#'   }
#' 
#'   If the \code{weights} argument is omitted, then the function
#'   \code{run_mean()} simply calculates the running means of \eqn{p_t}:
#'   \deqn{
#'     \mu_t = (1-\lambda) p_t + \lambda \mu_{t-1}
#'   }
#'   
#'   The above recursive formulas are convenient for processing live streaming
#'   data because it doesn't require maintaining a buffer of past data.
#'   The formula is equivalent to a convolution with exponentially decaying
#'   weights, but it's faster.
#'   
#'   The value of the decay factor \eqn{\lambda} should be in the range between
#'   \code{0} and \code{1}.  
#'   If \eqn{\lambda} is close to \code{1} then the decay is weak and past
#'   values have a greater weight, and the running mean values have a stronger
#'   dependence on past values.  This is equivalent to a long look-back
#'   interval.
#'   If \eqn{\lambda} is much less than \code{1} then the decay is strong and
#'   past values have a smaller weight, and the running mean values have a
#'   weaker dependence on past values.  This is equivalent to a short look-back
#'   interval.
#' 
#'   The function \code{run_mean()} performs the same calculation as the
#'   standard \code{R} function\cr\code{stats::filter(x=series, filter=lambda,
#'   method="recursive")}, but it's several times faster.
#' 
#'   The function \code{run_mean()} returns a \emph{matrix} with the same
#'   dimensions as the input argument \code{tseries}.
#'   
#' @examples
#' \dontrun{
#' # Calculate historical prices
#' ohlc <- rutils::etfenv$VTI
#' closep <- quantmod::Cl(ohlc)
#' # Calculate the running means
#' lambda <- 0.95
#' means <- HighFreq::run_mean(closep, lambda=lambda)
#' # Calculate running means using R code
#' filtered <- (1-lambda)*filter(prices, 
#'   filter=lambda, init=as.numeric(prices[1, 1])/(1-lambda), 
#'   method="recursive")
#' all.equal(drop(means), unclass(filtered), check.attributes=FALSE)
#' 
#' # Compare the speed of RcppArmadillo with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::run_mean(prices, lambda=lambda),
#'   Rcode=filter(prices, filter=lambda, init=as.numeric(prices[1, 1])/(1-lambda), method="recursive"),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#'   
#' # Create weights equal to the trading volumes
#' weights <- quantmod::Vo(ohlc)
#' # Calculate the running weighted means
#' meanw <- HighFreq::run_mean(prices, lambda=lambda, weights=weights)
#' # Plot dygraph of the running weighted means
#' datav <- xts(cbind(means, meanw), zoo::index(ohlc))
#' colnames(datav) <- c("means running", "means weighted")
#' dygraphs::dygraph(datav, main="Running Means") %>%
#'   dyOptions(colors=c("blue", "red"), strokeWidth=1) %>%
#'   dyLegend(show="always", width=500)
#' }
#' 
#' @export
run_mean <- function(tseries, lambda, weights = 0L) {
    .Call('_HighFreq_run_mean', PACKAGE = 'HighFreq', tseries, lambda, weights)
}

#' Calculate the running maximum of streaming \emph{time series} data.
#' 
#' @param \code{tseries} A \emph{time series} or a \emph{matrix}.
#' 
#' @param \code{lambda} A \emph{numeric} decay factor to multiply past
#'   estimates.
#'   
#' @return A \emph{matrix} with the same dimensions as the input argument
#'   \code{tseries}.
#'
#' @details
#'   The function \code{run_max()} calculates the running maximum of streaming
#'   \emph{time series} data by recursively weighing present and past values
#'   using the decay factor \eqn{\lambda}.
#'
#'   It first calculates the running mean of streaming data:
#'   \deqn{
#'     \mu_t = (1-\lambda) p_t + \lambda \mu_{t-1}
#'   }
#'   Where \eqn{\mu_t} is the mean value at time \eqn{t}, and \eqn{p_t} is the
#'   streaming data.
#'
#'   It then calculates the running maximums of streaming data, \eqn{p^{max}_t}:
#'   \deqn{
#'     p^{max}_t = max(p_t, p^{max}_{t-1}) + (1-\lambda) (\mu_{t-1} - p^{max}_{t-1})
#'   }
#' 
#'   The second term pulls the maximum value down to the mean value, allowing
#'   it to gradually "forget" the maximum value from the more distant past.
#' 
#'   The above recursive formulas are convenient for processing live streaming
#'   data because it doesn't require maintaining a buffer of past data.
#'   
#'   The value of the decay factor \eqn{\lambda} should be in the range between
#'   \code{0} and \code{1}.  
#'   If \eqn{\lambda} is close to \code{1} then the decay is weak and past
#'   values have a greater weight, and the running maximum values have a stronger
#'   dependence on past values.  This is equivalent to a long look-back
#'   interval.
#'   If \eqn{\lambda} is much less than \code{1} then the decay is strong and
#'   past values have a smaller weight, and the running maximum values have a
#'   weaker dependence on past values.  This is equivalent to a short look-back
#'   interval.
#' 
#'   The function \code{run_max()} returns a \emph{matrix} with the same
#'   dimensions as the input argument \code{tseries}.
#'   
#' @examples
#' \dontrun{
#' # Calculate historical prices
#' prices <- zoo::coredata(quantmod::Cl(rutils::etfenv$VTI))
#' # Calculate the running maximums
#' lambda <- 0.9
#' maxs <- HighFreq::run_max(prices, lambda=lambda)
#' # Plot dygraph of VTI prices and running maximums
#' datav <- cbind(quantmod::Cl(rutils::etfenv$VTI), maxs)
#' colnames(datav) <- c("prices", "max")
#' colnamev <- colnames(datav)
#' dygraphs::dygraph(datav, main="VTI Prices and Running Maximums") %>%
#'   dySeries(name=colnamev[1], label=colnamev[1], strokeWidth=2, col="blue") %>%
#'   dySeries(name=colnamev[2], label=colnamev[2], strokeWidth=2, col="red")
#' }
#' 
#' @export
run_max <- function(tseries, lambda) {
    .Call('_HighFreq_run_max', PACKAGE = 'HighFreq', tseries, lambda)
}

#' Calculate the running minimum of streaming \emph{time series} data.
#' 
#' @param \code{tseries} A \emph{time series} or a \emph{matrix}.
#' 
#' @param \code{lambda} A \emph{numeric} decay factor to multiply past
#'   estimates.
#'   
#' @return A \emph{matrix} with the same dimensions as the input argument
#'   \code{tseries}.
#'
#' @details
#'   The function \code{run_min()} calculates the running minimum of streaming
#'   \emph{time series} data by recursively weighing present and past values
#'   using the decay factor \eqn{\lambda}.
#'
#'   It first calculates the running mean of streaming data:
#'   \deqn{
#'     \mu_t = (1-\lambda) p_t + \lambda \mu_{t-1}
#'   }
#'   Where \eqn{\mu_t} is the mean value at time \eqn{t}, and \eqn{p_t} is the
#'   streaming data.
#'
#'   It then calculates the running minimums of streaming data, \eqn{p^{min}_t}:
#'   \deqn{
#'     p^{min}_t = min(p_t, p^{min}_{t-1}) + (1-\lambda) (\mu_{t-1} - p^{min}_{t-1})
#'   }
#' 
#'   The second term pulls the minimum value up to the mean value, allowing
#'   it to gradually "forget" the minimum value from the more distant past.
#' 
#'   The above recursive formula is convenient for processing live streaming
#'   data because it doesn't require maintaining a buffer of past data.
#' 
#'   The value of the decay factor \eqn{\lambda} should be in the range between
#'   \code{0} and \code{1}.  
#'   If \eqn{\lambda} is close to \code{1} then the decay is weak and past
#'   values have a greater weight, and the running minimum values have a stronger
#'   dependence on past values.  This is equivalent to a long look-back
#'   interval.
#'   If \eqn{\lambda} is much less than \code{1} then the decay is strong and
#'   past values have a smaller weight, and the running minimum values have a
#'   weaker dependence on past values.  This is equivalent to a short look-back
#'   interval.
#' 
#'   The function \code{run_min()} returns a \emph{matrix} with the same
#'   dimensions as the input argument \code{tseries}.
#'   
#' @examples
#' \dontrun{
#' # Calculate historical prices
#' prices <- zoo::coredata(quantmod::Cl(rutils::etfenv$VTI))
#' # Calculate the running minimums
#' lambda <- 0.9
#' mins <- HighFreq::run_min(prices, lambda=lambda)
#' # Plot dygraph of VTI prices and running minimums
#' datav <- cbind(quantmod::Cl(rutils::etfenv$VTI), mins)
#' colnames(datav) <- c("prices", "min")
#' colnamev <- colnames(datav)
#' dygraphs::dygraph(datav, main="VTI Prices and Running Minimums") %>%
#'   dySeries(name=colnamev[1], label=colnamev[1], strokeWidth=2, col="blue") %>%
#'   dySeries(name=colnamev[2], label=colnamev[2], strokeWidth=2, col="red")
#' }
#' 
#' @export
run_min <- function(tseries, lambda) {
    .Call('_HighFreq_run_min', PACKAGE = 'HighFreq', tseries, lambda)
}

#' Calculate the running variance of streaming \emph{time series} of returns.
#' 
#' @param \code{tseries} A \emph{time series} or a \emph{matrix} of returns.
#' 
#' @param \code{lambda} A \emph{numeric} decay factor to multiply past
#'   estimates.
#'   
#' @return A \emph{matrix} with the same dimensions as the input argument
#'   \code{tseries}.
#'
#' @details
#'   The function \code{run_var()} calculates the running variance of a
#'   streaming \emph{time series} of returns, by recursively weighing the
#'   squared returns \eqn{r^2_t} minus the squared means \eqn{\mu^2_t}, with
#'   the past variance estimates \eqn{\sigma^2_{t-1}}, using the decay factor
#'   \eqn{\lambda}:
#'   \deqn{
#'     \mu_t = (1-\lambda) r_t + \lambda \mu_{t-1}
#'   }
#'   \deqn{
#'     \sigma^2_t = (1-\lambda) (r^2_t - \mu^2_t) + \lambda \sigma^2_{t-1}
#'   }
#'   Where \eqn{\sigma^2_t} is the variance estimate at time \eqn{t}, and
#'   \eqn{r_t} are the streaming returns data.
#' 
#'   The above recursive formula is convenient for processing live streaming
#'   data because it doesn't require maintaining a buffer of past data.
#'   The formula is equivalent to a convolution with exponentially decaying
#'   weights, but it's faster.
#' 
#'   The value of the decay factor \eqn{\lambda} should be in the range between
#'   \code{0} and \code{1}.  
#'   If \eqn{\lambda} is close to \code{1} then the decay is weak and past
#'   values have a greater weight, and the running variance values have a
#'   stronger dependence on past values.  This is equivalent to a long
#'   look-back interval.
#'   If \eqn{\lambda} is much less than \code{1} then the decay is strong and
#'   past values have a smaller weight, and the running variance values have a
#'   weaker dependence on past values.  This is equivalent to a short look-back
#'   interval.
#' 
#'   The function \code{run_var()} performs the same calculation as the
#'   standard \code{R} function\cr\code{stats::filter(x=series,
#'   filter=weights, method="recursive")}, but it's several times faster.
#' 
#'   The function \code{run_var()} returns a \emph{matrix} with the same
#'   dimensions as the input argument \code{tseries}.
#'   
#' @examples
#' \dontrun{
#' # Calculate historical returns
#' returns <- zoo::coredata(na.omit(rutils::etfenv$returns$VTI))
#' # Calculate the running variance
#' lambda <- 0.9
#' vars <- HighFreq::run_var(returns, lambda=lambda)
#' # Calculate running variance using R code
#' filtered <- (1-lambda)*filter(returns^2, filter=lambda, 
#'   init=as.numeric(returns[1, 1])^2/(1-lambda), 
#'   method="recursive")
#' all.equal(vars, unclass(filtered), check.attributes=FALSE)
#' # Compare the speed of RcppArmadillo with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::run_var(returns, lambda=lambda),
#'   Rcode=filter(returns^2, filter=lambda, init=as.numeric(returns[1, 1])^2/(1-lambda), method="recursive"),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' 
#' @export
run_var <- function(tseries, lambda) {
    .Call('_HighFreq_run_var', PACKAGE = 'HighFreq', tseries, lambda)
}

#' Calculate the running variance of streaming \emph{OHLC} price data.
#' 
#' @param \code{ohlc} A \emph{time series} or a \emph{matrix} with \emph{OHLC}
#'   price data.
#'   
#' @param \code{lambda} A \emph{numeric} decay factor to multiply past
#'   estimates.
#'
#' @return A single-column \emph{matrix} of variance estimates, with the same
#'   number of rows as the input \code{ohlc} price data.
#'
#' @details
#'   The function \code{run_var_ohlc()} calculates a single-column
#'   \emph{matrix} of variance estimates of streaming \emph{OHLC} price data.
#'   
#'   The function \code{run_var_ohlc()} calculates the variance from the
#'   differences between the \emph{Open}, \emph{High}, \emph{Low}, and
#'   \emph{Close} prices, using the \emph{Yang-Zhang} range volatility
#'   estimator:
#'   \deqn{
#'     \sigma^2_t = (1-\lambda) ((O_t - C_{t-1})^2 + 0.134 (C_t - O_t)^2 + 
#'     0.866 ((H_i - O_i) (H_i - C_i) + (L_i - O_i) (L_i - C_i))) + 
#'     \lambda \sigma^2_{t-1}
#'   }
#'   It recursively weighs the current variance estimate with the past
#'   estimates \eqn{\sigma^2_{t-1}}, using the decay factor \eqn{\lambda}.
#'
#'   The above recursive formula is convenient for processing live streaming
#'   data because it doesn't require maintaining a buffer of past data.
#'   The formula is equivalent to a convolution with exponentially decaying
#'   weights, but it's faster.
#'   
#'   The function \code{run_var_ohlc()} does not calculate the logarithm of
#'   the prices.
#'   So if the argument \code{ohlc} contains dollar prices then
#'   \code{run_var_ohlc()} calculates the dollar variance.
#'   If the argument \code{ohlc} contains the log prices then
#'   \code{run_var_ohlc()} calculates the percentage variance.
#'   
#'   The function \code{run_var_ohlc()} is implemented in \code{RcppArmadillo}
#'   \code{C++} code, so it's many times faster than the equivalent \code{R}
#'   code.
#'
#' @examples
#' \dontrun{
#' # Extract the log OHLC prices of VTI
#' ohlc <- log(rutils::etfenv$VTI)
#' # Calculate the running variance
#' var_running <- HighFreq::run_var_ohlc(ohlc, lambda=0.8)
#' # Calculate the rolling variance
#' var_rolling <- HighFreq::roll_var_ohlc(ohlc, look_back=5, method="yang_zhang", scale=FALSE)
#' datav <- cbind(var_running, var_rolling)
#' colnames(datav) <- c("running", "rolling")
#' colnamev <- colnames(datav)
#' datav <- xts::xts(datav, index(ohlc))
#' # dygraph plot of VTI running versus rolling volatility
#' dygraphs::dygraph(sqrt(datav[-(1:111), ]), main="Running and Rolling Volatility of VTI") %>%
#'   dyOptions(colors=c("red", "blue"), strokeWidth=1) %>%
#'   dyLegend(show="always", width=500)
#' # Compare the speed of running versus rolling volatility
#' library(microbenchmark)
#' summary(microbenchmark(
#'   running=HighFreq::run_var_ohlc(ohlc, lambda=0.8),
#'   rolling=HighFreq::roll_var_ohlc(ohlc, look_back=5, method="yang_zhang", scale=FALSE),
#'   times=10))[, c(1, 4, 5)]
#' }
#' @export
run_var_ohlc <- function(ohlc, lambda) {
    .Call('_HighFreq_run_var_ohlc', PACKAGE = 'HighFreq', ohlc, lambda)
}

#' Calculate the running covariance of two streaming \emph{time series} of
#' returns.
#' 
#' @param \code{tseries} A \emph{time series} or a \emph{matrix} with two
#'   columns of returns data.
#' 
#' @param \code{lambda} A \emph{numeric} decay factor to multiply past
#'   estimates.
#'   
#' @return A \emph{matrix} with three columns of data: the covariance and the
#'   variances of the two columns of the argument \code{tseries}.
#'
#' @details
#'   The function \code{run_covar()} calculates the running covariance of two
#'   streaming \emph{time series} of returns, by recursively weighing the
#'   products of their returns minus their means, with past covariance
#'   estimates \eqn{\sigma^{cov}_{t-1}}, using the decay factor \eqn{\lambda}:
#'   \deqn{
#'     \mu^1_t = (1-\lambda) r^1_t + \lambda \mu^1_{t-1}
#'   }
#'   \deqn{
#'     \mu^2_t = (1-\lambda) r^2_t + \lambda \mu^2_{t-1}
#'   }
#'   \deqn{
#'     \sigma^{cov}_t = (1-\lambda) (r^1_t - \mu^1_t) (r^2_t - \mu^2_t) + \lambda \sigma^{cov}_{t-1}
#'   }
#'   Where \eqn{\sigma^{cov}_t} is the covariance estimate at time \eqn{t},
#'   \eqn{r^1_t} and \eqn{r^2_t} are the two streaming returns data, and
#'   \eqn{\mu^1_t} and \eqn{\mu^2_t} are the means of the returns.
#' 
#'   The above recursive formula is convenient for processing live streaming
#'   data because it doesn't require maintaining a buffer of past data.
#'   The formula is equivalent to a convolution with exponentially decaying
#'   weights, but it's faster.
#' 
#'   The value of the decay factor \eqn{\lambda} should be in the range between
#'   \code{0} and \code{1}.  
#'   If \eqn{\lambda} is close to \code{1} then the decay is weak and past
#'   values have a greater weight, and the running covariance values have a
#'   stronger dependence on past values.  This is equivalent to a long
#'   look-back interval.
#'   If \eqn{\lambda} is much less than \code{1} then the decay is strong and
#'   past values have a smaller weight, and the running covariance values have
#'   a weaker dependence on past values.  This is equivalent to a short
#'   look-back interval.
#' 
#'   The function \code{run_covar()} returns three columns of data: the
#'   covariance and the variances of the two columns of the argument
#'   \code{tseries}.  This allows calculating the running correlation.
#' 
#' @examples
#' \dontrun{
#' # Calculate historical returns
#' returns <- zoo::coredata(na.omit(rutils::etfenv$returns[, c("IEF", "VTI")]))
#' # Calculate the running covariance
#' lambda <- 0.9
#' covars <- HighFreq::run_covar(returns, lambda=lambda)
#' # Calculate running covariance using R code
#' filtered <- (1-lambda)*filter(returns[, 1]*returns[, 2], 
#'   filter=lambda, init=as.numeric(returns[1, 1]*returns[1, 2])/(1-lambda), 
#'   method="recursive")
#' all.equal(covars[, 1], unclass(filtered), check.attributes=FALSE)
#' # Calculate the running correlation
#' correl <- covars[, 1]/sqrt(covars[, 2]*covars[, 3])
#' }
#' 
#' @export
run_covar <- function(tseries, lambda) {
    .Call('_HighFreq_run_covar', PACKAGE = 'HighFreq', tseries, lambda)
}

#' Perform running regressions of streaming \emph{time series} of response and
#' predictor data, and calculate the alphas, betas, and the residuals.
#' 
#' @param \code{response} A single-column \emph{time series} or a single-column
#'   \emph{matrix} of response data.
#' 
#' @param \code{predictor} A \emph{time series} or a \emph{matrix} of predictor
#'   data.
#' 
#' @param \code{lambda} A \emph{numeric} decay factor to multiply past
#'   estimates.
#'   
#' @param \code{method} A \emph{string} specifying the method for scaling the
#'   residuals (see Details) (the default is \code{method = "none"} - no
#'   scaling)
#'   
#' @return A \emph{matrix} with the regression alphas, betas, and residuals.
#'
#' @details
#'   The function \code{run_reg()} calculates the vectors of \emph{alphas}
#'   \eqn{\alpha_t}, \emph{betas} \eqn{\beta_t}, and the \emph{residuals}
#'   \eqn{\epsilon_t} of running regressions, by recursively weighing the
#'   current estimates with past estimates, using the decay factor
#'   \eqn{\lambda}:
#'   \deqn{
#'     \mu^r_t = (1-\lambda) r^r_t + \lambda \mu^r_{t-1}
#'   }
#'   \deqn{
#'     \mu^p_t = (1-\lambda) r^p_t + \lambda \mu^p_{t-1}
#'   }
#'   \deqn{
#'     \sigma^2_t = (1-\lambda) ({r^p_t}^2 - {\mu^p_t}^2) + \lambda \sigma^2_{t-1}
#'   }
#'   \deqn{
#'     \sigma^{cov}_t = (1-\lambda) (r^r_t - \mu^r_t) (r^p_t - \mu^p_t) + \lambda \sigma^{cov}_{t-1}
#'   }
#'   \deqn{
#'     \beta_t = (1-\lambda) \frac{\sigma^{cov}_t}{\sigma^2_t} + \lambda \beta_{t-1}
#'   }
#'   \deqn{
#'     \epsilon_t = (1-\lambda) (r^r_t - \beta_t r^p_t) + \lambda \epsilon_{t-1}
#'   }
#'   Where \eqn{\sigma^{cov}_t} are the covariances between the response and
#'   predictor data at time \eqn{t};
#'   \eqn{\sigma^2_t} is the vector of predictor variances,
#'   and \eqn{r^r_t} and \eqn{r^p_t} are the streaming data of the response
#'   and predictor data.
#' 
#'   The matrices \eqn{\sigma^2}, \eqn{\sigma^{cov}}, and \eqn{\beta} have the
#'   same dimensions as the input argument \code{predictor}.
#'
#'   The above recursive formulas are convenient for processing live streaming
#'   data because it doesn't require maintaining a buffer of past data.
#'   The formula is equivalent to a convolution with exponentially decaying
#'   weights, but it's faster to calculate.
#'
#'   The value of the decay factor \eqn{\lambda} should be in the range between
#'   \code{0} and \code{1}.
#'   If \eqn{\lambda} is close to \code{1} then the decay is weak and past
#'   values have a greater weight, and the running \emph{z-score} values have a
#'   stronger dependence on past values.  This is equivalent to a long
#'   look-back interval.
#'   If \eqn{\lambda} is much less than \code{1} then the decay is strong and
#'   past values have a smaller weight, and the running \emph{z-score} values
#'   have a weaker dependence on past values.  This is equivalent to a short
#'   look-back interval.
#' 
#'   The \emph{residuals} may be scaled by their volatilities. The default is
#'   \code{method = "none"} - no scaling.
#'   If the argument \code{method = "scale"} then the \emph{residuals}
#'   \eqn{\epsilon_t} are divided by their volatilities \eqn{\sigma^{\epsilon}}
#'   without subtracting their means:
#'   \deqn{
#'     \epsilon_t = \frac{\epsilon_t}{\sigma^{\epsilon}}
#'   }
#'   If the argument \code{method = "standardize"} then the means
#'   \eqn{\mu_{\epsilon}} are subtracted from the \emph{residuals}, and then
#'   they are divided by their volatilities \eqn{\sigma^{\epsilon}}:
#'   \deqn{
#'     \epsilon_t = \frac{\epsilon_t - \mu_{\epsilon}}{\sigma^{\epsilon}}
#'   }
#' 
#'   The function \code{run_reg()} returns multiple columns of data. If the
#'   matrix \code{predictor} has \code{n} columns then \code{run_reg()} returns
#'   a matrix with \code{n+2} columns.  The first column contains the
#'   \emph{residuals}, the second the \emph{alphas}, and the last columns
#'   contain the \emph{betas}.
#' 
#' @examples
#' \dontrun{
#' # Calculate historical returns
#' returns <- na.omit(rutils::etfenv$returns[, c("XLF", "VTI", "IEF")])
#' # Response equals XLF returns
#' response <- returns[, 1]
#' # Predictor matrix equals VTI and IEF returns
#' predictor <- returns[, -1]
#' # Calculate the running regressions
#' lambda <- 0.9
#' regs <- HighFreq::run_reg(response=response, predictor=predictor, lambda=lambda)
#' # Plot the running alphas
#' datav <- cbind(cumsum(response), regs[, 1])
#' colnames(datav) <- c("XLF", "alphas")
#' colnamev <- colnames(datav)
#' dygraphs::dygraph(datav, main="Alphas of XLF Versus VTI and IEF") %>%
#'   dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
#'   dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
#'   dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=1, col="blue") %>%
#'   dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=1, col="red")
#' }
#' 
#' @export
run_reg <- function(response, predictor, lambda, method = "none") {
    .Call('_HighFreq_run_reg', PACKAGE = 'HighFreq', response, predictor, lambda, method)
}

#' Calculate the z-scores of running regressions of streaming \emph{time
#' series} of returns.
#' 
#' @param \code{response} A single-column \emph{time series} or a single-column
#'   \emph{matrix} of response data.
#' 
#' @param \code{predictor} A \emph{time series} or a \emph{matrix} of predictor
#'   data.
#' 
#' @param \code{lambda} A \emph{numeric} decay factor to multiply past
#'   estimates.
#'   
#' @param \code{demean} A \emph{Boolean} specifying whether the \emph{z-scores}
#'   should be de-meaned (the default is \code{demean = TRUE}).
#'
#' @return A \emph{matrix} with the z-scores, betas, and the variances of the
#'   predictor data.
#'
#' @details
#'   The function \code{run_zscores()} calculates the vectors of \emph{betas}
#'   \eqn{\beta_t} and the residuals \eqn{\epsilon_t} of running regressions by
#'   recursively weighing the current estimates with past estimates, using the
#'   decay factor \eqn{\lambda}:
#'   \deqn{
#'     \sigma^2_t = (1-\lambda) {r^p_t}^2 + \lambda \sigma^2_{t-1}
#'   }
#'   \deqn{
#'     \sigma^{cov}_t = (1-\lambda) r^r_t r^p_t + \lambda \sigma^{cov}_{t-1}
#'   }
#'   \deqn{
#'     \beta_t = (1-\lambda) \frac{\sigma^{cov}_t}{\sigma^2_t} + \lambda \beta_{t-1}
#'   }
#'   \deqn{
#'     \epsilon_t = (1-\lambda) (r^r_t - \beta_t r^p_t) + \lambda \epsilon_{t-1}
#'   }
#'   Where \eqn{\sigma^{cov}_t} is the vector of covariances between the
#'   response and predictor returns, at time \eqn{t};
#'   \eqn{\sigma^2_t} is the vector of predictor variances,
#'   and \eqn{r^r_t} and \eqn{r^p_t} are the streaming returns of the response
#'   and predictor data.
#' 
#'   The above formulas for \eqn{\sigma^2} and \eqn{\sigma^{cov}} are
#'   approximate because they don't subtract the means before squaring the
#'   returns.  But they're very good approximations for daily returns.
#' 
#'   The matrices \eqn{\sigma^2}, \eqn{\sigma^{cov}}, \eqn{\beta} have the same
#'   dimensions as the input argument \code{predictor}.
#'
#'   If the argument \code{demean = TRUE} (the default) then the
#'   \emph{z-scores} \eqn{z_t} are calculated as equal to the residuals
#'   \eqn{\epsilon_t} minus their means \eqn{\mu_{\epsilon}}, divided by their
#'   volatilities \eqn{\sigma^{\epsilon}}:
#'   \deqn{
#'     z_t = \frac{\epsilon_t - \mu_{\epsilon}}{\sigma^{\epsilon}}
#'   }
#'   If the argument \code{demean = FALSE} then the \emph{z-scores} are
#'   only divided by their volatilities without subtracting their means:
#'   \deqn{
#'     z_t = \frac{\epsilon_t}{\sigma^{\epsilon}}
#'   }
#' 
#'   The above recursive formulas are convenient for processing live streaming
#'   data because it doesn't require maintaining a buffer of past data.
#'   The formula is equivalent to a convolution with exponentially decaying
#'   weights, but it's faster to calculate.
#' 
#'   The value of the decay factor \eqn{\lambda} should be in the range between
#'   \code{0} and \code{1}.
#'   If \eqn{\lambda} is close to \code{1} then the decay is weak and past
#'   values have a greater weight, and the running \emph{z-score} values have a
#'   stronger dependence on past values.  This is equivalent to a long
#'   look-back interval.
#'   If \eqn{\lambda} is much less than \code{1} then the decay is strong and
#'   past values have a smaller weight, and the running \emph{z-score} values
#'   have a weaker dependence on past values.  This is equivalent to a short
#'   look-back interval.
#' 
#'   The function \code{run_zscores()} returns multiple columns of data. 
#'   If the matrix \code{predictor} has \code{n} columns then \code{run_zscores()}
#'   returns a matrix with \code{2n+1} columns.  The first column contains the
#'   \emph{z-scores}, and the remaining columns contain the \emph{betas} and
#'   the \emph{variances} of the predictor data.
#' 
#' @examples
#' \dontrun{
#' # Calculate historical returns
#' returns <- na.omit(rutils::etfenv$returns[, c("XLF", "VTI", "IEF")])
#' # Response equals XLF returns
#' response <- returns[, 1]
#' # Predictor matrix equals VTI and IEF returns
#' predictor <- returns[, -1]
#' # Calculate the running z-scores
#' lambda <- 0.9
#' zscores <- HighFreq::run_zscores(response=response, predictor=predictor, lambda=lambda)
#' # Plot the running z-scores
#' datav <- cbind(cumsum(response), zscores[, 1])
#' colnames(datav) <- c("XLF", "zscores")
#' colnamev <- colnames(datav)
#' dygraphs::dygraph(datav, main="Z-Scores of XLF Versus VTI and IEF") %>%
#'   dyAxis("y", label=colnamev[1], independentTicks=TRUE) %>%
#'   dyAxis("y2", label=colnamev[2], independentTicks=TRUE) %>%
#'   dySeries(name=colnamev[1], axis="y", label=colnamev[1], strokeWidth=1, col="blue") %>%
#'   dySeries(name=colnamev[2], axis="y2", label=colnamev[2], strokeWidth=1, col="red")
#' }
#' 
#' @export
run_zscores <- function(response, predictor, lambda, demean = TRUE) {
    .Call('_HighFreq_run_zscores', PACKAGE = 'HighFreq', response, predictor, lambda, demean)
}

#' Calculate the mean (location) of the columns of a \emph{time series} or a
#' \emph{matrix} using \code{RcppArmadillo}.
#'
#' @param \code{tseries} A \emph{time series} or a \emph{matrix} of data.
#'
#' @param \code{method} A \emph{string} specifying the type of the mean
#'   (location) model (the default is \code{method = "moment"} - see Details).
#'
#' @param \code{conf_lev} The confidence level for calculating the
#'   quantiles (the default is \code{conf_lev = 0.75}).
#'
#' @return A single-row matrix with the mean (location) of the columns of
#'   \code{tseries}.
#'
#' @details
#'   The function \code{calc_mean()} calculates the mean (location) values of
#'   the columns of the \emph{time series} \code{tseries} using
#'   \code{RcppArmadillo} \code{C++} code.
#'
#'   If \code{method = "moment"} (the default) then \code{calc_mean()}
#'   calculates the location as the mean - the first moment of the data.
#'
#'   If \code{method = "quantile"} then it calculates the location \eqn{\mu} as
#'   the sum of the quantiles as follows:
#'   \deqn{
#'     \mu = q_{\alpha} + q_{1-\alpha}
#'   }
#'   Where \eqn{\alpha} is the confidence level for calculating the quantiles.
#'
#'   If \code{method = "nonparametric"} then it calculates the location as the
#'   median.
#'   
#'   The code examples below compare the function \code{calc_mean()} with the
#'   mean (location) calculated using \code{R} code.
#'
#' @examples
#' \dontrun{
#' # Calculate historical returns
#' returns <- na.omit(rutils::etfenv$returns[, c("XLP", "VTI")])
#' # Calculate the column means in RcppArmadillo
#' HighFreq::calc_mean(returns)
#' # Calculate the column means in R
#' sapply(returns, mean)
#' # Compare the values
#' all.equal(drop(HighFreq::calc_mean(returns)), 
#'   sapply(returns, mean), check.attributes=FALSE)
#' # Compare the speed of RcppArmadillo with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::calc_mean(returns),
#'   Rcode=sapply(returns, mean),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' # Calculate the quantile mean (location)
#' HighFreq::calc_mean(returns, method="quantile", conf_lev=0.9)
#' # Calculate the quantile mean (location) in R
#' colSums(sapply(returns, quantile, c(0.9, 0.1), type=5))
#' # Compare the values
#' all.equal(drop(HighFreq::calc_mean(returns, method="quantile", conf_lev=0.9)), 
#'   colSums(sapply(returns, quantile, c(0.9, 0.1), type=5)), 
#'   check.attributes=FALSE)
#' # Compare the speed of RcppArmadillo with R code
#' summary(microbenchmark(
#'   Rcpp=HighFreq::calc_mean(returns, method="quantile", conf_lev=0.9),
#'   Rcode=colSums(sapply(returns, quantile, c(0.9, 0.1), type=5)),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' # Calculate the column medians in RcppArmadillo
#' HighFreq::calc_mean(returns, method="nonparametric")
#' # Calculate the column medians in R
#' sapply(returns, median)
#' # Compare the values
#' all.equal(drop(HighFreq::calc_mean(returns, method="nonparametric")), 
#'   sapply(returns, median), check.attributes=FALSE)
#' # Compare the speed of RcppArmadillo with R code
#' summary(microbenchmark(
#'   Rcpp=HighFreq::calc_mean(returns, method="nonparametric"),
#'   Rcode=sapply(returns, median),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' 
#' @export
calc_mean <- function(tseries, method = "moment", conf_lev = 0.75) {
    .Call('_HighFreq_calc_mean', PACKAGE = 'HighFreq', tseries, method, conf_lev)
}

#' Calculate the variance of a single-column \emph{time series} or a
#' \emph{vector} using \code{RcppArmadillo}.
#' 
#' @param \code{tseries} A single-column \emph{time series} or a \emph{vector}.
#'
#' @return A \emph{numeric} value equal to the variance of the \emph{vector}.
#'
#' @details
#'   The function \code{calc_varvec()} calculates the variance of a
#'   \emph{vector} using \code{RcppArmadillo} \code{C++} code, so it's
#'   significantly faster than the \code{R} function \code{var()}.
#'
#' @examples
#' \dontrun{
#' # Create a vector of random returns
#' returns <- rnorm(1e6)
#' # Compare calc_varvec() with standard var()
#' all.equal(HighFreq::calc_varvec(returns), 
#'   var(returns))
#' # Compare the speed of RcppArmadillo with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::calc_varvec(returns),
#'   Rcode=var(returns),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' 
#' @export
calc_varvec <- function(tseries) {
    .Call('_HighFreq_calc_varvec', PACKAGE = 'HighFreq', tseries)
}

#' Calculate the dispersion (variance) of the columns of a \emph{time series}
#' or a \emph{matrix} using \code{RcppArmadillo}.
#' 
#' @param \code{tseries} A \emph{time series} or a \emph{matrix} of data.
#'   
#' @param \code{method} A \emph{string} specifying the type of the dispersion
#'   model (the default is \code{method = "moment"} - see Details).
#'    
#' @param \code{conf_lev} The confidence level for calculating the
#'   quantiles (the default is \code{conf_lev = 0.75}).
#'
#' @return A row vector equal to the dispersion of the columns of the matrix
#'   \code{tseries}.
#'
#' @details
#'   The dispersion is a measure of the variability of the data.  Examples of
#'   dispersion are the variance and the Median Absolute Deviation (\emph{MAD}).
#'
#'   The function \code{calc_var()} calculates the dispersion of the
#'   columns of a \emph{time series} or a \emph{matrix} of data using
#'   \code{RcppArmadillo} \code{C++} code.
#'   
#'   If \code{method = "moment"} (the default) then \code{calc_var()}
#'   calculates the dispersion as the second moment of the data \eqn{\sigma^2}
#'   (the variance).
#'
#'   If \code{method = "moment"} then \code{calc_var()} performs the same
#'   calculation as the function \code{colVars()} from package
#'   \href{https://cran.r-project.org/web/packages/matrixStats/index.html}{matrixStats},
#'   but it's much faster because it uses \code{RcppArmadillo} \code{C++} code.
#'
#'   If \code{method = "quantile"} then it calculates the dispersion as the
#'   difference between the quantiles as follows:
#'   \deqn{
#'     \mu = q_{\alpha} - q_{1-\alpha}
#'   }
#'   Where \eqn{\alpha} is the confidence level for calculating the quantiles.
#'   
#'   If \code{method = "nonparametric"} then it calculates the dispersion as the
#'   Median Absolute Deviation (\emph{MAD}):
#'   \deqn{
#'     MAD = median(abs(x - median(x)))
#'   }
#'   It also multiplies the \emph{MAD} by a factor of \code{1.4826}, to make it
#'   comparable to the standard deviation.
#'
#'   If \code{method = "nonparametric"} then \code{calc_var()} performs the
#'   same calculation as the function \code{stats::mad()}, but it's much faster
#'   because it uses \code{RcppArmadillo} \code{C++} code.
#'
#'   If the number of rows of \code{tseries} is less than \code{3} then it
#'   returns zeros.
#'   
#' @examples
#' \dontrun{
#' # Calculate VTI and XLF returns
#' returns <- na.omit(rutils::etfenv$returns[, c("VTI", "XLF")])
#' # Compare HighFreq::calc_var() with standard var()
#' all.equal(drop(HighFreq::calc_var(returns)), 
#'   apply(returns, 2, var), check.attributes=FALSE)
#' # Compare HighFreq::calc_var() with matrixStats
#' all.equal(drop(HighFreq::calc_var(returns)), 
#'   matrixStats::colVars(returns), check.attributes=FALSE)
#' # Compare the speed of RcppArmadillo with matrixStats and with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::calc_var(returns),
#'   matrixStats=matrixStats::colVars(returns),
#'   Rcode=apply(returns, 2, var),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' # Compare HighFreq::calc_var() with stats::mad()
#' all.equal(drop(HighFreq::calc_var(returns, method="nonparametric")), 
#'   sapply(returns, mad), check.attributes=FALSE)
#' # Compare the speed of RcppArmadillo with stats::mad()
#' summary(microbenchmark(
#'   Rcpp=HighFreq::calc_var(returns, method="nonparametric"),
#'   Rcode=sapply(returns, mad),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' 
#' @export
calc_var <- function(tseries, method = "moment", conf_lev = 0.75) {
    .Call('_HighFreq_calc_var', PACKAGE = 'HighFreq', tseries, method, conf_lev)
}

#' Calculate the variance of returns aggregated over end points. 
#'
#' @param \code{tseries} A \emph{time series} or a \emph{matrix} of prices.
#'
#' @param \code{step} The number of periods in each interval between
#'   neighboring end points.
#' 
#' @return The variance of aggregated returns.
#'
#' @details
#'   The function \code{calc_var_ag()} calculates the variance of returns
#'   aggregated over end points.
#'
#'   It first calculates the end points spaced apart by the number of periods
#'   equal to the argument \code{step}.  Then it calculates the aggregated
#'   returns by differencing the prices \code{tseries} calculated at the end
#'   points. Finally it calculates the variance of the returns.
#'
#'   If there are extra periods that don't fit over the length of
#'   \code{tseries}, then \code{calc_var_ag()} loops over all possible stub
#'   intervals, then it calculates all the corresponding variance values, and
#'   averages them.
#'
#'   For example, if the number of rows of \code{tseries} is equal to
#'   \code{20}, and \code{step=3} then \code{6} end points fit over the length
#'   of \code{tseries}, and there are \code{2} extra periods that must fit into
#'   stubs, either at the beginning or at the end (or both).
#' 
#'   The aggregated volatility \eqn{\sigma_t} scales (increases) with the
#'   length of the aggregation interval \eqn{\Delta t} raised to the power of
#'   the \emph{Hurst exponent} \eqn{H}:
#'     \deqn{
#'       \sigma_t = \sigma {\Delta t}^H
#'     }
#'   Where \eqn{\sigma} is the daily return volatility.
#' 
#'   The function \code{calc_var_ag()} can therefore be used to calculate the
#'   \emph{Hurst exponent} from the volatility ratio.
#'
#' @examples
#' \dontrun{
#' # Calculate the log prices
#' prices <- na.omit(rutils::etfenv$prices[, c("XLP", "VTI")])
#' prices <- log(prices)
#' # Calculate the daily variance of percentage returns
#' calc_var_ag(prices, step=1)
#' # Calculate the daily variance using R
#' sapply(rutils::diffit(prices), var)
#' # Calculate the variance of returns aggregated over 21 days
#' calc_var_ag(prices, step=21)
#' # The variance over 21 days is approximately 21 times the daily variance
#' 21*calc_var_ag(prices, step=1)
#' }
#' 
#' @export
calc_var_ag <- function(tseries, step = 1L) {
    .Call('_HighFreq_calc_var_ag', PACKAGE = 'HighFreq', tseries, step)
}

#' Calculate the variance of returns from \emph{OHLC} prices using different
#' price range estimators.
#'
#' @param \code{ohlc} A \emph{time series} or a \emph{matrix} of \emph{OHLC}
#'   prices.
#'   
#' @param \code{method} A \emph{character} string representing the price range
#'   estimator for calculating the variance.  The estimators include:
#'   \itemize{
#'     \item "close" close-to-close estimator,
#'     \item "rogers_satchell" Rogers-Satchell estimator,
#'     \item "garman_klass" Garman-Klass estimator,
#'     \item "garman_klass_yz" Garman-Klass with account for close-to-open price jumps,
#'     \item "yang_zhang" Yang-Zhang estimator,
#'    }
#'    (The default is the \code{method = "yang_zhang"}.)
#'    
#' @param \code{close_lag} A \emph{vector} with the lagged \emph{close} prices
#'   of the \emph{OHLC time series}.  This is an optional argument. (The
#'   default is \code{close_lag = 0}).
#'   
#' @param \code{scale} \emph{Boolean} argument: Should the returns be divided
#'   by the time index, the number of seconds in each period? (The default is
#'   \code{scale = TRUE}).
#'
#' @param \code{index} A \emph{vector} with the time index of the \emph{time
#'   series}.  This is an optional argument (the default is \code{index = 0}).
#'   
#' @return A single \emph{numeric} value equal to the variance of the
#'   \emph{OHLC time series}.
#'
#' @details
#'   The function \code{calc_var_ohlc()} calculates the variance from all the
#'   different intra-day and day-over-day returns (defined as the differences
#'   of \emph{OHLC} prices), using several different variance estimation
#'   methods.
#'
#'   The function \code{calc_var_ohlc()} does not calculate the logarithm of
#'   the prices.
#'   So if the argument \code{ohlc} contains dollar prices then
#'   \code{calc_var_ohlc()} calculates the dollar variance.
#'   If the argument \code{ohlc} contains the log prices then
#'   \code{calc_var_ohlc()} calculates the percentage variance.
#'
#'   The default \code{method} is \emph{"yang_zhang"}, which theoretically
#'   has the lowest standard error among unbiased estimators.
#'   The methods \emph{"close"}, \emph{"garman_klass_yz"}, and
#'   \emph{"yang_zhang"} do account for \emph{close-to-open} price jumps, while
#'   the methods \emph{"garman_klass"} and \emph{"rogers_satchell"} do not
#'   account for \emph{close-to-open} price jumps.
#'
#'   If \code{scale} is \code{TRUE} (the default), then the returns are
#'   divided by the differences of the time index (which scales the variance to
#'   the units of variance per second squared). This is useful when calculating
#'   the variance from minutely bar data, because dividing returns by the
#'   number of seconds decreases the effect of overnight price jumps. If the
#'   time index is in days, then the variance is equal to the variance per day
#'   squared.
#'   
#'   If the number of rows of \code{ohlc} is less than \code{3} then it
#'   returns zero.
#'   
#'   The optional argument \code{index} is the time index of the \emph{time
#'   series} \code{ohlc}. If the time index is in seconds, then the
#'   differences of the index are equal to the number of seconds in each time
#'   period.  If the time index is in days, then the differences are equal to
#'   the number of days in each time period.
#'   
#'   The optional argument \code{close_lag} are the lagged \emph{close} prices
#'   of the \emph{OHLC time series}.  Passing in the lagged \emph{close} prices
#'   speeds up the calculation, so it's useful for rolling calculations.
#'   
#'   The function \code{calc_var_ohlc()} is implemented in \code{RcppArmadillo}
#'   \code{C++} code, and it's over \code{10} times faster than
#'   \code{calc_var_ohlc_r()}, which is implemented in \code{R} code.
#'
#' @examples
#' \dontrun{
#' # Extract the log OHLC prices of SPY
#' ohlc <- log(HighFreq::SPY)
#' # Extract the time index of SPY prices
#' indeks <- c(1, diff(xts::.index(ohlc)))
#' # Calculate the variance of SPY returns, with scaling of the returns
#' HighFreq::calc_var_ohlc(ohlc, 
#'  method="yang_zhang", scale=TRUE, index=indeks)
#' # Calculate variance without accounting for overnight jumps
#' HighFreq::calc_var_ohlc(ohlc, 
#'  method="rogers_satchell", scale=TRUE, index=indeks)
#' # Calculate the variance without scaling the returns
#' HighFreq::calc_var_ohlc(ohlc, scale=FALSE)
#' # Calculate the variance by passing in the lagged close prices
#' close_lag <- HighFreq::lagit(ohlc[, 4])
#' all.equal(HighFreq::calc_var_ohlc(ohlc), 
#'   HighFreq::calc_var_ohlc(ohlc, close_lag=close_lag))
#' # Compare with HighFreq::calc_var_ohlc_r()
#' all.equal(HighFreq::calc_var_ohlc(ohlc, index=indeks), 
#'   HighFreq::calc_var_ohlc_r(ohlc))
#' # Compare the speed of Rcpp with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::calc_var_ohlc(ohlc),
#'   Rcode=HighFreq::calc_var_ohlc_r(ohlc),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' @export
calc_var_ohlc <- function(ohlc, method = "yang_zhang", close_lag = 0L, scale = TRUE, index = 0L) {
    .Call('_HighFreq_calc_var_ohlc', PACKAGE = 'HighFreq', ohlc, method, close_lag, scale, index)
}

#' Calculate the variance of aggregated \emph{OHLC} prices using different
#' price range estimators.
#'
#' @param \code{ohlc} A \emph{time series} or a \emph{matrix} of \emph{OHLC}
#'   prices.
#'
#' @param \code{step} The number of periods in each interval between
#'   neighboring end points.
#' 
#' @param \code{method} A \emph{character} string representing the price range
#'   estimator for calculating the variance.  The estimators include:
#'   \itemize{
#'     \item "close" close-to-close estimator,
#'     \item "rogers_satchell" Rogers-Satchell estimator,
#'     \item "garman_klass" Garman-Klass estimator,
#'     \item "garman_klass_yz" Garman-Klass with account for close-to-open price jumps,
#'     \item "yang_zhang" Yang-Zhang estimator,
#'    }
#'    (The default is the \code{method = "yang_zhang"}.)
#'    
#' @param \code{close_lag} A \emph{vector} with the lagged \emph{close} prices
#'   of the \emph{OHLC time series}.  This is an optional argument. (The
#'   default is \code{close_lag = 0}).
#'   
#' @param \code{scale} \emph{Boolean} argument: Should the returns be divided
#'   by the time index, the number of seconds in each period? (The default is
#'   \code{scale = TRUE}).
#'
#' @param \code{index} A \emph{vector} with the time index of the \emph{time
#'   series}.  This is an optional argument (the default is \code{index = 0}).
#'   
#' @return The variance of aggregated \emph{OHLC} prices.
#'
#' @details
#'   The function \code{calc_var_ohlc_ag()} calculates the variance of
#'   \emph{OHLC} prices aggregated over end points.
#'
#'   It first calculates the end points spaced apart by the number of periods
#'   equal to the argument \code{step}.  Then it aggregates the \emph{OHLC}
#'   prices to the end points. Finally it calculates the variance of the
#'   aggregated \emph{OHLC} prices.
#'
#'   If there are extra periods that don't fit over the length of \code{ohlc},
#'   then \code{calc_var_ohlc_ag()} loops over all possible stub intervals,
#'   it calculates all the corresponding variance values, and it averages
#'   them.
#'
#'   For example, if the number of rows of \code{ohlc} is equal to \code{20},
#'   and \code{step=3} then \code{6} end points fit over the length of
#'   \code{ohlc}, and there are \code{2} extra periods that must fit into
#'   stubs, either at the beginning or at the end (or both).
#' 
#'   The aggregated volatility \eqn{\sigma_t} scales (increases) with the
#'   length of the aggregation interval \eqn{\Delta t} raised to the power of
#'   the \emph{Hurst exponent} \eqn{H}:
#'     \deqn{
#'       \sigma_t = \sigma {\Delta t}^H
#'     }
#'   Where \eqn{\sigma} is the daily return volatility.
#' 
#'   The function \code{calc_var_ohlc_ag()} can therefore be used to calculate the
#'   \emph{Hurst exponent} from the volatility ratio.
#'
#' @examples
#' \dontrun{
#' # Calculate the log ohlc prices
#' ohlc <- log(rutils::etfenv$VTI)
#' # Calculate the daily variance of percentage returns
#' calc_var_ohlc_ag(ohlc, step=1)
#' # Calculate the variance of returns aggregated over 21 days
#' calc_var_ohlc_ag(ohlc, step=21)
#' # The variance over 21 days is approximately 21 times the daily variance
#' 21*calc_var_ohlc_ag(ohlc, step=1)
#' }
#' 
#' @export
calc_var_ohlc_ag <- function(ohlc, step = 1L, method = "yang_zhang", close_lag = 0L, scale = TRUE, index = 0L) {
    .Call('_HighFreq_calc_var_ohlc_ag', PACKAGE = 'HighFreq', ohlc, step, method, close_lag, scale, index)
}

#' Calculate the skewness of the columns of a \emph{time series} or a
#' \emph{matrix} using \code{RcppArmadillo}.
#'
#' @param \code{tseries} A \emph{time series} or a \emph{matrix} of data.
#'
#' @param \code{method} A \emph{string} specifying the type of the skewness
#'   model (the default is \code{method = "moment"} - see Details).
#'
#' @param \code{conf_lev} The confidence level for calculating the
#'   quantiles (the default is \code{conf_lev = 0.75}).
#'
#' @return A single-row matrix with the skewness of the columns of
#'   \code{tseries}.
#'
#' @details
#'   The function \code{calc_skew()} calculates the skewness of the columns of
#'   a \emph{time series} or a \emph{matrix} of data using \code{RcppArmadillo}
#'   \code{C++} code.
#'
#'   If \code{method = "moment"} (the default) then \code{calc_skew()}
#'   calculates the skewness as the third moment of the data.
#'
#'   If \code{method = "quantile"} then it calculates the skewness
#'   \eqn{\varsigma} from the differences between the quantiles of the data as
#'   follows:
#'   \deqn{
#'     \varsigma = \frac{q_{\alpha} + q_{1-\alpha} - 2*q_{0.5}}{q_{\alpha} - q_{1-\alpha}}
#'   }
#'   Where \eqn{\alpha} is the confidence level for calculating the quantiles.
#'
#'   If \code{method = "nonparametric"} then it calculates the skewness as the
#'   difference between the mean of the data minus its median, divided by the
#'   standard deviation.
#'   
#'   If the number of rows of \code{tseries} is less than \code{3} then it
#'   returns zeros.
#'   
#'   The code examples below compare the function \code{calc_skew()} with the
#'   skewness calculated using \code{R} code.
#'
#' @examples
#' \dontrun{
#' # Define a single-column time series of returns
#' returns <- na.omit(rutils::etfenv$returns$VTI)
#' # Calculate the moment skewness
#' HighFreq::calc_skew(returns)
#' # Calculate the moment skewness in R
#' calc_skewr <- function(x) {
#'   x <- (x-mean(x))
#'   sum(x^3)/var(x)^1.5/NROW(x)
#' }  # end calc_skewr
#' all.equal(HighFreq::calc_skew(returns), 
#'   calc_skewr(returns), check.attributes=FALSE)
#' # Compare the speed of RcppArmadillo with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::calc_skew(returns),
#'   Rcode=calc_skewr(returns),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' # Calculate the quantile skewness
#' HighFreq::calc_skew(returns, method="quantile", conf_lev=0.9)
#' # Calculate the quantile skewness in R
#' calc_skewq <- function(x, a = 0.75) {
#'   	quantiles <- quantile(x, c(1-a, 0.5, a), type=5)
#'   	(quantiles[3] + quantiles[1] - 2*quantiles[2])/(quantiles[3] - quantiles[1])
#' }  # end calc_skewq
#' all.equal(drop(HighFreq::calc_skew(returns, method="quantile", conf_lev=0.9)), 
#'   calc_skewq(returns, a=0.9), check.attributes=FALSE)
#' # Compare the speed of RcppArmadillo with R code
#' summary(microbenchmark(
#'   Rcpp=HighFreq::calc_skew(returns, method="quantile"),
#'   Rcode=calc_skewq(returns),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' # Calculate the nonparametric skewness
#' HighFreq::calc_skew(returns, method="nonparametric")
#' # Compare HighFreq::calc_skew() with R nonparametric skewness
#' all.equal(drop(HighFreq::calc_skew(returns, method="nonparametric")), 
#'   (mean(returns)-median(returns))/sd(returns), 
#'   check.attributes=FALSE)
#' # Compare the speed of RcppArmadillo with R code
#' summary(microbenchmark(
#'   Rcpp=HighFreq::calc_skew(returns, method="nonparametric"),
#'   Rcode=(mean(returns)-median(returns))/sd(returns),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' 
#' @export
calc_skew <- function(tseries, method = "moment", conf_lev = 0.75) {
    .Call('_HighFreq_calc_skew', PACKAGE = 'HighFreq', tseries, method, conf_lev)
}

#' Calculate the kurtosis of the columns of a \emph{time series} or a
#' \emph{matrix} using \code{RcppArmadillo}.
#'
#' @param \code{tseries} A \emph{time series} or a \emph{matrix} of data.
#'
#' @param \code{method} A \emph{string} specifying the type of the kurtosis
#'   model (the default is \code{method = "moment"} - see Details).
#'
#' @param \code{conf_lev} The confidence level for calculating the
#'   quantiles (the default is \code{conf_lev = 0.75}).
#'
#' @return A single-row matrix with the kurtosis of the columns of
#'   \code{tseries}.
#'
#' @details
#'   The function \code{calc_kurtosis()} calculates the kurtosis of the columns
#'   of the \emph{matrix} \code{tseries} using \code{RcppArmadillo} \code{C++}
#'   code.
#'
#'   If \code{method = "moment"} (the default) then \code{calc_kurtosis()}
#'   calculates the fourth moment of the data.
#'   But it doesn't de-mean the columns of \code{tseries} because that requires
#'   copying the matrix \code{tseries}, so it's time-consuming.
#'
#'   If \code{method = "quantile"} then it calculates the skewness
#'   \eqn{\kappa} from the differences between the quantiles of the data as
#'   follows:
#'   \deqn{
#'     \kappa = \frac{q_{\alpha} - q_{1-\alpha}}{q_{0.75} - q_{0.25}}
#'   }
#'   Where \eqn{\alpha} is the confidence level for calculating the quantiles.
#'
#'   If \code{method = "nonparametric"} then it calculates the kurtosis as the
#'   difference between the mean of the data minus its median, divided by the
#'   standard deviation.
#'   
#'   If the number of rows of \code{tseries} is less than \code{3} then it
#'   returns zeros.
#'   
#'   The code examples below compare the function \code{calc_kurtosis()} with the
#'   kurtosis calculated using \code{R} code.
#'
#' @examples
#' \dontrun{
#' # Define a single-column time series of returns
#' returns <- na.omit(rutils::etfenv$returns$VTI)
#' # Calculate the moment kurtosis
#' HighFreq::calc_kurtosis(returns)
#' # Calculate the moment kurtosis in R
#' calc_kurtr <- function(x) {
#'   x <- (x-mean(x))
#'   sum(x^4)/var(x)^2/NROW(x)
#' }  # end calc_kurtr
#' all.equal(HighFreq::calc_kurtosis(returns), 
#'   calc_kurtr(returns), check.attributes=FALSE)
#' # Compare the speed of RcppArmadillo with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::calc_kurtosis(returns),
#'   Rcode=calc_kurtr(returns),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' # Calculate the quantile kurtosis
#' HighFreq::calc_kurtosis(returns, method="quantile", conf_lev=0.9)
#' # Calculate the quantile kurtosis in R
#' calc_kurtq <- function(x, a=0.9) {
#'   	quantiles <- quantile(x, c(1-a, 0.25, 0.75, a), type=5)
#'   	(quantiles[4] - quantiles[1])/(quantiles[3] - quantiles[2])
#' }  # end calc_kurtq
#' all.equal(drop(HighFreq::calc_kurtosis(returns, method="quantile", conf_lev=0.9)), 
#'   calc_kurtq(returns, a=0.9), check.attributes=FALSE)
#' # Compare the speed of RcppArmadillo with R code
#' summary(microbenchmark(
#'   Rcpp=HighFreq::calc_kurtosis(returns, method="quantile"),
#'   Rcode=calc_kurtq(returns),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' # Calculate the nonparametric kurtosis
#' HighFreq::calc_kurtosis(returns, method="nonparametric")
#' # Compare HighFreq::calc_kurtosis() with R nonparametric kurtosis
#' all.equal(drop(HighFreq::calc_kurtosis(returns, method="nonparametric")), 
#'   (mean(returns)-median(returns))/sd(returns), 
#'   check.attributes=FALSE)
#' # Compare the speed of RcppArmadillo with R code
#' summary(microbenchmark(
#'   Rcpp=HighFreq::calc_kurtosis(returns, method="nonparametric"),
#'   Rcode=(mean(returns)-median(returns))/sd(returns),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' 
#' @export
calc_kurtosis <- function(tseries, method = "moment", conf_lev = 0.75) {
    .Call('_HighFreq_calc_kurtosis', PACKAGE = 'HighFreq', tseries, method, conf_lev)
}

#' Calculate the Hurst exponent from the volatility ratio of aggregated returns.
#'
#' @param \code{tseries} A \emph{time series} or a \emph{matrix} of prices.
#'
#' @param \code{step} The number of periods in each interval between
#'   neighboring end points.
#' 
#' @return The Hurst exponent calculated from the variance of aggregated
#'   returns.
#'
#' @details
#'   The function \code{calc_hurst()} calculates the Hurst exponent from the
#'   ratios of the volatilities of aggregated returns.
#'
#'   The aggregated volatility \eqn{\sigma_t} scales (increases) with the
#'   length of the aggregation interval \eqn{\Delta t} raised to the power of
#'   the \emph{Hurst exponent} \eqn{H}:
#'     \deqn{
#'       \sigma_t = \sigma {\Delta t}^H
#'     }
#'   Where \eqn{\sigma} is the daily return volatility.
#' 
#'   The \emph{Hurst exponent} \eqn{H} is equal to the logarithm of the ratio
#'   of the volatilities divided by the logarithm of the time interval
#'   \eqn{\Delta t}:
#'     \deqn{
#'       H = \frac{\log{\sigma_t} - \log{\sigma}}{\log{\Delta t}}
#'     }
#' 
#'   The function \code{calc_hurst()} calls the function \code{calc_var_ag()}
#'   to calculate the aggregated volatility \eqn{\sigma_t}.
#' 
#' @examples
#' \dontrun{
#' # Calculate the log prices
#' prices <- na.omit(rutils::etfenv$prices[, c("XLP", "VTI")])
#' prices <- log(prices)
#' # Calculate the Hurst exponent from 21 day aggregations
#' calc_hurst(prices, step=21)
#' }
#' 
#' @export
calc_hurst <- function(tseries, step = 1L) {
    .Call('_HighFreq_calc_hurst', PACKAGE = 'HighFreq', tseries, step)
}

#' Calculate the Hurst exponent from the volatility ratio of aggregated
#' \emph{OHLC} prices.
#'
#' @param \code{ohlc} A \emph{time series} or a \emph{matrix} of \emph{OHLC}
#'   prices.
#'
#' @param \code{step} The number of periods in each interval between
#'   neighboring end points.
#' 
#' @param \code{method} A \emph{character} string representing the price range
#'   estimator for calculating the variance.  The estimators include:
#'   \itemize{
#'     \item "close" close-to-close estimator,
#'     \item "rogers_satchell" Rogers-Satchell estimator,
#'     \item "garman_klass" Garman-Klass estimator,
#'     \item "garman_klass_yz" Garman-Klass with account for close-to-open price jumps,
#'     \item "yang_zhang" Yang-Zhang estimator,
#'    }
#'    (The default is the \code{method = "yang_zhang"}.)
#'    
#' @param \code{close_lag} A \emph{vector} with the lagged \emph{close} prices
#'   of the \emph{OHLC time series}.  This is an optional argument. (The
#'   default is \code{close_lag = 0}).
#'   
#' @param \code{scale} \emph{Boolean} argument: Should the returns be divided
#'   by the time index, the number of seconds in each period? (The default is
#'   \code{scale = TRUE}).
#'
#' @param \code{index} A \emph{vector} with the time index of the \emph{time
#'   series}.  This is an optional argument (the default is \code{index = 0}).
#'   
#' @return The Hurst exponent calculated from the variance ratio of aggregated
#' \emph{OHLC} prices.
#'
#' @details
#' The function \code{calc_hurst_ohlc()} calculates the Hurst exponent from the
#' ratios of the volatilities of aggregated \emph{OHLC} prices.
#'
#'   The aggregated volatility \eqn{\sigma_t} scales (increases) with the
#'   length of the aggregation interval \eqn{\Delta t} raised to the power of
#'   the \emph{Hurst exponent} \eqn{H}:
#'     \deqn{
#'       \sigma_t = \sigma {\Delta t}^H
#'     }
#'   Where \eqn{\sigma} is the daily return volatility.
#' 
#'   The \emph{Hurst exponent} \eqn{H} is equal to the logarithm of the ratio
#'   of the volatilities divided by the logarithm of the time interval
#'   \eqn{\Delta t}:
#'     \deqn{
#'       H = \frac{\log{\sigma_t} - \log{\sigma}}{\log{\Delta t}}
#'     }
#' 
#'   The function \code{calc_hurst_ohlc()} calls the function
#'   \code{calc_var_ohlc_ag()} to calculate the aggregated volatility
#'   \eqn{\sigma_t}.
#' 
#' @examples
#' \dontrun{
#' # Calculate the log ohlc prices
#' ohlc <- log(rutils::etfenv$VTI)
#' # Calculate the Hurst exponent from 21 day aggregations
#' calc_hurst_ohlc(ohlc, step=21)
#' }
#' 
#' @export
calc_hurst_ohlc <- function(ohlc, step = 1L, method = "yang_zhang", close_lag = 0L, scale = TRUE, index = 0L) {
    .Call('_HighFreq_calc_hurst_ohlc', PACKAGE = 'HighFreq', ohlc, step, method, close_lag, scale, index)
}

#' Perform multivariate linear regression using least squares and return a
#' named list of regression coefficients, their t-values, and p-values.
#' 
#' @param \code{response} A single-column \emph{time series} or a \emph{vector}
#'   of response data.
#' 
#' @param \code{predictor} A \emph{time series} or a \emph{matrix} of predictor
#'   data.
#' 
#' @return A named list with three elements: a \emph{matrix} of coefficients
#'   (named \emph{"coefficients"}), the \emph{z-score} of the last residual
#'   (named \emph{"zscore"}), and a \emph{vector} with the R-squared and
#'   F-statistic (named \emph{"stats"}). The numeric \emph{matrix} of
#'   coefficients named \emph{"coefficients"} contains the alpha and beta
#'   coefficients, and their \emph{t-values} and \emph{p-values}.
#'
#' @details
#'   The function \code{calc_lm()} performs the same calculations as the
#'   function \code{lm()} from package \emph{stats}. 
#'   It uses \code{RcppArmadillo} \code{C++} code so it's several times faster
#'   than \code{lm()}. The code was inspired by this article (but it's not
#'   identical to it):
#'   http://gallery.rcpp.org/articles/fast-linear-model-with-armadillo/
#'
#' @examples
#' \dontrun{
#' # Calculate historical returns
#' returns <- na.omit(rutils::etfenv$returns[, c("XLF", "VTI", "IEF")])
#' # Response equals XLF returns
#' response <- returns[, 1]
#' # Predictor matrix equals VTI and IEF returns
#' predictor <- returns[, -1]
#' # Perform multivariate regression using lm()
#' reg_model <- lm(response ~ predictor)
#' sum_mary <- summary(reg_model)
#' # Perform multivariate regression using calc_lm()
#' reg_arma <- HighFreq::calc_lm(response=response, predictor=predictor)
#' # Compare the outputs of both functions
#' all.equal(reg_arma$coefficients[, "coeff"], unname(coef(reg_model)))
#' all.equal(unname(reg_arma$coefficients), unname(sum_mary$coefficients))
#' all.equal(unname(reg_arma$stats), c(sum_mary$r.squared, unname(sum_mary$fstatistic[1])))
#' # Compare the speed of RcppArmadillo with R code
#' summary(microbenchmark(
#'   Rcpp=HighFreq::calc_lm(response=response, predictor=predictor),
#'   Rcode=lm(response ~ predictor),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' 
#' @export
calc_lm <- function(response, predictor) {
    .Call('_HighFreq_calc_lm', PACKAGE = 'HighFreq', response, predictor)
}

#' Perform multivariate regression using different methods, and return a vector
#' of regression coefficients, their t-values, and the last residual z-score.
#' 
#' @param \code{response} A single-column \emph{time series} or a \emph{vector}
#'   of response data.
#' 
#' @param \code{predictor} A \emph{time series} or a \emph{matrix} of predictor
#'   data.
#' 
#' @param \code{intercept} A \emph{Boolean} specifying whether an intercept
#'   term should be added to the predictor (the default is \code{intercept =
#'   FALSE}).
#'
#' @param \code{method} A \emph{string} specifying the type of the regression
#'   model the default is \code{method = "least_squares"} - see Details).
#'   
#' @param \code{eigen_thresh} A \emph{numeric} threshold level for discarding
#'   small singular values in order to regularize the inverse of the
#'   \code{predictor} matrix (the default is \code{1e-5}).
#'   
#' @param \code{eigen_max} An \emph{integer} equal to the number of singular
#'   values used for calculating the shrinkage inverse of the \code{predictor}
#'   matrix (the default is \code{0} - equivalent to \code{eigen_max} equal to
#'   the number of columns of \code{predictor}).
#'   
#' @param \code{conf_lev} The confidence level for calculating the
#'   quantiles (the default is \code{conf_lev = 0.75}).
#'
#' @param \code{alpha} The shrinkage intensity between \code{0} and \code{1}.
#'   (the default is \code{0}).
#' 
#' @return A single-row matrix with
#' A vector with the regression coefficients, their t-values, and the
#'   last residual z-score.
#'
#' @details
#'   The function \code{calc_reg()} performs multivariate regression using
#'   different methods, and returns a vector of regression coefficients, their
#'   t-values, and the last residual z-score.
#'
#'   If \code{method = "least_squares"} (the default) then it performs the
#'   standard least squares regression, the same as the function
#'   \code{calc_lm()}, and the function \code{lm()} from the \code{R} package
#'   \emph{stats}.
#'   But it uses \code{RcppArmadillo} \code{C++} code so it's several times
#'   faster than \code{lm()}.
#'
#'   If \code{method = "regular"} then it performs shrinkage regression.  It
#'   calculates the shrinkage inverse of the \code{predictor} matrix from its
#'   singular value decomposition.  It applies dimension regularization by
#'   selecting only the largest singular values equal in number to
#'   \code{eigen_max}.
#'   
#'   If \code{method = "quantile"} then it performs quantile regression (not
#'   implemented yet).
#' 
#'   If \code{intercept = TRUE} then an extra intercept column (unit column) is
#'   added to the predictor matrix (the default is \code{intercept = FALSE}).
#'   
#'   The length of the return vector depends on the number of columns of the
#'   \code{predictor} matrix (including the intercept column, if it's added).
#'   The length of the return vector is equal to the number of regression
#'   coefficients, plus their t-values, plus the z-score.
#'   The number of regression coefficients is equal to the number of columns of
#'   the \code{predictor} matrix (including the intercept column, if it's
#'   added).
#'   The number of t-values is equal to the number of coefficients.
#' 
#'   For example, if the number of columns of the \code{predictor} matrix is
#'   equal to \code{n}, and if \code{intercept = TRUE}, then \code{calc_reg()}
#'   returns a vector with \code{2n+3} elements: \code{n+1} regression
#'   coefficients (including the intercept coefficient), \code{n+1}
#'   corresponding t-values, and \code{1} z-score.
#'
#' @examples
#' \dontrun{
#' # Calculate historical returns
#' returns <- na.omit(rutils::etfenv$returns[, c("XLF", "VTI", "IEF")])
#' # Response equals XLF returns
#' response <- returns[, 1]
#' # Predictor matrix equals VTI and IEF returns
#' predictor <- returns[, -1]
#' # Perform multivariate regression using lm()
#' reg_model <- lm(response ~ predictor)
#' sum_mary <- summary(reg_model)
#' coeff <- sum_mary$coefficients
#' # Perform multivariate regression using calc_reg()
#' reg_arma <- drop(HighFreq::calc_reg(response=response, predictor=predictor))
#' # Compare the outputs of both functions
#' all.equal(reg_arma[1:(2*(1+NCOL(predictor)))], 
#'   c(coeff[, "Estimate"], coeff[, "t value"]), check.attributes=FALSE)
#' # Compare the speed of RcppArmadillo with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::calc_reg(response=response, predictor=predictor),
#'   Rcode=lm(response ~ predictor),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' 
#' @export
calc_reg <- function(response, predictor, intercept = FALSE, method = "least_squares", eigen_thresh = 1e-5, eigen_max = 0L, conf_lev = 0.1, alpha = 0.0) {
    .Call('_HighFreq_calc_reg', PACKAGE = 'HighFreq', response, predictor, intercept, method, eigen_thresh, eigen_max, conf_lev, alpha)
}

#' Calculate a \emph{matrix} of mean (location) estimates over a rolling
#' look-back interval attached at the end points of a \emph{time series} or a
#' \emph{matrix}.
#'
#' @param \code{tseries} A \emph{time series} or a \emph{matrix} of data.
#'
#' @param \code{startp} An \emph{integer} vector of start points (the default
#'   is \code{startp = 0}).
#' 
#' @param \code{endp} An \emph{integer} vector of end points (the default is
#'   \code{endp = 0}).
#' 
#' @param \code{step} The number of time periods between the end points (the
#'   default is \code{step = 1}).
#'
#' @param \code{look_back} The number of end points in the look-back interval
#'   (the default is \code{look_back = 1}).
#'   
#' @param \code{stub} An \emph{integer} value equal to the first end point for
#'   calculating the end points (the default is \code{stub = 0}).
#' 
#' @param \code{method} A \emph{character} string representing the type of mean 
#'   measure of (the default is \code{method = "moment"}).
#'
#' @return A \emph{matrix} of mean (location) estimates with the same number of
#'   columns as the input time series \code{tseries}, and the number of rows
#'   equal to the number of end points.
#'   
#' @details
#'   The function \code{roll_mean()} calculates a \emph{matrix} of mean
#'   (location) estimates over rolling look-back intervals attached at the end
#'   points of the \emph{time series} \code{tseries}.
#'   
#'   The function \code{roll_mean()} performs a loop over the end points, and at
#'   each end point it subsets the time series \code{tseries} over a look-back
#'   interval equal to \code{look_back} number of end points.
#'   
#'   It passes the subset time series to the function \code{calc_mean()}, which
#'   calculates the mean (location).
#'   See the function \code{calc_mean()} for a description of the mean methods.
#'   
#'   If the arguments \code{endp} and \code{startp} are not given then it
#'   first calculates a vector of end points separated by \code{step} time
#'   periods. It calculates the end points along the rows of \code{tseries}
#'   using the function \code{calc_endpoints()}, with the number of time
#'   periods between the end points equal to \code{step} time periods.
#' 
#'   For example, the rolling mean at \code{25} day end points, with a
#'   \code{75} day look-back, can be calculated using the parameters
#'   \code{step = 25} and \code{look_back = 3}.
#'
#'   The function \code{roll_mean()} with the parameter \code{step = 1}
#'   performs the same calculation as the function \code{roll_mean()} from
#'   package
#'   \href{https://cran.r-project.org/web/packages/RcppRoll/index.html}{RcppRoll},
#'   but it's several times faster because it uses \code{RcppArmadillo}
#'   \code{C++} code.
#'
#'   The function \code{roll_mean()} is implemented in \code{RcppArmadillo}
#'   \code{C++} code, so it's many times faster than the equivalent \code{R}
#'   code.
#'
#'   If only a simple rolling mean is required (not the median) then other
#'   functions like \code{roll_sum()} or \code{roll_vec()} may be even faster.
#'
#' @examples
#' \dontrun{
#' # Define time series of returns using package rutils
#' returns <- na.omit(rutils::etfenv$returns$VTI)
#' # Calculate the rolling means at 25 day end points, with a 75 day look-back
#' means <- HighFreq::roll_mean(returns, step=25, look_back=3)
#' # Compare the mean estimates over 11-period look-back intervals
#' all.equal(HighFreq::roll_mean(returns, look_back=11)[-(1:10), ], 
#'   drop(RcppRoll::roll_mean(returns, n=11)), check.attributes=FALSE)
#' # Define end points and start points
#' endp <- HighFreq::calc_endpoints(NROW(returns), step=25)
#' startp <- HighFreq::calc_startpoints(endp, look_back=3)
#' # Calculate the rolling means using RcppArmadillo
#' means <- HighFreq::roll_mean(returns, startp=startp, endp=endp)
#' # Calculate the rolling medians using RcppArmadillo
#' medianscpp <- HighFreq::roll_mean(returns, startp=startp, endp=endp, method="nonparametric")
#' # Calculate the rolling medians using R
#' medians = sapply(1:NROW(endp), function(i) {
#'   median(returns[startp[i]:endp[i] + 1])
#' })  # end sapply
#' all.equal(medians, drop(medianscpp))
#' # Compare the speed of RcppArmadillo with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::roll_mean(returns, startp=startp, endp=endp, method="nonparametric"),
#'   Rcode=sapply(1:NROW(endp), function(i) {median(returns[startp[i]:endp[i] + 1])}),
#'   times=10))[, c(1, 4, 5)]
#' }
#' @export
roll_mean <- function(tseries, startp = 0L, endp = 0L, step = 1L, look_back = 1L, stub = 0L, method = "moment", conf_lev = 0.75) {
    .Call('_HighFreq_roll_mean', PACKAGE = 'HighFreq', tseries, startp, endp, step, look_back, stub, method, conf_lev)
}

#' Calculate a \emph{vector} of variance estimates over a rolling look-back
#' interval for a single-column \emph{time series} or a single-column
#' \emph{matrix}, using \code{RcppArmadillo}.
#'
#' @param \code{tseries} A single-column \emph{time series} or a single-column
#'   \emph{matrix}.
#' 
#' @param \code{look_back} The length of the look-back interval, equal to the
#'   number of \emph{vector} elements used for calculating a single variance
#'   estimate (the default is \code{look_back = 1}).
#'
#' @return A single-column \emph{matrix} with the same number of elements as
#'   the input argument \code{tseries}.
#'
#' @details
#'   The function \code{roll_varvec()} calculates a \emph{vector} of variance
#'   estimates over a rolling look-back interval for a single-column \emph{time
#'   series} or a single-column \emph{matrix}, using \code{RcppArmadillo}
#'   \code{C++} code.
#'   
#'   The function \code{roll_varvec()} uses an expanding look-back interval in
#'   the initial warmup period, to calculate the same number of elements as the
#'   input argument \code{tseries}.
#'
#'   The function \code{roll_varvec()} performs the same calculation as the
#'   function \code{roll_var()} from package
#'   \href{https://cran.r-project.org/web/packages/RcppRoll/index.html}{RcppRoll},
#'   but it's several times faster because it uses \code{RcppArmadillo}
#'   \code{C++} code.
#'
#' @examples
#' \dontrun{
#' # Create a vector of random returns
#' returns <- rnorm(1e6)
#' # Compare the variance estimates over 11-period look-back intervals
#' all.equal(drop(HighFreq::roll_varvec(returns, look_back=11))[-(1:10)], 
#'   RcppRoll::roll_var(returns, n=11))
#' # Compare the speed of RcppArmadillo with RcppRoll
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::roll_varvec(returns, look_back=11),
#'   RcppRoll=RcppRoll::roll_var(returns, n=11),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' @export
roll_varvec <- function(tseries, look_back = 1L) {
    .Call('_HighFreq_roll_varvec', PACKAGE = 'HighFreq', tseries, look_back)
}

#' Calculate a \emph{matrix} of dispersion (variance) estimates over a rolling
#' look-back interval attached at the end points of a \emph{time series} or a
#' \emph{matrix}.
#'
#' @param \code{tseries} A \emph{time series} or a \emph{matrix} of data.
#'
#' @param \code{startp} An \emph{integer} vector of start points (the default
#'   is \code{startp = 0}).
#' 
#' @param \code{endp} An \emph{integer} vector of end points (the default is
#'   \code{endp = 0}).
#' 
#' @param \code{step} The number of time periods between the end points (the
#'   default is \code{step = 1}).
#'
#' @param \code{look_back} The number of end points in the look-back interval
#'   (the default is \code{look_back = 1}).
#'   
#' @param \code{stub} An \emph{integer} value equal to the first end point for
#'   calculating the end points (the default is \code{stub = 0}).
#' 
#' @param \code{method} A \emph{character} string representing the type of the
#'   measure of dispersion (the default is \code{method = "moment"}).
#'
#' @return A \emph{matrix} dispersion (variance) estimates with the same number
#'   of columns as the input time series \code{tseries}, and the number of rows
#'   equal to the number of end points.
#'   
#' @details
#'   The function \code{roll_var()} calculates a \emph{matrix} of dispersion
#'   (variance) estimates over rolling look-back intervals attached at the end
#'   points of the \emph{time series} \code{tseries}.
#'   
#'   The function \code{roll_var()} performs a loop over the end points, and at
#'   each end point it subsets the time series \code{tseries} over a look-back
#'   interval equal to \code{look_back} number of end points.
#'   
#'   It passes the subset time series to the function \code{calc_var()}, which
#'   calculates the dispersion.
#'   See the function \code{calc_var()} for a description of the dispersion
#'   methods.
#'   
#'   If the arguments \code{endp} and \code{startp} are not given then it
#'   first calculates a vector of end points separated by \code{step} time
#'   periods. It calculates the end points along the rows of \code{tseries}
#'   using the function \code{calc_endpoints()}, with the number of time
#'   periods between the end points equal to \code{step} time periods.
#' 
#'   For example, the rolling variance at \code{25} day end points, with a
#'   \code{75} day look-back, can be calculated using the parameters
#'   \code{step = 25} and \code{look_back = 3}.
#'
#'   The function \code{roll_var()} with the parameter \code{step = 1}
#'   performs the same calculation as the function \code{roll_var()} from
#'   package
#'   \href{https://cran.r-project.org/web/packages/RcppRoll/index.html}{RcppRoll},
#'   but it's several times faster because it uses \code{RcppArmadillo}
#'   \code{C++} code.
#'
#'   The function \code{roll_var()} is implemented in \code{RcppArmadillo}
#'   \code{C++} code, so it's many times faster than the equivalent \code{R}
#'   code.
#'
#' @examples
#' \dontrun{
#' # Define time series of returns using package rutils
#' returns <- na.omit(rutils::etfenv$returns$VTI)
#' # Calculate the rolling variance at 25 day end points, with a 75 day look-back
#' variance <- HighFreq::roll_var(returns, step=25, look_back=3)
#' # Compare the variance estimates over 11-period look-back intervals
#' all.equal(HighFreq::roll_var(returns, look_back=11)[-(1:10), ], 
#'   drop(RcppRoll::roll_var(returns, n=11)), check.attributes=FALSE)
#' # Compare the speed of HighFreq::roll_var() with RcppRoll::roll_var()
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::roll_var(returns, look_back=11),
#'   RcppRoll=RcppRoll::roll_var(returns, n=11),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' # Compare the speed of HighFreq::roll_var() with TTR::runMAD()
#' summary(microbenchmark(
#'     Rcpp=HighFreq::roll_var(returns, look_back=11, method="quantile"),
#'     TTR=TTR::runMAD(returns, n = 11),
#'     times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' @export
roll_var <- function(tseries, startp = 0L, endp = 0L, step = 1L, look_back = 1L, stub = 0L, method = "moment", conf_lev = 0.75) {
    .Call('_HighFreq_roll_var', PACKAGE = 'HighFreq', tseries, startp, endp, step, look_back, stub, method, conf_lev)
}

#' Calculate a \emph{vector} of variance estimates over a rolling look-back
#' interval attached at the end points of a \emph{time series} or a
#' \emph{matrix} with \emph{OHLC} price data.
#' 
#' @param \code{ohlc} A \emph{time series} or a \emph{matrix} with \emph{OHLC}
#'   price data.
#'   
#' @param \code{startp} An \emph{integer} vector of start points (the default
#'   is \code{startp = 0}).
#' 
#' @param \code{endp} An \emph{integer} vector of end points (the default is
#'   \code{endp = 0}).
#' 
#' @param \code{step} The number of time periods between the end points (the
#'   default is \code{step = 1}).
#'
#' @param \code{look_back} The number of end points in the look-back interval
#'   (the default is \code{look_back = 1}).
#'   
#' @param \code{stub} An \emph{integer} value equal to the first end point for
#'   calculating the end points (the default is \code{stub = 0}).
#' 
#' @param \code{method} A \emph{character} string representing the price range
#'   estimator for calculating the variance.  The estimators include:
#'   \itemize{
#'     \item "close" close-to-close estimator,
#'     \item "rogers_satchell" Rogers-Satchell estimator,
#'     \item "garman_klass" Garman-Klass estimator,
#'     \item "garman_klass_yz" Garman-Klass with account for close-to-open price jumps,
#'     \item "yang_zhang" Yang-Zhang estimator,
#'    }
#'    (The default is the \emph{"yang_zhang"} estimator.)
#'    
#' @param \code{scale} \emph{Boolean} argument: Should the returns be divided
#'   by the time index, the number of seconds in each period?  (The default is
#'   \code{scale = TRUE}.)
#'   
#' @param \code{index} A \emph{vector} with the time index of the \emph{time
#'   series}.  This is an optional argument (the default is \code{index=0}).
#'
#' @return A column \emph{vector} of variance estimates, with the number of
#'   rows equal to the number of end points.
#'
#' @details
#'   The function \code{roll_var_ohlc()} calculates a \emph{vector} of variance
#'   estimates over a rolling look-back interval attached at the end points of
#'   the \emph{time series} \code{ohlc}.
#'   
#'   The input \emph{OHLC time series} \code{ohlc} is assumed to contain the
#'   log prices.
#'
#'   The function \code{roll_var_ohlc()} performs a loop over the end points,
#'   subsets the previous (past) rows of \code{ohlc}, and passes them into the
#'   function \code{calc_var_ohlc()}.
#' 
#'   At each end point, the variance is calculated over a look-back interval
#'   equal to \code{look_back} number of end points.
#'   In the initial warmup period, the variance is calculated over an expanding
#'   look-back interval.
#'   
#'   If the arguments \code{endp} and \code{startp} are not given then it
#'   first calculates a vector of end points separated by \code{step} time
#'   periods. It calculates the end points along the rows of \code{ohlc}
#'   using the function \code{calc_endpoints()}, with the number of time
#'   periods between the end points equal to \code{step} time periods.
#' 
#'   For example, the rolling variance at daily end points with an \code{11}
#'   day look-back, can be calculated using the parameters \code{step = 1} and
#'   \code{look_back = 1} (Assuming the \code{ohlc} data has daily
#'   frequency.)
#' 
#'   Similarly, the rolling variance at \code{25} day end points with a
#'   \code{75} day look-back, can be calculated using the parameters
#'   \code{step = 25} and \code{look_back = 3} (because \code{3*25 = 75}).
#' 
#'   The function \code{roll_var_ohlc()} calculates the variance from all the
#'   different intra-day and day-over-day returns (defined as the differences
#'   between \emph{OHLC} prices), using several different variance estimation
#'   methods.
#'   
#'   The default \code{method} is \emph{"yang_zhang"}, which theoretically
#'   has the lowest standard error among unbiased estimators.
#'   The methods \emph{"close"}, \emph{"garman_klass_yz"}, and
#'   \emph{"yang_zhang"} do account for \emph{close-to-open} price jumps, while
#'   the methods \emph{"garman_klass"} and \emph{"rogers_satchell"} do not
#'   account for \emph{close-to-open} price jumps.
#'
#'   If \code{scale} is \code{TRUE} (the default), then the returns are
#'   divided by the differences of the time index (which scales the variance to
#'   the units of variance per second squared.) This is useful when calculating
#'   the variance from minutely bar data, because dividing returns by the
#'   number of seconds decreases the effect of overnight price jumps. If the
#'   time index is in days, then the variance is equal to the variance per day
#'   squared.
#'   
#'   The optional argument \code{index} is the time index of the \emph{time
#'   series} \code{ohlc}. If the time index is in seconds, then the
#'   differences of the index are equal to the number of seconds in each time
#'   period.  If the time index is in days, then the differences are equal to
#'   the number of days in each time period.
#'   
#'   The function \code{roll_var_ohlc()} is implemented in \code{RcppArmadillo}
#'   \code{C++} code, so it's many times faster than the equivalent \code{R}
#'   code.
#'
#' @examples
#' \dontrun{
#' # Extract the log OHLC prices of SPY
#' ohlc <- log(HighFreq::SPY)
#' # Extract the time index of SPY prices
#' indeks <- c(1, diff(xts::.index(ohlc)))
#' # Rolling variance at minutely end points, with a 21 minute look-back
#' var_rolling <- HighFreq::roll_var_ohlc(ohlc, 
#'                               step=1, look_back=21, 
#'                               method="yang_zhang", 
#'                               index=indeks, scale=TRUE)
#' # Daily OHLC prices
#' ohlc <- rutils::etfenv$VTI
#' indeks <- c(1, diff(xts::.index(ohlc)))
#' # Rolling variance at 5 day end points, with a 20 day look-back (20=4*5)
#' var_rolling <- HighFreq::roll_var_ohlc(ohlc, 
#'                               step=5, look_back=4, 
#'                               method="yang_zhang", 
#'                               index=indeks, scale=TRUE)
#' # Same calculation in R
#' nrows <- NROW(ohlc)
#' close_lag = HighFreq::lagit(ohlc[, 4])
#' endp <- drop(HighFreq::calc_endpoints(nrows, 3)) + 1
#' startp <- drop(HighFreq::calc_startpoints(endp, 2))
#' n_pts <- NROW(endp)
#' var_rollingr <- sapply(2:n_pts, function(it) {
#'   rangev <- startp[it]:endp[it]
#'   sub_ohlc = ohlc[rangev, ]
#'   sub_close = close_lag[rangev]
#'   sub_index = indeks[rangev]
#'   HighFreq::calc_var_ohlc(sub_ohlc, close_lag=sub_close, scale=TRUE, index=sub_index)
#' })  # end sapply
#' var_rollingr <- c(0, var_rollingr)
#' all.equal(drop(var_rolling), var_rollingr)
#' }
#' @export
roll_var_ohlc <- function(ohlc, startp = 0L, endp = 0L, step = 1L, look_back = 1L, stub = 0L, method = "yang_zhang", scale = TRUE, index = 0L) {
    .Call('_HighFreq_roll_var_ohlc', PACKAGE = 'HighFreq', ohlc, startp, endp, step, look_back, stub, method, scale, index)
}

#' Calculate a \emph{matrix} of skewness estimates over a rolling look-back
#' interval attached at the end points of a \emph{time series} or a
#' \emph{matrix}.
#'
#' @param \code{tseries} A \emph{time series} or a \emph{matrix} of data.
#'    
#' @param \code{startp} An \emph{integer} vector of start points (the default
#'   is \code{startp = 0}).
#' 
#' @param \code{endp} An \emph{integer} vector of end points (the default is 
#'   \code{endp = 0}).
#' 
#' @param \code{step} The number of time periods between the end points (the
#'   default is \code{step = 1}).
#'
#' @param \code{look_back} The number of end points in the look-back interval
#'   (the default is \code{look_back = 1}).
#'   
#' @param \code{stub} An \emph{integer} value equal to the first end point for
#'   calculating the end points (the default is \code{stub = 0}).
#' 
#' @param \code{method} A \emph{string} specifying the type of the skewness
#'   model (the default is \code{method = "moment"} - see Details).
#'
#' @param \code{conf_lev} The confidence level for calculating the
#'   quantiles (the default is \code{conf_lev = 0.75}).
#'
#' @return A \emph{matrix} of skewness estimates with the same number of
#'   columns as the input time series \code{tseries}, and the number of rows
#'   equal to the number of end points.
#'   
#' @details
#'   The function \code{roll_skew()} calculates a \emph{matrix} of skewness
#'   estimates over rolling look-back intervals attached at the end points of
#'   the \emph{time series} \code{tseries}.
#'   
#'   The function \code{roll_skew()} performs a loop over the end points, and
#'   at each end point it subsets the time series \code{tseries} over a
#'   look-back interval equal to \code{look_back} number of end points.
#'   
#'   It passes the subset time series to the function \code{calc_skew()}, which
#'   calculates the skewness.
#'   See the function \code{calc_skew()} for a description of the skewness
#'   methods.
#'   
#'   If the arguments \code{endp} and \code{startp} are not given then it
#'   first calculates a vector of end points separated by \code{step} time
#'   periods. It calculates the end points along the rows of \code{tseries}
#'   using the function \code{calc_endpoints()}, with the number of time
#'   periods between the end points equal to \code{step} time periods.
#' 
#'   For example, the rolling skewness at \code{25} day end points, with a
#'   \code{75} day look-back, can be calculated using the parameters
#'   \code{step = 25} and \code{look_back = 3}.
#'
#'   The function \code{roll_skew()} is implemented in \code{RcppArmadillo}
#'   \code{C++} code, so it's many times faster than the equivalent \code{R}
#'   code.
#'
#' @examples
#' \dontrun{
#' # Define time series of returns using package rutils
#' returns <- na.omit(rutils::etfenv$returns$VTI)
#' # Define end points and start points
#' endp <- 1 + HighFreq::calc_endpoints(NROW(returns), step=25)
#' startp <- HighFreq::calc_startpoints(endp, look_back=3)
#' # Calculate the rolling skewness at 25 day end points, with a 75 day look-back
#' skew_ness <- HighFreq::roll_skew(returns, step=25, look_back=3)
#' # Calculate the rolling skewness using R code
#' skew_r <- sapply(1:NROW(endp), function(it) {
#'   HighFreq::calc_skew(returns[startp[it]:endp[it], ])
#' })  # end sapply
#' # Compare the skewness estimates
#' all.equal(drop(skew_ness), skew_r, check.attributes=FALSE)
#' # Compare the speed of RcppArmadillo with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::roll_skew(returns, step=25, look_back=3),
#'   Rcode=sapply(1:NROW(endp), function(it) {
#'     HighFreq::calc_skew(returns[startp[it]:endp[it], ])
#'   }),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' @export
roll_skew <- function(tseries, startp = 0L, endp = 0L, step = 1L, look_back = 1L, stub = 0L, method = "moment", conf_lev = 0.75) {
    .Call('_HighFreq_roll_skew', PACKAGE = 'HighFreq', tseries, startp, endp, step, look_back, stub, method, conf_lev)
}

#' Calculate a \emph{matrix} of kurtosis estimates over a rolling look-back
#' interval attached at the end points of a \emph{time series} or a
#' \emph{matrix}.
#'
#' @param \code{tseries} A \emph{time series} or a \emph{matrix} of data.
#'    
#' @param \code{startp} An \emph{integer} vector of start points (the default
#'   is \code{startp = 0}).
#' 
#' @param \code{endp} An \emph{integer} vector of end points (the default is 
#'   \code{endp = 0}).
#' 
#' @param \code{step} The number of time periods between the end points (the
#'   default is \code{step = 1}).
#'
#' @param \code{look_back} The number of end points in the look-back interval
#'   (the default is \code{look_back = 1}).
#'   
#' @param \code{stub} An \emph{integer} value equal to the first end point for
#'   calculating the end points (the default is \code{stub = 0}).
#' 
#' @param \code{method} A \emph{string} specifying the type of the kurtosis
#'   model (the default is \code{method = "moment"} - see Details).
#'
#' @param \code{conf_lev} The confidence level for calculating the
#'   quantiles (the default is \code{conf_lev = 0.75}).
#'
#' @return A \emph{matrix} of kurtosis estimates with the same number of
#'   columns as the input time series \code{tseries}, and the number of rows
#'   equal to the number of end points.
#'   
#' @details
#'   The function \code{roll_kurtosis()} calculates a \emph{matrix} of kurtosis
#'   estimates over rolling look-back intervals attached at the end points of
#'   the \emph{time series} \code{tseries}.
#'   
#'   The function \code{roll_kurtosis()} performs a loop over the end points,
#'   and at each end point it subsets the time series \code{tseries} over a
#'   look-back interval equal to \code{look_back} number of end points.
#'   
#'   It passes the subset time series to the function \code{calc_kurtosis()},
#'   which calculates the kurtosis. See the function \code{calc_kurtosis()} for
#'   a description of the kurtosis methods.
#'   
#'   If the arguments \code{endp} and \code{startp} are not given then it
#'   first calculates a vector of end points separated by \code{step} time
#'   periods. It calculates the end points along the rows of \code{tseries}
#'   using the function \code{calc_endpoints()}, with the number of time
#'   periods between the end points equal to \code{step} time periods.
#' 
#'   For example, the rolling kurtosis at \code{25} day end points, with a
#'   \code{75} day look-back, can be calculated using the parameters
#'   \code{step = 25} and \code{look_back = 3}.
#'
#'   The function \code{roll_kurtosis()} is implemented in \code{RcppArmadillo}
#'   \code{C++} code, so it's many times faster than the equivalent \code{R}
#'   code.
#'
#' @examples
#' \dontrun{
#' # Define time series of returns using package rutils
#' returns <- na.omit(rutils::etfenv$returns$VTI)
#' # Define end points and start points
#' endp <- 1 + HighFreq::calc_endpoints(NROW(returns), step=25)
#' startp <- HighFreq::calc_startpoints(endp, look_back=3)
#' # Calculate the rolling kurtosis at 25 day end points, with a 75 day look-back
#' kurto_sis <- HighFreq::roll_kurtosis(returns, step=25, look_back=3)
#' # Calculate the rolling kurtosis using R code
#' kurt_r <- sapply(1:NROW(endp), function(it) {
#'   HighFreq::calc_kurtosis(returns[startp[it]:endp[it], ])
#' })  # end sapply
#' # Compare the kurtosis estimates
#' all.equal(drop(kurto_sis), kurt_r, check.attributes=FALSE)
#' # Compare the speed of RcppArmadillo with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::roll_kurtosis(returns, step=25, look_back=3),
#'   Rcode=sapply(1:NROW(endp), function(it) {
#'     HighFreq::calc_kurtosis(returns[startp[it]:endp[it], ])
#'   }),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' @export
roll_kurtosis <- function(tseries, startp = 0L, endp = 0L, step = 1L, look_back = 1L, stub = 0L, method = "moment", conf_lev = 0.75) {
    .Call('_HighFreq_roll_kurtosis', PACKAGE = 'HighFreq', tseries, startp, endp, step, look_back, stub, method, conf_lev)
}

#' Calculate a \emph{matrix} of regression coefficients, their t-values, and
#' z-scores, at the end points of the predictor matrix.
#' 
#' @param \code{response} A single-column \emph{time series} or a \emph{vector}
#'   of response data.
#' 
#' @param \code{predictor} A \emph{time series} or a \emph{matrix} of predictor
#'   data.
#'   
#' @param \code{intercept} A \emph{Boolean} specifying whether an intercept
#'   term should be added to the predictor (the default is \code{intercept =
#'   FALSE}).
#'
#' @param \code{startp} An \emph{integer} vector of start points (the default
#'   is \code{startp = 0}).
#' 
#' @param \code{endp} An \emph{integer} vector of end points (the default is 
#'   \code{endp = 0}).
#' 
#' @param \code{step} The number of time periods between the end points (the
#'   default is \code{step = 1}).
#'
#' @param \code{look_back} The number of end points in the look-back interval
#'   (the default is \code{look_back = 1}).
#'   
#' @param \code{stub} An \emph{integer} value equal to the first end point for
#'   calculating the end points (the default is \code{stub = 0}).
#' 
#' @param \code{intercept} A \emph{Boolean} specifying whether an intercept
#'   term should be added to the predictor (the default is \code{intercept =
#'   FALSE}).
#'
#' @param \code{method} A \emph{string} specifying the type of the regression
#'   model the default is \code{method = "least_squares"} - see Details).
#'   
#' @param \code{eigen_thresh} A \emph{numeric} threshold level for discarding
#'   small singular values in order to regularize the inverse of the
#'   \code{predictor} matrix (the default is \code{1e-5}).
#'   
#' @param \code{eigen_max} An \emph{integer} equal to the number of singular
#'   values used for calculating the shrinkage inverse of the \code{predictor}
#'   matrix (the default is \code{0} - equivalent to \code{eigen_max} equal to
#'   the number of columns of \code{predictor}).
#'   
#' @param \code{conf_lev} The confidence level for calculating the
#'   quantiles (the default is \code{conf_lev = 0.75}).
#'
#' @param \code{alpha} The shrinkage intensity between \code{0} and \code{1}.
#'   (the default is \code{0}).
#' 
#' @return A \emph{matrix} with the regression coefficients, their t-values, and
#' z-scores, and with the 
#' same number of rows as \code{predictor}
#' a
#'   number of columns equal to \code{2n+3}, where \code{n} is the number of
#'   columns of \code{predictor}.
#'
#' @details
#'   The function \code{roll_reg()} calculates a \emph{matrix} of regression
#'   coefficients, their t-values, and z-scores at the end points of the predictor
#'   matrix.
#'   
#'   The function \code{roll_reg()} performs a loop over the end points, and at
#'   each end point it subsets the time series \code{predictor} over a look-back
#'   interval equal to \code{look_back} number of end points.
#'   
#'   If the arguments \code{endp} and \code{startp} are not given then it
#'   first calculates a vector of end points separated by \code{step} time
#'   periods. It calculates the end points along the rows of \code{predictor}
#'   using the function \code{calc_endpoints()}, with the number of time
#'   periods between the end points equal to \code{step} time periods.
#'   
#'   For example, the rolling regression at \code{25} day end points, with a
#'   \code{75} day look-back, can be calculated using the parameters
#'   \code{step = 25} and \code{look_back = 3}.
#'
#'   It passes the subset time series to the function \code{calc_reg()}, which
#'   calculates the regression coefficients, their t-values, and the z-score.
#'   
#'   If \code{intercept = TRUE} then an extra intercept column (unit column) is
#'   added to the predictor matrix (the default is \code{intercept = FALSE}).
#'   
#'   The number of columns of the return matrix depends on the number of
#'   columns of the \code{predictor} matrix (including the intercept column, if
#'   it's added).
#'   The number of columns of the return matrix is equal to the number of
#'   regression coefficients, plus their t-values, plus the z-score column.
#'   The number of regression coefficients is equal to the number of columns of
#'   the \code{predictor} matrix (including the intercept column, if it's
#'   added).
#'   The number of t-values is equal to the number of coefficients.
#'   For example, if the number of columns of the \code{predictor} matrix is
#'   equal to \code{n}, and if \code{intercept = TRUE}, then \code{roll_reg()}
#'   returns a matrix with \code{2n+3} columns: \code{n+1} regression
#'   coefficients (including the intercept coefficient), \code{n+1}
#'   corresponding t-values, and \code{1} z-score column.
#' 
#' @examples
#' \dontrun{
#' # Calculate historical returns
#' returns <- na.omit(rutils::etfenv$returns[, c("XLP", "VTI")])
#' # Define monthly end points and start points
#' endp <- xts::endpoints(returns, on="months")[-1]
#' look_back <- 12
#' startp <- c(rep(1, look_back), endp[1:(NROW(endp)-look_back)])
#' # Calculate rolling betas using RcppArmadillo
#' reg_stats <- HighFreq::roll_reg(response=returns[, 1], predictor=returns[, 2], endp=(endp-1), startp=(startp-1))
#' betas <- reg_stats[, 2]
#' # Calculate rolling betas in R
#' betas_r <- sapply(1:NROW(endp), FUN=function(ep) {
#'   datav <- returns[startp[ep]:endp[ep], ]
#'   drop(cov(datav[, 1], datav[, 2])/var(datav[, 2]))
#' })  # end sapply
#' # Compare the outputs of both functions
#' all.equal(betas, betas_r, check.attributes=FALSE)
#' }
#' 
#' @export
roll_reg <- function(response, predictor, intercept = FALSE, startp = 0L, endp = 0L, step = 1L, look_back = 1L, stub = 0L, method = "least_squares", eigen_thresh = 1e-5, eigen_max = 0L, conf_lev = 0.1, alpha = 0.0) {
    .Call('_HighFreq_roll_reg', PACKAGE = 'HighFreq', response, predictor, intercept, startp, endp, step, look_back, stub, method, eigen_thresh, eigen_max, conf_lev, alpha)
}

#' Perform a rolling scaling (standardization) of the columns of a
#' \emph{matrix} of data using \code{RcppArmadillo}.
#' 
#' @param \code{matrix} A \emph{matrix} of data.
#' 
#' @param \code{look_back} The length of the look-back interval, equal to the number 
#'   of rows of data used in the scaling.
#'   
#' @param use_median A \emph{Boolean} argument: if \code{TRUE} then the 
#'   centrality (central tendency) is calculated as the \emph{median} and the 
#'   dispersion is calculated as the \emph{median absolute deviation}
#'   (\emph{MAD}).
#'   If \code{use_median} is \code{FALSE} then the centrality is calculated as 
#'   the \emph{mean} and the dispersion is calculated as the \emph{standard
#'   deviation} (the default is \code{use_median = FALSE})
#'
#' @return A \emph{matrix} with the same dimensions as the input argument
#'   \code{matrix}.
#'
#' @details
#'   The function \code{roll_scale()} performs a rolling scaling
#'   (standardization) of the columns of the \code{matrix} argument using
#'   \code{RcppArmadillo}.
#'   The function \code{roll_scale()} performs a loop over the rows of 
#'   \code{matrix}, subsets a number of previous (past) rows equal to 
#'   \code{look_back}, and scales the subset matrix.  It assigns the last row
#'   of the scaled subset \emph{matrix} to the return matrix.
#'   
#'   If the argument \code{use_median} is \code{FALSE} (the default), then it
#'   performs the same calculation as the function \code{roll::roll_scale()}.
#'   If the argument \code{use_median} is \code{TRUE}, then it calculates the
#'   centrality as the \emph{median} and the dispersion as the \emph{median
#'   absolute deviation} (\emph{MAD}).
#'   
#' @examples
#' \dontrun{
#' matrixv <- matrix(rnorm(20000), nc=2)
#' look_back <- 11
#' rolled_scaled <- roll::roll_scale(data=matrixv, width = look_back, min_obs=1)
#' rolled_scaled2 <- roll_scale(matrix=matrixv, look_back = look_back, use_median=FALSE)
#' all.equal(rolled_scaled[-1, ], rolled_scaled2[-1, ])
#' }
#' 
#' @export
roll_scale <- function(matrix, look_back, use_median = FALSE) {
    .Call('_HighFreq_roll_scale', PACKAGE = 'HighFreq', matrix, look_back, use_median)
}

#' Calculate a \emph{vector} of z-scores of the residuals of rolling
#' regressions at the end points of the predictor matrix.
#' 
#' @param \code{response} A single-column \emph{time series} or a \emph{vector}
#'   of response data.
#' 
#' @param \code{predictor} A \emph{time series} or a \emph{matrix} of predictor
#'   data.
#'   
#' @param \code{startp} An \emph{integer} vector of start points (the default
#'   is \code{startp = 0}).
#' 
#' @param \code{endp} An \emph{integer} vector of end points (the default is
#'   \code{endp = 0}).
#' 
#' @param \code{step} The number of time periods between the end points (the
#'   default is \code{step = 1}).
#'
#' @param \code{look_back} The number of end points in the look-back interval
#'   (the default is \code{look_back = 1}).
#'   
#' @param \code{stub} An \emph{integer} value equal to the first end point for
#'   calculating the end points (the default is \code{stub = 0}).
#' 
#' @return A column \emph{vector} of the same length as the number of rows of
#'   \code{predictor}.
#'
#' @details
#'   The function \code{roll_zscores()} calculates a \emph{vector} of z-scores
#'   of the residuals of rolling regressions at the end points of the
#'   \emph{time series} \code{predictor}.
#'   
#'   The function \code{roll_zscores()} performs a loop over the end points,
#'   and at each end point it subsets the time series \code{predictor} over a
#'   look-back interval equal to \code{look_back} number of end points.
#'   
#'   It passes the subset time series to the function \code{calc_lm()}, which
#'   calculates the regression data.
#'   
#'   If the arguments \code{endp} and \code{startp} are not given then it
#'   first calculates a vector of end points separated by \code{step} time
#'   periods. It calculates the end points along the rows of \code{predictor}
#'   using the function \code{calc_endpoints()}, with the number of time
#'   periods between the end points equal to \code{step} time periods.
#'   
#'   For example, the rolling variance at \code{25} day end points, with a
#'   \code{75} day look-back, can be calculated using the parameters
#'   \code{step = 25} and \code{look_back = 3}.
#'
#' @examples
#' \dontrun{
#' # Calculate historical returns
#' returns <- na.omit(rutils::etfenv$returns[, c("XLF", "VTI", "IEF")])
#' # Response equals XLF returns
#' response <- returns[, 1]
#' # Predictor matrix equals VTI and IEF returns
#' predictor <- returns[, -1]
#' # Calculate Z-scores from rolling time series regression using RcppArmadillo
#' look_back <- 11
#' z_scores <- HighFreq::roll_zscores(response=response, predictor=predictor, look_back)
#' # Calculate z-scores in R from rolling multivariate regression using lm()
#' z_scoresr <- sapply(1:NROW(predictor), function(ro_w) {
#'   if (ro_w == 1) return(0)
#'   startpoint <- max(1, ro_w-look_back+1)
#'   responsi <- response[startpoint:ro_w]
#'   predicti <- predictor[startpoint:ro_w, ]
#'   reg_model <- lm(responsi ~ predicti)
#'   residuals <- reg_model$residuals
#'   residuals[NROW(residuals)]/sd(residuals)
#' })  # end sapply
#' # Compare the outputs of both functions
#' all.equal(z_scores[-(1:look_back)], z_scoresr[-(1:look_back)], 
#'   check.attributes=FALSE)
#' }
#' 
#' @export
roll_zscores <- function(response, predictor, startp = 0L, endp = 0L, step = 1L, look_back = 1L, stub = 0L) {
    .Call('_HighFreq_roll_zscores', PACKAGE = 'HighFreq', response, predictor, startp, endp, step, look_back, stub)
}

#' Calculate a \emph{matrix} of estimator values over a rolling look-back
#' interval attached at the end points of a \emph{time series} or a
#' \emph{matrix}.
#'
#' @param \code{tseries} A \emph{time series} or a \emph{matrix} of data.
#'    
#' @param \code{fun} A \emph{string} specifying the estimator function (the
#'   default is \code{fun = "calc_var"}.)
#'
#' @param \code{startp} An \emph{integer} vector of start points (the default
#'   is \code{startp = 0}).
#' 
#' @param \code{endp} An \emph{integer} vector of end points (the default is 
#'   \code{endp = 0}).
#' 
#' @param \code{step} The number of time periods between the end points (the
#'   default is \code{step = 1}).
#'
#' @param \code{look_back} The number of end points in the look-back interval
#'   (the default is \code{look_back = 1}).
#'   
#' @param \code{stub} An \emph{integer} value equal to the first end point for
#'   calculating the end points (the default is \code{stub = 0}).
#' 
#' @param \code{method} A \emph{string} specifying the type of the model for the
#'   estimator (the default is \code{method = "moment"}.)
#'
#' @param \code{conf_lev} The confidence level for calculating the
#'   quantiles (the default is \code{conf_lev = 0.75}).
#'
#' @return A \emph{matrix} with the same number of columns as the input time
#'   series \code{tseries}, and the number of rows equal to the number of end
#'   points.
#'   
#' @details
#'   The function \code{roll_fun()} calculates a \emph{matrix} of estimator
#'   values, over rolling look-back intervals attached at the end points of the
#'   \emph{time series} \code{tseries}.
#'   
#'   The function \code{roll_fun()} performs a loop over the end points, and at
#'   each end point it subsets the time series \code{tseries} over a look-back
#'   interval equal to \code{look_back} number of end points.
#'   
#'   It passes the subset time series to the function specified by the argument
#'   \code{fun}, which calculates the statistic.
#'   See the functions \code{calc_*()} for a description of the different
#'   estimators.
#'   
#'   If the arguments \code{endp} and \code{startp} are not given then it
#'   first calculates a vector of end points separated by \code{step} time
#'   periods. It calculates the end points along the rows of \code{tseries}
#'   using the function \code{calc_endpoints()}, with the number of time
#'   periods between the end points equal to \code{step} time periods.
#' 
#'   For example, the rolling variance at \code{25} day end points, with a
#'   \code{75} day look-back, can be calculated using the parameters
#'   \code{step = 25} and \code{look_back = 3}.
#'
#'   The function \code{roll_fun()} is implemented in \code{RcppArmadillo}
#'   \code{C++} code, so it's many times faster than the equivalent \code{R}
#'   code.
#'
#' @examples
#' \dontrun{
#' # Define time series of returns using package rutils
#' returns <- na.omit(rutils::etfenv$returns$VTI)
#' # Calculate the rolling variance at 25 day end points, with a 75 day look-back
#' var_rollfun <- HighFreq::roll_fun(returns, fun="calc_var", step=25, look_back=3)
#' # Calculate the rolling variance using roll_var()
#' var_roll <- HighFreq::roll_var(returns, step=25, look_back=3)
#' # Compare the two methods
#' all.equal(var_rollfun, var_roll, check.attributes=FALSE)
#' # Define end points and start points
#' endp <- HighFreq::calc_endpoints(NROW(returns), step=25)
#' startp <- HighFreq::calc_startpoints(endp, look_back=3)
#' # Calculate the rolling variance using RcppArmadillo
#' var_rollfun <- HighFreq::roll_fun(returns, fun="calc_var", startp=startp, endp=endp)
#' # Calculate the rolling variance using R code
#' var_roll <- sapply(1:NROW(endp), function(it) {
#'   var(returns[startp[it]:endp[it]+1, ])
#' })  # end sapply
#' # Compare the two methods
#' all.equal(drop(var_rollfun), var_roll, check.attributes=FALSE)
#' # Compare the speed of RcppArmadillo with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::roll_fun(returns, fun="calc_var", startp=startp, endp=endp),
#'   Rcode=sapply(1:NROW(endp), function(it) {
#'     var(returns[startp[it]:endp[it]+1, ])
#'   }),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' @export
roll_fun <- function(tseries, fun = "calc_var", startp = 0L, endp = 0L, step = 1L, look_back = 1L, stub = 0L, method = "moment", conf_lev = 0.75) {
    .Call('_HighFreq_roll_fun', PACKAGE = 'HighFreq', tseries, fun, startp, endp, step, look_back, stub, method, conf_lev)
}

#' Simulate or estimate the rolling variance under a \emph{GARCH(1,1)} process
#' using \emph{Rcpp}.
#' 
#' @param \code{omega} Parameter proportional to the long-term average level
#'   of variance.
#' 
#' @param \code{alpha} The weight associated with recent realized variance
#'   updates.
#' 
#' @param \code{beta} The weight associated with the past variance estimates.
#' 
#' @param \code{innov} A single-column \emph{matrix} of innovations.
#' 
#' @param \code{is_random} \emph{Boolean} argument: Are the innovations random
#'   numbers or historical returns? (The default is \code{is_random = TRUE}.)
#'
#' @return A \emph{matrix} with two columns and with the same number of rows as
#'   the argument \code{innov}.  The first column are the simulated returns and
#'   the second column is the variance.
#'
#' @details
#'   The function \code{sim_garch()} simulates or estimates the rolling variance
#'   under a \emph{GARCH(1,1)} process using \emph{Rcpp}.
#'
#'   If \code{is_random = TRUE} (the default) then the innovations \code{innov}
#'   are treated as random numbers \eqn{\xi_i} and the \emph{GARCH(1,1)}
#'   process is given by:
#'   \deqn{
#'     r_i = \sigma_{i-1} \xi_i
#'   }
#'   \deqn{
#'     \sigma^2_i = \omega + \alpha r^2_i + \beta \sigma_{i-1}^2
#'   }
#'   Where \eqn{r_i} and \eqn{\sigma^2_i} are the simulated returns and
#'   variance, and \eqn{\omega}, \eqn{\alpha}, and \eqn{\beta} are the
#'   \emph{GARCH} parameters, and \eqn{\xi_i} are standard normal
#'   \emph{innovations}.
#'
#'   The long-term equilibrium level of the simulated variance is proportional
#'   to the parameter \eqn{\omega}:
#'   \deqn{
#'     \sigma^2 = \frac{\omega}{1 - \alpha - \beta}
#'   }
#'   So the sum of \eqn{\alpha} plus \eqn{\beta} should be less than \eqn{1},
#'   otherwise the volatility becomes explosive.
#'   
#'   If \code{is_random = FALSE} then the function \code{sim_garch()}
#'   \emph{estimates} the rolling variance from the historical returns. The
#'   innovations \code{innov} are equal to the historical returns \eqn{r_i} and
#'   the \emph{GARCH(1,1)} process is simply:
#'   \deqn{
#'     \sigma^2_i = \omega + \alpha r^2_i + \beta \sigma_{i-1}^2
#'   }
#'   Where \eqn{\sigma^2_i} is the rolling variance.
#'   
#'   The above should be viewed as a formula for \emph{estimating} the rolling
#'   variance from the historical returns, rather than simulating them. It
#'   represents exponential smoothing of the squared returns with a decay
#'   factor equal to \eqn{\beta}.
#'
#'   The function \code{sim_garch()} simulates the \emph{GARCH} process using
#'   fast \emph{Rcpp} \code{C++} code.
#'
#' @examples
#' \dontrun{
#' # Define the GARCH model parameters
#' alpha <- 0.79
#' betav <- 0.2
#' om_ega <- 1e-4*(1-alpha-betav)
#' # Calculate matrix of standard normal innovations
#' innov <- matrix(rnorm(1e3))
#' # Simulate the GARCH process using Rcpp
#' garch_data <- HighFreq::sim_garch(omega=om_ega, alpha=alpha,  beta=betav, innov=innov)
#' # Plot the GARCH rolling volatility and cumulative returns
#' plot(sqrt(garch_data[, 2]), t="l", main="Simulated GARCH Volatility", ylab="volatility")
#' plot(cumsum(garch_data[, 1]), t="l", main="Simulated GARCH Cumulative Returns", ylab="cumulative returns")
#' # Calculate historical VTI returns
#' returns <- na.omit(rutils::etfenv$returns$VTI)
#' # Estimate the GARCH volatility of VTI returns
#' garch_data <- HighFreq::sim_garch(omega=om_ega, alpha=alpha,  beta=betav, 
#'   innov=returns, is_random=FALSE)
#' # Plot dygraph of the estimated GARCH volatility
#' dygraphs::dygraph(xts::xts(sqrt(garch_data[, 2]), index(returns)), 
#'   main="Estimated GARCH Volatility of VTI")
#' }
#' 
#' @export
sim_garch <- function(omega, alpha, beta, innov, is_random = TRUE) {
    .Call('_HighFreq_sim_garch', PACKAGE = 'HighFreq', omega, alpha, beta, innov, is_random)
}

#' Simulate an \emph{Ornstein-Uhlenbeck} process using \emph{Rcpp}.
#' 
#' @param \code{init_price} The initial price. 
#' 
#' @param \code{eq_price} The equilibrium price. 
#' 
#' @param \code{theta} The strength of mean reversion.
#' 
#' @param \code{innov} A single-column \emph{matrix} of innovations (random
#'   numbers).
#' 
#' @return A single-column \emph{matrix} of simulated prices, with the same
#'   number of rows as the argument \code{innov}.
#'
#' @details
#'   The function \code{sim_ou()} simulates the following
#'   \emph{Ornstein-Uhlenbeck} process:
#'   \deqn{
#'     r_i = p_i - p_{i-1} = \theta \, (\mu - p_{i-1}) + \xi_i
#'   }
#'   \deqn{
#'     p_i = p_{i-1} + r_i
#'   }
#'   Where \eqn{r_i} and \eqn{p_i} are the simulated returns and prices,
#'   \eqn{\theta}, \eqn{\mu}, and \eqn{\sigma} are the
#'   \emph{Ornstein-Uhlenbeck} parameters, and \eqn{\xi_i} are the standard
#'   \emph{innovations}.
#'   The recursion starts with: \eqn{r_1 = \xi_1} and \eqn{p_1 = init\_price}.
#'
#'   The function \code{sim_ou()} simulates the percentage returns as equal to
#'   the difference between the equilibrium price \eqn{\mu} minus the latest
#'   price \eqn{p_{i-1}}, times the mean reversion parameter \eqn{\theta}, plus
#'   a random normal innovation. The log prices are calculated as the sum of
#'   returns (not compounded), so they can become negative.
#'
#'   The function \code{sim_ou()} simulates the \emph{Ornstein-Uhlenbeck}
#'   process using fast \emph{Rcpp} \code{C++} code.
#'
#'   The function \code{sim_ou()} returns a single-column \emph{matrix}
#'   representing the \emph{time series} of simulated prices.
#'
#' @examples
#' \dontrun{
#' # Define the Ornstein-Uhlenbeck model parameters
#' init_price <- 0.0
#' eq_price <- 1.0
#' sigmav <- 0.01
#' thetav <- 0.01
#' innov <- matrix(rnorm(1e3))
#' # Simulate Ornstein-Uhlenbeck process using Rcpp
#' prices <- HighFreq::sim_ou(init_price=init_price, eq_price=eq_price, volat=sigmav, theta=thetav, innov=innov)
#' plot(prices, t="l", main="Simulated Ornstein-Uhlenbeck Prices", ylab="prices")
#' }
#' 
#' @export
sim_ou <- function(init_price, eq_price, theta, innov) {
    .Call('_HighFreq_sim_ou', PACKAGE = 'HighFreq', init_price, eq_price, theta, innov)
}

#' Simulate a \emph{Schwartz} process using \emph{Rcpp}.
#' 
#' @param \code{init_price} The initial price. 
#' 
#' @param \code{eq_price} The equilibrium price. 
#' 
#' @param \code{theta} The strength of mean reversion.
#' 
#' @param \code{innov} A single-column \emph{matrix} of innovations (random
#'   numbers).
#' 
#' @return A single-column \emph{matrix} of simulated prices, with the same
#'   number of rows as the argument \code{innov}.
#'
#' @details
#'   The function \code{sim_schwartz()} simulates a \emph{Schwartz} process
#'   using fast \emph{Rcpp} \code{C++} code.
#'   
#'   The \emph{Schwartz} process is the exponential of the
#'   \emph{Ornstein-Uhlenbeck} process, and similar comments apply to it.
#'   The prices are calculated as the exponentially compounded returns, so they
#'   are never negative. The log prices can be obtained by taking the logarithm
#'   of the prices.
#'   
#'   The function \code{sim_schwartz()} simulates the percentage returns as
#'   equal to the difference between the equilibrium price \eqn{\mu} minus the
#'   latest price \eqn{p_{i-1}}, times the mean reversion parameter
#'   \eqn{\theta}, plus a random normal innovation.
#'
#'   The function \code{sim_schwartz()} returns a single-column \emph{matrix}
#'   representing the \emph{time series} of simulated prices.
#'
#' @examples
#' \dontrun{
#' # Define the Schwartz model parameters
#' init_price <- 1.0
#' eq_price <- 2.0
#' thetav <- 0.01
#' innov <- matrix(rnorm(1e3, sd=0.01))
#' # Simulate Schwartz process using Rcpp
#' prices <- HighFreq::sim_schwartz(init_price=init_price, eq_price=eq_price, theta=thetav, innov=innov)
#' plot(prices, t="l", main="Simulated Schwartz Prices", ylab="prices")
#' }
#' 
#' @export
sim_schwartz <- function(init_price, eq_price, theta, innov) {
    .Call('_HighFreq_sim_schwartz', PACKAGE = 'HighFreq', init_price, eq_price, theta, innov)
}

#' Simulate \emph{autoregressive} returns by recursively filtering a
#' \emph{matrix} of innovations through a \emph{matrix} of
#' \emph{autoregressive} coefficients.
#' 
#' @param \code{innov} A single-column \emph{matrix} of innovations.
#' 
#' @param \code{coeff} A single-column \emph{matrix} of \emph{autoregressive}
#'   coefficients.
#'
#' @return A single-column \emph{matrix} of simulated returns, with the same
#'   number of rows as the argument \code{innov}.
#'
#' @details
#'   The function \code{sim_ar()} recursively filters the \emph{matrix} of
#'   innovations \code{innov} through the \emph{matrix} of
#'   \emph{autoregressive} coefficients \code{coeff}, using fast
#'   \code{RcppArmadillo} \code{C++} code.
#'
#'   The function \code{sim_ar()} simulates an \emph{autoregressive} process
#'   \eqn{AR(n)} of order \eqn{n}:
#'   \deqn{
#'     r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_n r_{i-n} + \xi_i
#'   }
#'   Where \eqn{r_i} is the simulated output time series, \eqn{\varphi_i} are
#'   the \emph{autoregressive} coefficients, and \eqn{\xi_i} are the standard
#'   normal \emph{innovations}.
#'
#'   The order \eqn{n} of the \emph{autoregressive} process \eqn{AR(n)}, is
#'   equal to the number of rows of the \emph{autoregressive} coefficients
#'   \code{coeff}.
#'
#'   The function \code{sim_ar()} performs the same calculation as the standard
#'   \code{R} function \cr\code{filter(x=innov, filter=coeff,
#'   method="recursive")}, but it's several times faster.
#'   
#' @examples
#' \dontrun{
#' # Define AR coefficients
#' coeff <- matrix(c(0.1, 0.3, 0.5))
#' # Calculate matrix of innovations
#' innov <- matrix(rnorm(1e4, sd=0.01))
#' # Calculate recursive filter using filter()
#' filtered <- filter(innov, filter=coeff, method="recursive")
#' # Calculate recursive filter using RcppArmadillo
#' returns <- HighFreq::sim_ar(coeff, innov)
#' # Compare the two methods
#' all.equal(as.numeric(returns), as.numeric(filtered))
#' # Compare the speed of RcppArmadillo with R code
#' library(microbenchmark)
#' summary(microbenchmark(
#'   Rcpp=HighFreq::sim_ar(coeff, innov),
#'   Rcode=filter(innov, filter=coeff, method="recursive"),
#'   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
#' }
#' 
#' @export
sim_ar <- function(coeff, innov) {
    .Call('_HighFreq_sim_ar', PACKAGE = 'HighFreq', coeff, innov)
}

#' Simulate a \emph{Dickey-Fuller} process using \emph{Rcpp}.
#' 
#' @param \code{init_price} The initial price. 
#' 
#' @param \code{eq_price} The equilibrium price. 
#' 
#' @param \code{theta} The strength of mean reversion.
#' 
#' @param \code{coeff} A single-column \emph{matrix} of \emph{autoregressive}
#'   coefficients.
#'
#' @param \code{innov} A single-column \emph{matrix} of innovations (random
#'   numbers).
#' 
#' @return A single-column \emph{matrix} of simulated prices, with the same
#'   number of rows as the argument \code{innov}.
#'
#' @details
#'   The function \code{sim_df()} simulates the following \emph{Dickey-Fuller}
#'   process:
#'   \deqn{
#'     r_i = \theta \, (\mu - p_{i-1}) + \varphi_1 r_{i-1} + \ldots + \varphi_n r_{i-n} + \xi_i
#'   }
#'   \deqn{
#'     p_i = p_{i-1} + r_i
#'   }
#'   Where \eqn{r_i} and \eqn{p_i} are the simulated returns and prices,
#'   \eqn{\theta} and \eqn{\mu} are the \emph{Ornstein-Uhlenbeck} parameters,
#'   \eqn{\varphi_i} are the \emph{autoregressive} coefficients, and
#'   \eqn{\xi_i} are the normal \emph{innovations}.
#'   The recursion starts with: \eqn{r_1 = \xi_1} and \eqn{p_1 = init\_price}.
#'
#'   The \emph{Dickey-Fuller} process is a combination of an
#'   \emph{Ornstein-Uhlenbeck} process and an \emph{autoregressive} process.
#'   The order \eqn{n} of the \emph{autoregressive} process \eqn{AR(n)}, is
#'   equal to the number of rows of the \emph{autoregressive} coefficients
#'   \code{coeff}.
#'
#'   The function \code{sim_df()} simulates the \emph{Dickey-Fuller}
#'   process using fast \emph{Rcpp} \code{C++} code.
#'
#'   The function \code{sim_df()} returns a single-column \emph{matrix}
#'   representing the \emph{time series} of prices.
#'
#' @examples
#' \dontrun{
#' # Define the Ornstein-Uhlenbeck model parameters
#' init_price <- 1.0
#' eq_price <- 2.0
#' thetav <- 0.01
#' # Define AR coefficients
#' coeff <- matrix(c(0.1, 0.3, 0.5))
#' # Calculate matrix of standard normal innovations
#' innov <- matrix(rnorm(1e3, sd=0.01))
#' # Simulate Dickey-Fuller process using Rcpp
#' prices <- HighFreq::sim_df(init_price=init_price, eq_price=eq_price, theta=thetav, coeff=coeff, innov=innov)
#' plot(prices, t="l", main="Simulated Dickey-Fuller Prices")
#' }
#' 
#' @export
sim_df <- function(init_price, eq_price, theta, coeff, innov) {
    .Call('_HighFreq_sim_df', PACKAGE = 'HighFreq', init_price, eq_price, theta, coeff, innov)
}

#' Calculate the log-likelihood of a time series of returns assuming a
#' \emph{GARCH(1,1)} process.
#' 
#' @param \code{omega} Parameter proportional to the long-term average level
#'   of variance.
#' 
#' @param \code{alpha} The weight associated with recent realized variance
#'   updates.
#' 
#' @param \code{beta} The weight associated with the past variance estimates.
#' 
#' @param \code{returns} A single-column \emph{matrix} of returns.
#' 
#' @param \code{minval} The floor value applied to the variance, to avoid zero
#'   values. (The default is \code{minval = 0.000001}.)
#' 
#' @return The log-likelihood value.
#'
#' @details
#'   The function \code{lik_garch()} calculates the log-likelihood of a time
#'   series of returns assuming a \emph{GARCH(1,1)} process.
#'   
#'   It first estimates the rolling variance of the \code{returns} argument
#'   using function \code{sim_garch()}:
#'   \deqn{
#'     \sigma^2_i = \omega + \alpha r^2_i + \beta \sigma_{i-1}^2
#'   }
#'   Where \eqn{r_i} is the time series of returns, and \eqn{\sigma^2_i} is the
#'   estimated rolling variance.
#'   And \eqn{\omega}, \eqn{\alpha}, and \eqn{\beta} are the \emph{GARCH}
#'   parameters.
#'   It applies the floor value \code{minval} to the variance, to avoid zero
#'   values.  So the minimum value of the variance is equal to \code{minval}.
#'
#'   The function \code{lik_garch()} calculates the log-likelihood assuming a
#'   normal distribution of returns conditional on the variance
#'   \eqn{\sigma^2_{i-1}} in the previous period, as follows:
#'   \deqn{
#'     likelihood = - \sum_{i=1}^n (\frac{r^2_i}{\sigma^2_{i-1}} + \log(\sigma^2_{i-1}))
#'   }
#'
#' @examples
#' \dontrun{
#' # Define the GARCH model parameters
#' alpha <- 0.79
#' betav <- 0.2
#' om_ega <- 1e-4*(1-alpha-betav)
#' # Calculate historical VTI returns
#' returns <- na.omit(rutils::etfenv$returns$VTI)
#' # Calculate the log-likelihood of VTI returns assuming GARCH(1,1)
#' HighFreq::lik_garch(omega=om_ega, alpha=alpha,  beta=betav, returns=returns)
#' }
#' 
#' @export
lik_garch <- function(omega, alpha, beta, returns, minval = 0.000001) {
    .Call('_HighFreq_lik_garch', PACKAGE = 'HighFreq', omega, alpha, beta, returns, minval)
}

#' Calculate the optimal portfolio weights for different types of objective
#' functions.
#' 
#' @param \code{returns} A \emph{time series} or a \emph{matrix} of returns
#'   data (the returns in excess of the risk-free rate).
#'   
#' @param \code{method} A \emph{string} specifying the method for
#'   calculating the weights (see Details) (the default is \code{method =
#'   "ranksharpe"})
#'   
#' @param \code{eigen_thresh} A \emph{numeric} threshold level for discarding
#'   small singular values in order to regularize the inverse of the
#'   \code{returns} matrix (the default is \code{1e-5}).
#'   
#' @param \code{eigen_max} An \emph{integer} equal to the number of singular
#'   values used for calculating the shrinkage inverse of the \code{returns}
#'   matrix (the default is \code{0} - equivalent to \code{eigen_max} equal to
#'   the number of columns of \code{returns}).
#'   
#' @param \code{conf_lev} The confidence level for calculating the
#'   quantiles (the default is \code{conf_lev = 0.75}).
#'
#' @param \code{alpha} The shrinkage intensity between \code{0} and \code{1}.
#'   (the default is \code{0}).
#' 
#' @param \code{scale} A \emph{Boolean} specifying whether the weights should
#'   be scaled (the default is \code{scale = TRUE}).
#'
#' @param \code{vol_target} A \emph{numeric} volatility target for scaling the
#'   weights (the default is \code{0.001})
#'   
#' @return A column \emph{vector} of the same length as the number of columns
#'   of \code{returns}.
#'
#' @details
#'   The function \code{calc_weights()} calculates the optimal portfolio
#'   weights for different types of methods, using \code{RcppArmadillo}
#'   \code{C++} code.
#' 
#'   If \code{method = "ranksharpe"} (the default) then it calculates the
#'   weights as the ranks (order index) of the trailing Sharpe ratios of the
#'   asset \code{returns}.
#'
#'   If \code{method = "rank"} then it calculates the weights as the ranks
#'   (order index) of the last row of the \code{returns}.
#'
#'   If \code{method = "max_sharpe"} then \code{calc_weights()} calculates
#'   the weights of the maximum Sharpe portfolio, by multiplying the inverse of
#'   the covariance \emph{matrix} times the mean column returns.
#'
#'   If \code{method = "min_var"} then it calculates the weights of the
#'   minimum variance portfolio under linear constraints.
#'
#'   If \code{method = "min_varpca"} then it calculates the weights of the
#'   minimum variance portfolio under quadratic constraints (which is the
#'   highest order principal component).
#'
#'   If \code{scale = TRUE} (the default) then the weights are scaled so that
#'   the resulting portfolio has a volatility equal to \code{vol_target}.
#'
#'   \code{calc_weights()} calculates the shrinkage inverse of the covariance
#'   \emph{matrix} of \code{returns} from its eigen decomposition.  It applies
#'   dimension regularization by selecting only the largest eigenvalues equal
#'   in number to \code{eigen_max}. 
#'   
#'   In addition, \code{calc_weights()} applies shrinkage to the columns of
#'   \code{returns}, by shrinking their means to their common mean value. The
#'   shrinkage intensity \code{alpha} determines the amount of shrinkage that
#'   is applied, with \code{alpha = 0} representing no shrinkage (with the
#'   column means of \code{returns} unchanged), and \code{alpha = 1}
#'   representing complete shrinkage (with the column means of \code{returns}
#'   all equal to the single mean of all the columns).
#' 
#' @examples
#' \dontrun{
#' # Calculate covariance matrix of ETF returns
#' returns <- na.omit(rutils::etfenv$returns[, 1:16])
#' eigend <- eigen(cov(returns))
#' # Calculate shrinkage inverse of covariance matrix
#' eigen_max <- 3
#' eigenvec <- eigend$vectors[, 1:eigen_max]
#' eigenval <- eigend$values[1:eigen_max]
#' inverse <- eigenvec %*% (t(eigenvec) / eigenval)
#' # Define shrinkage intensity and apply shrinkage to the mean returns
#' alpha <- 0.5
#' colmeans <- colMeans(returns)
#' colmeans <- ((1-alpha)*colmeans + alpha*mean(colmeans))
#' # Calculate weights using R
#' weights <- inverse %*% colmeans
#' n_col <- NCOL(returns)
#' weightsr <- weightsr*sd(returns %*% rep(1/n_col, n_col))/sd(returns %*% weightsr)
#' # Calculate weights using RcppArmadillo
#' weights <- drop(HighFreq::calc_weights(returns, eigen_max, alpha=alpha))
#' all.equal(weights, weightsr)
#' }
#' 
#' @export
calc_weights <- function(returns, method = "ranksharpe", eigen_thresh = 1e-5, eigen_max = 0L, conf_lev = 0.1, alpha = 0.0, scale = TRUE, vol_target = 0.01) {
    .Call('_HighFreq_calc_weights', PACKAGE = 'HighFreq', returns, method, eigen_thresh, eigen_max, conf_lev, alpha, scale, vol_target)
}

#' Simulate (backtest) a rolling portfolio optimization strategy, using
#' \code{RcppArmadillo}.
#' 
#' @param \code{returns} A \emph{time series} or a \emph{matrix} of returns
#'   data (the returns in excess of the risk-free rate).
#'   
#' @param \code{excess} A \emph{time series} or a \emph{matrix} of excess
#'   returns data (the returns in excess of the risk-free rate).
#'   
#' @param \code{startp} An \emph{integer vector} of start points.
#' 
#' @param \code{endp} An \emph{integer vector} of end points.
#' 
#' @param \code{lambda} A \emph{numeric} decay factor to multiply the past
#'   portfolio weights.  (The default is \code{lambda = 0} - no memory.)
#'   
#' @param \code{coeff} A \emph{numeric} multiplier of the weights.  (The
#'   default is \code{1})
#'   
#' @param \code{bid_offer} A \emph{numeric} bid-offer spread (the default is
#'   \code{0})
#'
#' @param \code{method} A \emph{string} specifying the method for calculating
#'   the weights (see Details) (the default is \code{method = "ranksharpe"})
#'   
#' @param \code{eigen_thresh} A \emph{numeric} threshold level for discarding
#'   small singular values in order to regularize the inverse of the
#'   \code{returns} matrix (the default is \code{1e-5}).
#'   
#' @param \code{eigen_max} An \emph{integer} equal to the number of singular
#'   values used for calculating the shrinkage inverse of the \code{returns}
#'   matrix (the default is \code{0} - equivalent to \code{eigen_max} equal to
#'   the number of columns of \code{returns}).
#'   
#' @param \code{conf_lev} The confidence level for calculating the
#'   quantiles (the default is \code{conf_lev = 0.75}).
#'
#' @param \code{alpha} The shrinkage intensity between \code{0} and \code{1}.
#'   (the default is \code{0}).
#' 
#' @param \code{scale} A \emph{Boolean} specifying whether the weights should
#'   be scaled (the default is \code{scale = TRUE}).
#'
#' @param \code{vol_target} A \emph{numeric} volatility target for scaling the
#'   weights (the default is \code{1e-5})
#'   
#' @return A column \emph{vector} of strategy returns, with the same length as
#'   the number of rows of \code{returns}.
#'
#' @details
#'   The function \code{back_test()} performs a backtest simulation of a
#'   rolling portfolio optimization strategy over a \emph{vector} of the end
#'   points \code{endp}.
#'   
#'   It performs a loop over the end points \code{endp}, and subsets the
#'   \emph{matrix} of the excess asset returns \code{excess} along its rows,
#'   between the corresponding \emph{start point} and the \emph{end point}. It
#'   passes the subset matrix of excess returns into the function
#'   \code{calc_weights()}, which calculates the optimal portfolio weights at
#'   each \emph{end point}. The arguments \code{eigen_max}, \code{alpha},
#'   \code{method}, and \code{scale} are also passed to the function
#'   \code{calc_weights()}.
#'   
#'   It then recursively averages the weights \eqn{w_i} at the \emph{end point
#'   = i} with the weights \eqn{w_{i-1}} from the previous \emph{end point =
#'   (i-1)}, using the decay factor \code{lambda = \eqn{\lambda}}:
#'   \deqn{
#'     w_i = (1-\lambda) w_i + \lambda w_{i-1}
#'   }
#'   The purpose of averaging the weights is to reduce their variance to
#'   improve their out-of-sample performance.  It is equivalent to extending
#'   the portfolio holding period beyond the time interval between neighboring
#'   \emph{end points}.
#'   
#'   The function \code{back_test()} then calculates the out-of-sample strategy
#'   returns by multiplying the average weights times the future asset returns.
#'   
#'   The function \code{back_test()} multiplies the out-of-sample strategy
#'   returns by the coefficient \code{coeff} (with default equal to \code{1}),
#'   which allows simulating either a trending strategy (if \code{coeff = 1}),
#'   or a reverting strategy (if \code{coeff = -1}).
#'   
#'   The function \code{back_test()} calculates the transaction costs by
#'   multiplying the bid-offer spread \code{bid_offer} times the absolute
#'   difference between the current weights minus the weights from the previous
#'   period. Then it subtracts the transaction costs from the out-of-sample
#'   strategy returns.
#'   
#'   The function \code{back_test()} returns a \emph{time series} (column
#'   \emph{vector}) of strategy returns, of the same length as the number of
#'   rows of \code{returns}.
#'
#' @examples
#' \dontrun{
#' # Calculate the ETF daily excess returns
#' returns <- na.omit(rutils::etfenv$returns[, 1:16])
#' # riskf is the daily risk-free rate
#' riskf <- 0.03/260
#' excess <- returns - riskf
#' # Define monthly end points without initial warmpup period
#' endp <- rutils::calc_endpoints(returns, interval="months")
#' endp <- endp[endp > 0]
#' nrows <- NROW(endp)
#' # Define 12-month look-back interval and start points over sliding window
#' look_back <- 12
#' startp <- c(rep_len(1, look_back-1), endp[1:(nrows-look_back+1)])
#' # Define shrinkage and regularization intensities
#' alpha <- 0.5
#' eigen_max <- 3
#' # Simulate a monthly rolling portfolio optimization strategy
#' pnls <- HighFreq::back_test(excess, returns, 
#'                             startp-1, endp-1, 
#'                             eigen_max = eigen_max, 
#'                             alpha = alpha)
#' pnls <- xts::xts(pnls, index(returns))
#' colnames(pnls) <- "strat_rets"
#' # Plot dygraph of strategy
#' dygraphs::dygraph(cumsum(pnls), 
#'   main="Cumulative Returns of Max Sharpe Portfolio Strategy")
#' }
#' 
#' @export
back_test <- function(excess, returns, startp, endp, lambda, method = "ranksharpe", eigen_thresh = 1e-5, eigen_max = 0L, conf_lev = 0.1, alpha = 0.0, scale = TRUE, vol_target = 0.01, coeff = 1.0, bid_offer = 0.0) {
    .Call('_HighFreq_back_test', PACKAGE = 'HighFreq', excess, returns, startp, endp, lambda, method, eigen_thresh, eigen_max, conf_lev, alpha, scale, vol_target, coeff, bid_offer)
}

