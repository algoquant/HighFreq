############
### ToDo list for package HighFreq: high frequency data scrubbing

### Scripts for package building

## Steps for installing package called my_package, containing Rcpp code:
1. Create empty package directory with Rcpp::Rcpp.package.skeleton()
2. In the R directory create a file called my_package.R and add the header:
#' @useDynLib my_package
#' @importFrom Rcpp evalCpp
#' @exportPattern "^[[:alpha:]]+"
NULL
Add your R functions below the header.
3. Finally, from the Build menu run Clean and rebuild.


## How to update the NAMESPACE file after adding a new function
Edit the NAMESPACE file and add a line to export the new function new_func(): 
export(new_func)


## Steps to fix the NAMESPACE file after adding a new function - sometimes produces a bad NAMESPACE file
1. Edit the NAMESPACE file and remove all lines
2. In RStudio run "document package" - sometimes produces a bad NAMESPACE file
3. Edit the NAMESPACE file again and remove all lines that don't contain export()
4. Add by hand to top of NAMESPACE file the lines:
useDynLib(HighFreq)
importFrom(Rcpp, evalCpp)


## Build scripts:

# Install package from source on local drive
install.packages(pkgs="C:/Develop/R/HighFreq", repos=NULL, type="source")
# Install package from source on local drive using R CMD
R CMD INSTALL C:\Develop\R\HighFreq
# Install package from github
devtools::install_github(repo="algoquant/HighFreq", force=TRUE)
# build vignette package reference manual from *.Rd files
system("R CMD Rd2pdf C:/Develop/R/HighFreq")
cd C:\Develop\R\HighFreq\vignettes
R CMD Rd2pdf C:\Develop\R\HighFreq\



### Comments

# When performing a rolling calculation, it's better to apply a single lag to a time series outside the loop instead of lagging it inside at each end point

# Passing pointers as arguments into functions increases their speed by 30% or more for very large vectors with a million or more elements.  But it can make it harder to pass in arguments when calling them from inside other functions.  The solution is to avoid nesting function calls, and instead create intermediate variables, and pass them into functions.  Passing in pointers also allows using const arguments so that they can't be modified inside the function.

# Volume weighted volatility is higher than simple average volatility because volumes are higher when volatility is higher. 

# The yang_zhang method has the lowest standard error assuming normal distribution, but not necessarily for fat-tailed distributions.
In addition, the yang_zhang method has lower bias caused by finite sampling.
The rogers_satchell method has the biggest bias caused by finite sampling and opening price jumps, and seriously underestimates the variance.

# Theoretically, the Yang-Zhang (YZ) and Garman-Klass-Yang-Zhang (GKYZ) range variance estimators are unbiased and have up to seven times smaller standard errors than the standard close-to-close estimator.  But in practice, prices are not observed continuously, so the price range is underestimated, and so is the variance when using the YZ and GKYZ range estimators.  Therefore in practice the YZ and GKYZ range estimators are biased (underestimate volatility).  In addition, the standard errors are reduced less than by the theoretical amount, for the same reason. 

# Using the average of open and close prices in the standard variance estimator (instead of just the close prices) doesn't reduce the standard error.

# High frequency data has three tenor horizons: daily (daily or lower frequency of data), intraday (secondly to daily frequency of data), and sub-second tenor horizons (sub-secondly or higher frequency of data).
At the sub-second tenor horizon, returns are dominated by noise and the bid-ask bounce.
Returns are dominated in their scale by the volatility, and they have no discernable trend.
At the sub-second horizon technical indicators are meaningless, and instead traders analyze the order book for clues about future returns.
At the intraday tenor horizon volatility, skew, and kurtosis are high, and mean-reversion dominates trending.
Technical indicators based on past returns and trading volumes can offer clues about future returns.
At the daily tenor horizon skew and kurtosis are small, and trending dominates mean-reversion.

# Calculating returns on 1-second bars doesn't make sense for two reasons: 
First, because the changes in price are very small, comparable to the precision of prices (number of significant digits).
As a result, price changes on 1-second bars are either zero or low multiples of the precision. 
Second, the 1-second bar prices bounce randomly between static bid and ask prices (the bid-ask bounce).  
Therefore 1-second bar returns carry almost no information.


### Tasks to-do

+ [ ] Enhance the function calc_var() to calculate the variance for overlapping returns: the variance of returns aggregated over k time periods

+ [ ] Create a function called get_fun() for dispatching functions in C++: it should accept a string with a function name and return a pointer to the function with that name

+ [ ] Create a function called roll_mean() for the rolling location estimator (first moment) using RcppArmadillo: rolling mean, median, Hodges-Lehmann, and other unimodal robust estimators

+ [ ] Create function calc_covar() to calculate a nonparametric (robust) covariance matrix
Two possible methods are: 
Quantile estimator: weighted sums of the median and higher quantiles of asset returns.
Hodges-Lehmann estimator: 

+ [ ] In function back_test() add parameter for lagging the weights - for applying the weights with an extra lag

+ [ ] In function calc_weights() shrink the returns to a portfolio with weights proportional to the asset betas

+ [ ] Add unit testing using testthat and devtools: create /tests directory

+ [ ] Create function calc_quantile() to calculate or update the quantile of a vector of numbers
The value of the quantile can be updated based on the old quantile value and the new updated vector values.
If there are very few new vector values and if they're all far from the old quantile value, then the quantile may remain unchanged.
Apply it to the calculation or rolling quantiles.

+ [ ] Modify function back_test() so it accepts a C++ prediction/forecasting function (functor, function pointer)  
https://stackoverflow.com/questions/7143120/convert-string-to-variable-name-or-variable-type
http://bytes.com/forum/thread656124.html
https://bytes.com/topic/c/answers/561361-string-convert-function-name

+ [ ] Pass a function pointer
https://gallery.rcpp.org/articles/passing-cpp-function-pointers/
https://en.wikipedia.org/wiki/Function_pointer#Example_in_C

+ [ ] Pass model function and its parameters into back_test() as a vector or a list
http://gallery.rcpp.org/articles/rcppnt2-introduction/
http://gallery.rcpp.org/articles/passing-cpp-function-pointers/
https://stackoverflow.com/questions/43616778/passing-user-created-c-functions-in-rcpp-as-arguments
https://stackoverflow.com/questions/51274385/calling-rcpp-function-with-xptr-and-function-only-xptr-case-works
https://stackoverflow.com/questions/50548060/generating-xptr-from-rcpp-function
https://stackoverflow.com/questions/49539341/rcpp-lost-on-how-to-utilize-rcppxptr-to-wrap-a-pointer

+ [ ] Pass parameters to function back_test() using parameter pack and dots argument to C++ functions
https://vcpptips.wordpress.com/tag/three-dots-as-parameter/
https://xenakios.wordpress.com/2014/01/16/c11-the-three-dots-that-is-variadic-templates-part/
https://stackoverflow.com/questions/24590946/passing-many-argumentes-by-ellipsis-in-rcpp
https://kevinushey.github.io/blog/2016/01/27/introduction-to-c++-variadic-templates/
https://codesteps.com/2014/05/16/how-to-pass-variable-number-of-arguments-to-c-cpp-functions/
https://stackoverflow.com/questions/39792417/what-does-this-three-dots-means-in-c
https://www.wikihow.com/Use-Function-Template-Parameter-Packs-in-C%2B%2B
https://en.cppreference.com/w/cpp/language/parameter_pack

+ [ ] Convert all loops to STL iterators: iterators are pointers to vector and matrix elements 
https://www.geeksforgeeks.org/iterators-c-stl/

+ [ ] Replace only some Rcpp syntax with Armadillo or Boost or STL
There are several advantages of using Rcpp::NumericVector instead of std::vector<double> :
NumericVector avoids deep copy of data between R and C++
NumericVector provides access to Sugar functions.
For example, if you replace NumericVector with std::vector<double> then you lose flexibility and need two copies of every function - one for integer and one for double.
cpp::NumericVector over std::vector<double>
https://stackoverflow.com/questions/41602024/should-i-prefer-rcppnumericvector-over-stdvector

+ [ ] Create function calc_alpha() to calculate the performance of an asset relative to an index. 
calc_alpha() should implement several different methods, with the default being Jensen's alpha. 
One robust method should be the Wilcoxon W statistic.
Another robust method should be the Kruskal-Wallis H statistic as follows:
Combine the given asset returns with the index returns into a single vector.
Rank the vector of combined returns.
Calculate the H statistic as the difference between the sum of the ranks of the asset returns minus the sum of the ranks of the S&P500 returns.
The H statistic is reminiscent of the difference between the mean asset returns minus the mean S&P500 returns.

+ [ ] Create function calc_ret_stats() for calculating aggregations (statistics) over a vector of returns
Examples of statistics are the mean, median, Sharpe ratio (t-value), Wilcoxon statistic, etc.

+ [ ] Create function calc_reg() for performing different types of regressions: standard lm regression, PCA regression with shrinkage, robust Theil-Sen regression, rank regression, etc.
It should return a named vector of coefficients, z-scores, R-squared, etc.
It should calculate Pearson, Spearman, and Kendall correlations.

+ [ ] Create function roll_reg() for performing rolling PCA and robust regressions  
Add option to perform rolling predictions using the regressions.

+ [ ] Create a function called roll_stdev() for the rolling dispersion estimator (second moment) using RcppArmadillo: rolling standard deviation, MAD, and other unimodal robust estimators
Use moment update formulas from package fromo: Pebay Fast Moments.pdf
Calculate running variance efficiently (used by package fromo): https://www.johndcook.com/blog/standard_deviation/

+ [ ] Create a function called roll_skew() for the rolling skewness estimator (third moment) using RcppArmadillo: include robust estimators

+ [ ] Create a function called roll_count() using RcppArmadillo which reproduces roll_countr() in scratch.R
roll_count() is already in lm_arma.cpp.
roll_count() should count the number of consecutive TRUE elements, and reset to zero after every FALSE element.
Create a contrarian strategy using roll_count(): the number of consecutive close_low or close_high.
Scale the count threshold levels depending on the level of volatility.

+ [ ] Modify function back_test() so it returns a matrix of strategy returns and positions

+ [ ] Modify function roll_vwap() so that it calls RcppRoll::roll_sum() ? 

+ [ ] Convert function calls to roll functions from package RcppRoll to native Rcpp functions: roll_max() and roll_var()

+ [ ] Rename roll_moment(): it doesn't calculate moments, because it doesn't subtract the rolling means
Deprecate function roll_moment() because it's similar to roll_vwap() ?
Update the description documentation of roll_moment()

+ [ ] Perform rolling regressions over trading time - using a time-dependent look-back interval, so that the look-back interval always spans the same traded volume  
Inoue Rolling Regressions Time Series Bias Variance Tradeoff.pdf
Easley Volume Clock Trading Paradigm.pdf
https://quantivity.wordpress.com/2012/10/23/volume-clock-gaps-and-goog/

+ [ ] Modify roll_zscores() so it updates the previous regression, instead of performing a new regression at every point in time

+ [ ] Modify function calc_scaled() so it performs the calculation in place, without copying the matrix in memory ?
That may not always be correct, for example in the case of rolling calculations.

+ [ ] Create a function called na_locf()
// [[Rcpp::export]]
NumericVector na_locf(NumericVector x) {
  NumericVector output = Rcpp::clone(x);
  double lastNonNA = NA_REAL;
  int n = x.size();

  for (int i = 0; i < n; ++i)
  {
    double value = output[i];
    if (!ISNAN(value))
      lastNonNA = value;
    else
      output[i] = lastNonNA;
  }
  return output;
}
arma::uvec any_na_x(const NumericMatrix& x) {
  int n_rows_x = x.nrow();
  int n_cols_x = x.ncol();
  arma::uvec result(n_rows_x);
  
  for (int i = 0; i < n_rows_x; i++) {
    int any_na = 0;
    int j = 0;
    
    while ((any_na == 0) && (j < n_cols_x)) {
      if (std::isnan(x(i, j))) {
        any_na = 1;
      }
      j += 1;
    }
    result[i] = any_na;
  }
  return result;
}
arma::uvec any_na_xy(const NumericMatrix& x, const NumericMatrix& y) {
  int n_rows_xy = x.nrow();
  int n_cols_x = x.ncol();
  int n_cols_y = y.ncol();
  arma::uvec result(n_rows_xy);
  
  for (int i = 0; i < n_rows_xy; i++) {
    int any_na = 0;
    int j = 0;
    int k = 0;
    while ((any_na == 0) && (j < n_cols_x)) {
      if (std::isnan(x(i, j))) {
        any_na = 1;
      }
      j += 1;
    }
    while ((any_na == 0) && (k < n_cols_y)) {
      if (std::isnan(y(i, k))) {
        any_na = 1;
      }
      k += 1;
    }
    result[i] = any_na;
  }
  return result;
}

+ [ ] Create a function in RcppArmadillo which replicates the function zoo::na.fill(), and which replaces NA, NaN, and Inf values, but in place, without returning a value

+ [ ] Add functions for median and MAD in Rcpp
http://gallery.rcpp.org/articles/robust-estimators/

+ [ ] Apply STL algorithm function to perform find/which operation
http://adv-r.had.co.nz/Rcpp.html#stl

+ [ ] Add very fast RcppArmadillo functions: whi_ch(), cum_sum(), select_sub_mat()
http://arma.sourceforge.net/docs.html
https://stackoverflow.com/questions/23849354/equivalent-of-which-function-in-rcpp

+ [ ] Create new roll functions using RcppRoll::rollit() ?

+ [ ] Convert roll_*() functions to RcppRoll ?
No, because RcppArmadillo and Rcpp
are better

+ [ ] Add to back_test() the simulation with limit orders

+ [ ] Remove all parallel code and references to parallel packages from HighFreq  

+ [ ] Create function predict_lm() which calculates the predicted out-of-sample values based on the coefficients from calc_lm()

+ [ ] Modify functional roll_backtest() so that it performs lapply() and returns a list of trade_func() outputs, containing the xts time series of out-of-sample returns   
trade_func() should return a list with at least two elements: time series and named vector of statistics (Sharpe, etc.)

+ [ ] Create functions roll() and run() in Rcpp - already done ?

+ [ ] Modify function roll_sum() so that it also calculates the rolling sum over the columns of a matrix ?

+ [ ] Add to function random_ohlc(): simulate random non-normal OHLC prices from Heston model

+ [ ] Apply dynamic dispatch to create s single function to handle both vector and matrix arguments
https://gallery.rcpp.org/articles/dynamic-dispatch-for-sparse-matrices/
https://stackoverflow.com/questions/27466319/templated-matrix-in-rcpp
https://stackoverflow.com/questions/22513529/templated-function-for-sparse-and-dense-matrices-in-rcpparmadillo
https://stackoverflow.com/questions/45163887/how-can-i-use-a-c-function-template-in-r-via-rcpp

+ [ ] Fix bug in function random_ohlc() when sampling from rows of an input OHLC series (oh_lc not NULL): overnight returns are resampled into intraday periods, causing overestimation of variance.  
As a result, bootstrapped standard errors for Garman-Klass-Yang-Zhang and Yang-Zhang methods are too big.  
Possible fixes:
modify random_ohlc() so that it samples from secondly return data and aggregates to minutely OHLC data (does this solve the problem?) - save random minutely OHLC prices to file for future use  
modify random_ohlc() so that it samples from minutely return data scaled by time, and aggregates to minutely OHLC data

+ [ ] Bootstrap the range volatility estimators on random non-normal OHLC prices to check if indeed they have lowest standard error
use package meboot for maximum entropy bootstrap of time series ?
https://cran.r-project.org/web/packages/meboot/index.html

+ [ ] Create a function for calculating microprice  

+ [ ] Tell Joshua Ulrich that there's a bug in function TTR::volatility(), in the formula for the k coefficient in the Yang-Zhang estimator p.49
The correct formula is on p.7 of Yang OHLC Range Volatility Estimators.pdf

+ [x] Create the functions called agg_ohlc() and to_period() for aggregating an OHLC time series to lower periodicity, similar to xts::to.period()

+ [x] Modify the functions diff_it() and diff_vec() so they don't use arma::diff(), because it doesn't apply lags greater than one

+ [x] In function calc_weights() scale the weights so that the resulting portfolio has a volatility of 1% - make this a parameter

+ [x] In function calc_inv() add option to regularize the inverse by discarding small eigenvalues less than a tolerance level: use arma::pinv()
This is because the best choice for max_eigen can change in a rolling calculation, so it's better to introduce a tolerance cutoff.

+ [x] In function calc_weights() add option for quantile weights, with argument "typ_e == quantile" and add argument "quan_tile == 0.1": with weights equal to 1 for top quantile, -1 for bottom quantile, and otherwise 0.

+ [x] Change license from GPL to MPL, similar to package data.table.

+ [x] Rename roll_conv() to roll_conv_ref(), and modify roll_conv() so it doesn't copy over the input argument mat_rix.
Copying over the input argument mat_rix causes snooping data leak in backtest.

+ [x] Test the function roll_conv() to determine if it applies convolution only over past values.

+ [x] Rename function roll_wsum() to roll_conv(), because it replicates convolution - no!
The function roll_wsum() is for vectors and is not the same as roll_conv() which is for matrices.

+ [x] Create RcppArmadillo function calc_ranks() for calculating the ranks of the elements of a vector.

+ [x] Remove the existing function roll_variance() in R, and rewrite it in RcppArmadillo as roll_var()

+ [x] Compare the speed of performing RcppArmadillo loops over columns versus subviews of rows: loops over rows are slightly faster than over columns

+ [x] Create a function called roll_var_ohlc() in RcppArmadillo which calculates the rolling variance over a time series of OHLC prices

+ [x] Rename roll_var() to roll_var_vec() and update its documentation

+ [x] Create a function called roll_var() in RcppArmadillo which calculates the rolling variance over a matrix

+ [x] Create the functions called diff_vec() and diff_it() (for matrices), similar to rutils::diff_it(), using the RcppArmadillo function arma::diff()

+ [x] Create a function called calc_var_ohlc() in RcppArmadillo, which calculates the variance of a time series of OHLC prices, similar to the function calc_var_ohlc_r()

+ [x] Rename the existing R function calc_variance() to calc_var_ohlc_r(), and refactor the code to make it faster and more readable

+ [x] Create a function called lag_it() using RcppArmadillo which lags a column vector or a matrix, similar to rutils::lag_it()

+ [x] Create a function called lag_vec() using RcppArmadillo which lags a vector

+ [x] Create a function called calc_var() in RcppArmadillo to calculate the variance of a matrix

+ [x] Create a function called calc_var_vec() in RcppArmadillo to calculate the variance of a vector

+ [x] Add RcppArmadillo function mult_vec_mat(), which multiplies the columns or rows of a matrix times a vector, element-wise.

+ [x] In back_test() fix bid_offer bug: should be subtracted from PnL not added  

+ [x] In function calc_weights() add additional option for rank momentum, with argument "typ_e == rank": rank weights using trailing Sharpe

+ [x] In function back_test() add argument co_eff to multiply the weights.

+ [x] In function back_test() add transaction costs: remember previous weights and subtract current weights from previous weights
Add argument bid_offer the bid-offer spread.

+ [x] Rename function roll_portf() to back_test()

+ [x] Add header to HighFreq.R file to fix NAMESPACE issue:
#' @useDynLib HighFreq
#' @importFrom Rcpp evalCpp
#' @exportPattern "^[[:alpha:]]+"

+ [x] Create a function called roll_conv() for calculating the convolutions of the matrix columns

+ [x] In function calc_weights() add additional options for the typ_e argument: quadratic weights constraint (highest principal component)

+ [x] In function calc_weights() modify weight_s scaling: scale the weights to match the volatility of mean returns

+ [x] In function calc_weights() add argument called typ_e (with default value "max_sharpe"), which specifies the objective function of portfolio optimization used for calculating the weights

+ [x] Replace zoo::na.locf() with xts:::na.locf.xts()

+ [x] Remove RcppParallel dependency from DESCRIPTION

+ [x] Add ByteCompile: TRUE to the DESCRIPTION file

+ [x] Generate man files for Rcpp functions
select the "Document" item in build menu or run R command devtools::document()

+ [x] Modify DESCRIPTION file
Added: Encoding: UTF-8
Removed from Imports: caTools, lubridate, TTR, 

+ [x] Fix build error about RcppExports.R.

+ [x] Modify functional roll_apply()
Change endpoints calculation: define look_backs as a list of numeric vectors.
Calculate aggregations using lapply() loop over the look_backs, instead of sapply().
Add option to coerce output into xts series.

+ [x] Create functional roll_backtest() similar to roll_apply(), but accepting two functions as arguments

+ [x] Replace argument "lag" with "lagg" (to avoid confusing it with stats function lag())  

+ [x] Replace calls to is.vector(in_put) with is.null(dim(in_put))

+ [x] Replace "win_dow" with "look_back"

+ [x] Fix bug in function run_variance() for calc_method="yang_zhang": remove win_dow from formula for co_eff.

+ [x] Replace xts::.subset_xts() with brackets operator []

+ [x] Add volatility, drift rate, and convexity correction to functions random_taq() and random_ohlc()  

+ [x] Fix bug in random_ohlc(): transform to normal distribution before sampling from rows of OHLC (otherwise it produces negative prices)  

+ [x] Move functions adjust_ohlc(), to_period() and get_symbols() from HighFreq to rutils  

+ [x] Remove and/or adjust time scaling factors in run_variance(), run_skew(), and random_ohlc()

+ [x] Rename functions *_OHLC and *_TAQ to *_ohlc and *_taq

+ [x] Create function adjust_ohlc() that adjusts OHLC data, similar to quantmod::adjustOHLC()  

+ [x] Create function get_symbols() that downloads time series data, similar to quantmod::getSymbols()  

+ [x] Replace quantmod extractor functions Op(), Hi(), Lo(), Cl(), Vo() with direct subsetting: oh_lc[, 3] Instead of Lo(oh_lc)

+ [x] Modify function run_returns(): add argument  

+ [x] Update README and web page  

+ [x] Add vignettes directory and create multiple vignettes using files README and demo_HighFreq.R  

+ [x] Create function random_TAQ() that returns random TAQ data

+ [x] Create function random_OHLC()
returns random OHLC data used for testing for data snooping (look ahead bias) and benchmarking
Either create synthetic data or sample from real data.  
Run random data through model to test if there is data snooping.  

+ [x] Add a single day of TAQ data for SPY to hf_data.RData file  

+ [x] Add TLT, VXX, and SPY to hf_data.RData file  

+ [x] Add lubridate to Imports in DESCRIPTION file  

+ [x] Create function roll_sharpe()

+ [x] Modify functions run_variance() and run_skew() to scale their outputs by the time differences, similar to run_returns()  

+ [ ] Create function sea_son() for calculating time of day (year) as fraction
The time of day can be used as an input into a seasonal model

+ [ ] Create volume-weighted Hurst and density plots  
https://quantivity.wordpress.com/2012/10/23/volume-clock-gaps-and-goog/

+ [ ] Modify function season_ality() to discard elements corresponding to infrequent observations ?

+ [ ] Create Rcpp function for fast rolling aggregations over endpoints

+ [ ] Aggregate data to 10-second bars (?)  

+ [ ] Add wiki  

+ [ ] Run on travis, add .travis.yml file, add Build Status tag to README.Rmd

+ [x] Add vignette and automatically build vignette  

+ [ ] Introduce unit testing using testthat and devtools: create /tests directory

+ [ ] Modify function skew_ohlc() to accomodate case when open price isn't equal to previous close price.  
add argument "open_to_close" with default value TRUE  

+ [ ] Prove that OHLC skew formula is an estimator of skew  
Do Garman-Klass and Rogers-Satchell estimators work for processes that are not Gaussian?  
perform simulation to find out  
https://en.wikipedia.org/wiki/First-hitting-time_model  
https://en.wikipedia.org/wiki/Wiener_process  

+ [ ] Analyze paper: Shen Skew Kurtosis Stock Premiums Forecasting.pdf

+ [ ] Add logical arg as option to aggregate data or not

+ [ ] Calculate rets from scrubbed data

+ [ ] Calculate statistics (moments, quantiles) on tick and OHLC data and save them in files

+ [ ] Create function to estimate beta from HFreq data

+ [ ] Create function to forecast skewness
Show that variance is predictable over time 
Show that skewness is not predictable over time (Harvey and Siddique, 1999), 
so variables other than lagged skewness are required to forecast skewness.
Show that idiosyncratic volatility is a strong predictor of idiosyncratic skewness.

+ [x] Rename functions calc_returns(), vari_ance(), skew_ohlc(), sharpe_ohlc() to run_returns(), run_variance(), run_skew(), run_sharpe()  

+ [x] Rename function v_wap() to roll_vwap()  

+ [x] Modify function roll_hurst() using TTR::runMax()  

+ [x] Rename function hurst_ohlc() to sharpe_ohlc()  

+ [x] Rename and modify function calc_rets() to calc_returns(): 
return single column with name "SPY.returns"
will break save_rets() and save_rets_ohlc()

+ [x] Modify function v_wap() to accept argument x_ts  

+ [x] Remove .Rproj.user directory from GitHub repository
steps:
- open Windows PowerShell from GitHub Desktop
- run: git filter-branch --tree-filter 'rm -f -R .Rproj.user' HEAD
- run: git push origin master -f
http://stackoverflow.com/questions/32228841/github-gitignore-adds-folder-previously-not-on-gitignore

+ [x] Create project website for HighFreq on GitHub Pages using R Markdown  

+ [x] Rename README.md to .Rmd and add more detailed description similar this to README.md:  
https://github.com/RcppCore/rcpp-gallery/blob/gh-pages/src/2016-05-27-HRP.Rmd
use rmarkdown templates
https://rud.is/b/2016/02/04/alternate-r-markdown-templates/
https://github.com/hrbrmstr/markdowntemplates
http://svmiller.com/blog/2016/02/svm-r-markdown-manuscript/




### Tasks finished

+ [x] Create a function called sim_arima() in RcppArmadillo which replicates the function stats::filter() with method="recursive"
Adapt code from C:/Develop/R/lecture_slides/scripts/sim_arima.cpp

+ [x] Add sim_* functions using RcppArmadillo: sim_arima(), sim_garch()

+ [x] Add calc_* functions using RcppArmadillo: calc_eigen(), calc_inv(), calc_scaled(), calc_lm(), calc_weights()

+ [x] Add roll_* functions using RcppArmadillo: roll_wsum(), roll_zscores(), roll_scale(), roll_portf()

+ [x] Create function remove_jumps() to remove overnight close-to-open jumps from OHLC (?)  

+ [x] Replace na.locf() with rutils::na_locf()

+ [x] Rename function extreme_values() to which_extreme()  

+ [x] Rename function price_jumps() to which_jumps()  

+ [x] Add arguments lag and sca_le to function run_returns()

+ [x] In functions calc_variance(), run_variance(), roll_variance(), and run_skew() don't perform log(oh_lc) - that can be done externally

+ [x] Replace calls to lag_xts() with lag_it()

+ [x] Replace calls to diff_xts() with diff_it()

+ [x] Add argument sca_le to functions run_variance() and roll_variance()

+ [x] Fix functions roll_hurst() and roll_sharpe() so they call roll_variance() instead of run_variance()  
in roll_hurst() replace TTR::runMax() with RcppRoll::roll_max()

+ [x] Remove arguments off_set and roll_end_points from function roll_hurst()

+ [x] Create function calc_variance() which calculates a single variance number from the output of run_variance()

+ [x] Create function roll_variance() that replicates TTR::volatility()  
clone run_variance(), but modify it by subtracting the means
use functions RcppRoll::roll_var() and rutils::roll_sum()
add correct coefficient co_eff for yang_zhang method

+ [x] Modify comments for function run_variance() to explain that it's only an indicator, not an estimator

+ [x] Modify function run_returns(): remove time scaling, calculate percentage returns  

+ [x] Apply Hampel median filter to scrub data: that's what HighFreq does already  
http://dsp.stackexchange.com/questions/26552/what-is-a-hampel-filter-and-how-does-it-work
update the documentation to reflect that HighFreq already uses Hampel median filter

+ [x] Create functional roll_apply(), similar to xts:::rollapply.xts() and xts::period.apply()  
use apply_rolling() from utilLib

+ [x] Rename function roll_agg() to roll_moment(), and its argument esti_mator to mo_ment  

+ [x] Modify function hurst_ohlc() to calculate (C-O)/(H-L)

+ [x] Create function roll_hurst() for rolling Hurst exponent, similar to roll_sum()  
Detrended fluctuation analysis - is it like variance ratios?
Hurst analysis is related to Detrended fluctuation analysis:
https://en.wikipedia.org/wiki/Detrended_fluctuation_analysis
HurstIndex() from PerformanceAnalytics is wrong because it uses range of returns, instead of range of cumulative returns!
variance ratio test
http://quant.stackexchange.com/questions/7666/using-variance-ratios-to-test-for-mean-reversion

+ [x] Move function roll_sum() from HighFreq to rutils

+ [x] Add to function season_ality() in_dex argument

+ [x] Perform bootstrap estimation of standard errors for all the methods in function vari_ance()  
benchmark to random OHLC

+ [x] Add to function vari_ance() the methods "garman.klass_yz" and "yang.zhang"  

+ [x] Modify functions vari_ance() to accomodate case when open price isn't equal to previous close price - methods "garman.klass_yz" and "yang.zhang"  

+ [x] Rename function vol_ohlc() to vari_ance()

+ [x] Create function to_period()  

+ [x] Rename argument agg_fun to esti_mator, in functions agg_regate() and roll_agg()  

+ [x] Remove calc_method argument from agg_regate() and roll_agg() - instead use "..." argument

+ [x] Rename agg_ohlc and roll_agg_ohlc() to agg_regate() and roll_agg()  

+ [x] Dereference all external functions using "::", i.e. rutils::na_me()

+ [x] Add @export to roxygen code  

+ [x] Convert all code from nrow() and ncol() to NROW() and NCOL()  

+ [x] Replace strsplit(colnames(ohlc)[1], split="[.]")[[1]][1] With rutils::na_me(ohlc)  

+ [x] Fix roxygen .Rd documentation file building error - name at end of hf_data.R file was wrong - should be the names of objects, i.e. "SPY", "sym_bol", not the file name  

+ [x] Create data documention  

+ [x] Rename moment_ohlc() and roll_moment_ohlc() to agg_ohlc and roll_agg_ohlc()
mom_fun to agg_fun

+ [x] Create function hurst_ohlc() for calculating Hurst exponent from OHLC data
OHLC data naturally lends itself to Hurst analysis: ratio of (H-L)/(C-O)

+ [x] In moment_ohlc() and roll_moment_ohlc() removed log of ohlc
moment_ohlc() and roll_moment_ohlc() should pass ohlc to skew_ohlc() and vol_ohlc(), etc., not log ohlc

+ [x] Modify skew_ohlc() and vol_ohlc() to accept ohlc, not log ohlc, and to apply log internally

+ [x] Include rutil package 

+ [x] Create function for performing daily, weekly, and monthly seasonality aggregations

+ [x] Rename functions run_*() to roll_*()

+ [x] Rename save_OHLC() to save_scrub_agg()

+ [x] In save_rets_OHLC() update documentation

+ [x] In save_OHLC() combine sapply loops into one

+ [x] In save_OHLC() and save_rets() pass scrub params to scrub_agg

+ [x] Add timezone to argument list

+ [x] Create function save_TAQ()
	saves scrubbed TAQ and/or OHLC data in daily files
	similar to save_OHLC, but doesn't aggregate and saves into multiple files

+ [x] Create function calc_rets()
	calculate returns of time series

created function save_rets() (similar to save_OHLC): 
	scrub and aggregate data, calculate returns, and save them

created function save_rets_OHLC()
	similar to save_rets, but assumes clean OHLC input data
	- no scrubbing or aggregation


### Tasks deprecated

+ [ ] Modify function run_returns() to perform sapply() if argument has multiple columns ?

+ [ ] Remove extra increment of 1 in start_points index in roll_apply() - no, because otherwise it would create overlaps when aggregating returns (for example)  
start_points <-  end_points[c(rep_len(1, win_dow-1), 1:(len_gth-win_dow+1))]
instead of:
start_points <-  end_points[c(rep_len(1, win_dow-1), 1:(len_gth-win_dow+1))] + (NROW(oh_lc) > (len_gth+1))

+ [ ] Modify "close" method in function run_variance() by averaging open and close prices - no creates a bias underestimates variance  

+ [ ] Subtract mean in functions run_variance() and run_skew() - no  

+ [ ] Convert function season_ality() to use split.xts() (?)  

+ [ ] Replace caTools with TTR functions (?)  
only runquantile() is called from caTools  
but not easy to replace runquantile(), because no equivalent TTR function  
would require rewriting extreme_values() and price_jumps(), and then benchmarking  

+ [ ] Use .subset_xts() (?)

+ [ ] Rename save_rets_OHLC to rets_OHLC (?)  

+ [ ] Calculate volume-weighted moments and compare them to standard moments
add volume-weighting to vol_ohlc() and skew_ohlc()


### Commits

# commit 05-18-16
added rutils to depends in DESCRIPTION
moved do_call_rbind to rutils

# commit 05-17-16 bis
Added function season_ality()

# commit 05-17-16
Renamed functions from run_* to roll_*

# commit 11-03-15
fixed function v_wap() from Ad() to Cl()

# commit 10-03-15
added functions run_sum() and v_wap()

# commit 05-29-15
Finished R/Finance_2015 presentation

# commit 03-27-15
added timezone to argument list
updated functions: calc_rets, save_rets

# commit 03-16-15
added functions: calc_rets, save_rets, save_TAQ
updated functions: save_OHLC, scrub_TAQ
added "data" folder
updated roxygen documentation


# commit 03-01-15
added roxygen documentation

created functions:
save_OHLC
scrub_TAQ

scrub_agg:
change time zone
to trading hours
merge duplicate time stamps using make.index.unique - no
remove duplicate time stamps using duplicated
calculate mid price?
NA omit mid price
convert NA volumes to zero
replace NA trade prices with mid prices
scrub on mid price:
	bid-offer spread
	if bid-offer spread too wide then use trade price?
	hairlines


in save_OHLC
add save dir
coerce using quantmod.OHLC? - no

create function save_TAQ:
save scrubbed daily TAQ data to daily files
use quantmod standard headers - quantmod naming conventions

create function similar to getSymbols (?):
load and rbind TAQ data (without scrubbing or aggregating)


# commit 02-17-15
renamed many functions and variables
added roxygen comments
added demo folder and files
edited README
updated HighFreq-package.Rd



