% Define knitr options
% !Rnw weave=knitr
% Set global chunk options



% Define document options
\documentclass[10pt]{beamer}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0, 0, 0}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.502,0,0.502}{\textbf{#1}}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.651,0.522,0}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{1,0.502,0}{#1}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{1,0,0.502}{\textbf{#1}}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.733,0.475,0.467}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.502,0.502,0.753}{\textbf{#1}}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0,0.502,0.753}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0,0.267,0.4}{#1}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\mode<presentation>
\usetheme{AnnArbor}
% \usecolortheme{whale}
% Uncover everything in a step-wise fashion
% \beamerdefaultoverlayspecification{<+->}
% mathtools package for math symbols
\usepackage{mathtools}
% bbm package for unitary vector or matrix symbol
\usepackage{bbm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage[latin1]{inputenc}
\usepackage{hyperref}
\usepackage{fancybox}
\usepackage{url}
\usepackage[backend=bibtex,style=alphabetic]{biblatex} % bibstyle=numeric
% \bibliographystyle{amsalpha} % doesn't work
\addbibresource{FRE_lectures.bib}
% \addbibresource[location=remote]{http://www.citeulike.org/user/jerzyp}
\renewcommand\bibfont{\footnotesize}
\renewcommand{\pgfuseimage}[1]{\scalebox{0.75}{\includegraphics{#1}}} % scale bib icons
\setbeamertemplate{bibliography item}[text] % set bib icons
% \setbeamertemplate{bibliography item}{} % remove bib icons

% \usepackage{enumerate}
% \let\emph\textbf
% \let\alert\textbf
% Define colors for hyperlinks
\definecolor{links}{HTML}{2A1B81}
\hypersetup{colorlinks=true,linkcolor=,urlcolor=links}
% Make url text scriptsize
\renewcommand\UrlFont{\scriptsize}
% Make institute text italic and small
\setbeamerfont{institute}{size=\small,shape=\itshape}
\setbeamerfont{date}{size=\small}
\setbeamerfont{block title}{size=\normalsize} % shape=\itshape
\setbeamerfont{block body}{size=\footnotesize}



% Title page setup
\title[Time Series Univariate]{Time Series Univariate}
\subtitle{FRE6871 \& FRE7241, Spring 2021}
\institute[NYU Tandon]{NYU Tandon School of Engineering}
\titlegraphic{\includegraphics[scale=0.2]{image/tandon_long_color.png}}
\author[Jerzy Pawlowski]{Jerzy Pawlowski \emph{\href{mailto:jp3900@nyu.edu}{jp3900@nyu.edu}}}
% \email{jp3900@nyu.edu}
\date{\today}



%%%%%%%%%%%%%%%

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}


%%%%%%%%%%%%%%%
\maketitle


%%%%%%%%%%%%%%%
\section{Package \protect\emph{tseries} for Time Series Analysis}


%%%%%%%%%%%%%%%
\subsection{Package \protect\emph{tseries} for Time Series Analysis}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{tseries} contains functions for time series analysis and computational finance, such as:
      \begin{itemize}
        \item downloading historical data,
        \item plotting time series,
        \item calculating risk and performance measures,
        \item statistical \emph{hypothesis testing},
        \item calibrating models to time series,
        \item portfolio optimization,
      \end{itemize}
      Package \emph{tseries} accepts time series of class \texttt{"ts"} and \texttt{"zoo"}, and also has its own class \texttt{"irts"} for irregular spaced time-series objects.
      \vskip1ex
      The package \emph{zoo} is designed for managing \emph{time series} and ordered data objects.
      \vskip1ex
      The function \texttt{zoo::coredata()} extracts the underlying numeric data from a complex data object.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Get documentation for package tseries
> packageDescription("tseries")  # Get short description
> 
> help(package="tseries")  # Load help page
> 
> library(tseries)  # Load package tseries
> 
> data(package="tseries")  # List all datasets in "tseries"
> 
> ls("package:tseries")  # List all objects in "tseries"
> 
> detach("package:tseries")  # Remove tseries from search path
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting \protect\emph{OHLC} Time Series Using Package \protect\emph{tseries}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{tseries} contains functions for plotting time series:
      \begin{itemize}
        \item \texttt{seqplot.ts()} for plotting two time series in same panel.
        \item \texttt{plotOHLC()} for plotting \emph{OHLC} time series.
      \end{itemize}
      The function \texttt{plotOHLC()} from package \emph{tseries} plots \emph{OHLC} time series.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> load(file="C:/Develop/lecture_slides/data/zoo_data.RData")
> # Get start and end dates
> date_s <- time(ts_stx_adj)
> e_nd <- date_s[NROW(date_s)]
> st_art <- round((4*e_nd + date_s[1])/5)
> # Plot using plotOHLC
> plotOHLC(window(ts_stx_adj,
+           start=st_art,
+           end=e_nd)[, 1:4],
+    xlab="", ylab="")
> title(main="MSFT OHLC Prices")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/tseries_OHLC-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting Two Time Series Using \protect\emph{tseries}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{seqplot.ts()} from package \emph{tseries} plots two time series in same panel.
      \vskip1ex
      A \emph{ts} time series can be created from a \emph{zoo} time series using the function \emph{ts()}, after extracting the data and date attributes from the \emph{zoo} time series.
      \vskip1ex
      The function \texttt{decimal\_date()} from package \emph{lubridate} converts \texttt{POSIXct} objects into \texttt{numeric} \emph{year-fraction} dates.
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(lubridate)  # Load lubridate
> # Get start and end dates of ms_ft
> start_date <- decimal_date(start(ms_ft))
> end_date <- decimal_date(end(ms_ft))
> # Calculate frequency of ms_ft
> fre_quency <-
+   NROW(ms_ft)/(end_date-start_date)
> # Extract data from ms_ft
> da_ta <- zoo::coredata(
+   window(ms_ft, start=as.Date("2015-01-01"),
+    end=end(ms_ft)))
> # Create ts object using ts()
> ts_stx <- ts(data=da_ta,
+   start=decimal_date(as.Date("2015-01-01")),
+   frequency=fre_quency)
> seqplot.ts(x=ts_stx[, 1], y=ts_stx[, 4], xlab="", ylab="")
> title(main="MSFT Open and Close Prices", line=-1)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/tseries_seqplot-1}\\
      \vspace{-3em}
      The function \texttt{zoo::coredata()} extracts the underlying numeric data from a complex data object.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Risk and Performance Estimation Using \protect\emph{tseries}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{tseries} contains functions for calculating risk and performance:
      \begin{itemize}
        \item \texttt{maxdrawdown()} for calculating the maximum drawdown,
        \item \texttt{sharpe()} for calculating the \emph{Sharpe} ratio (defined as the excess return divided by the standard deviation),
        \item \texttt{sterling()} for calculating the \emph{Sterling} ratio (defined as the return divided by the maximum drawdown),
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(tseries)  # Load package tseries
> # Calculate maximum drawdown
> maxdrawdown(msft_adj[, "AdjClose"])
> max_drawd <- maxdrawdown(msft_adj[, "AdjClose"])
> index(msft_adj)[max_drawd$from]
> index(msft_adj)[max_drawd$to]
> # Calculate Sharpe ratio
> sharpe(msft_adj[, "AdjClose"])
> # Calculate Sterling ratio
> sterling(as.numeric(msft_adj[, "AdjClose"]))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Hypothesis Testing Using \protect\emph{tseries}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{tseries} contains functions for testing statistical hypothesis on time series:
      \begin{itemize}
        \item \texttt{jarque.bera.test()} \emph{Jarque-Bera} test for normality of distribution of returns,
        \item \texttt{adf.test()} \emph{Augmented Dickey-Fuller} test for existence of unit roots,
        \item \texttt{pp.test()} \emph{Phillips-Perron} test for existence of unit roots,
        \item \texttt{kpss.test()} \emph{KPSS} test for stationarity,
        \item \texttt{po.test()} \emph{Phillips-Ouliaris} test for cointegration,
        \item \texttt{bds.test()} \emph{BDS} test for randomness,
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ms_ft <- suppressWarnings(  # Load MSFT data
+   get.hist.quote(instrument="MSFT",
+            start=Sys.Date()-365,
+            end=Sys.Date(),
+            origin="1970-01-01")
+ )  # end suppressWarnings
> class(ms_ft)
> dim(ms_ft)
> tail(ms_ft, 4)
> 
> # Calculate Sharpe ratio
> sharpe(ms_ft[, "Close"], r=0.01)
> # Add title
> plot(ms_ft[, "Close"], xlab="", ylab="")
> title(main="MSFT Close Prices", line=-1)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Calibrating Time Series Models Using \protect\emph{tseries}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{tseries} contains functions for calibrating models to time series:
      \begin{itemize}
        \item \texttt{garch()} for calibrating \emph{GARCH} volatility models,
        \item \texttt{arma()} for calibrating \texttt{ARMA} models,
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(tseries)  # Load package tseries
> ms_ft <- suppressWarnings(  # Load MSFT data
+   get.hist.quote(instrument="MSFT",
+            start=Sys.Date()-365,
+            end=Sys.Date(),
+            origin="1970-01-01")
+ )  # end suppressWarnings
> class(ms_ft)
> dim(ms_ft)
> tail(ms_ft, 4)
> 
> # Calculate Sharpe ratio
> sharpe(ms_ft[, "Close"], r=0.01)
> # Add title
> plot(ms_ft[, "Close"], xlab="", ylab="")
> title(main="MSFT Close Prices", line=-1)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Portfolio Optimization Using \protect\emph{tseries}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{tseries} contains functions for miscellaneous functions:
      \vskip1ex
      \texttt{portfolio.optim()} for calculating mean-variance efficient portfolios.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(tseries)  # Load package tseries
> ms_ft <- suppressWarnings(  # Load MSFT data
+   get.hist.quote(instrument="MSFT",
+            start=Sys.Date()-365,
+            end=Sys.Date(),
+            origin="1970-01-01")
+ )  # end suppressWarnings
> class(ms_ft)
> dim(ms_ft)
> tail(ms_ft, 4)
> 
> # Calculate Sharpe ratio
> sharpe(ms_ft[, "Close"], r=0.01)
> # Add title
> plot(ms_ft[, "Close"], xlab="", ylab="")
> title(main="MSFT Close Prices", line=-1)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Package \protect\emph{quantmod} for Quantitative Financial Modeling}


%%%%%%%%%%%%%%%
\subsection{Package \protect\emph{quantmod} for Quantitative Financial Modeling}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{quantmod} is designed for downloading, manipulating, and visualizing \emph{OHLC} time series data.
      \vskip1ex
      \emph{quantmod} operates on time series of class \texttt{"xts"}, and provides many useful functions for building quantitative financial models:
      \begin{itemize}
        \item \texttt{getSymbols()} for downloading data from external sources (\emph{Yahoo}, \emph{FRED}, etc.),
        \item \texttt{getFinancials()} for downloading financial statements,
        \item \texttt{adjustOHLC()} for adjusting \emph{OHLC} data,
        \item \texttt{Op()}, \texttt{Cl()}, \texttt{Vo()}, etc. for extracting \emph{OHLC} data columns,
        \item \texttt{periodReturn()}, \texttt{dailyReturn()}, etc. for calculating periodic returns,
        \item \texttt{chartSeries()} for candlestick plots of \emph{OHLC} data,
        \item \texttt{addBBands()}, \texttt{addMA()}, \texttt{addVo()}, etc. for adding technical indicators (Moving Averages, Bollinger Bands) and volume data to a plot,
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Load package quantmod
> library(quantmod)
> # Get documentation for package quantmod
> # Get short description
> packageDescription("quantmod")
> # Load help page
> help(package="quantmod")
> # List all datasets in "quantmod"
> data(package="quantmod")
> # List all objects in "quantmod"
> ls("package:quantmod")
> # Remove quantmod from search path
> detach("package:quantmod")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting \protect\emph{OHLC} Time Series Using \texttt{chartSeries()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{chartSeries()} from package \emph{quantmod} can produce a variety of plots for \emph{OHLC} time series, including candlestick plots, bar plots, and line plots.
      \vskip1ex
      The argument \texttt{"type"} determines the type of plot (candlesticks, bars, or lines).
      \vskip1ex
      The argument \texttt{"theme"} accepts a \texttt{"chart.theme"} object, containing parameters that determine the plot appearance (colors, size, fonts).
      \vskip1ex
      \texttt{chartSeries()} automatically plots the volume data in a separate panel.
      \vskip1ex
      \emph{Candlestick} plots are designed to visualize \emph{OHLC} time series.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot OHLC candlechart with volume
> chartSeries(etf_env$VTI["2014-11"],
+       name="VTI",
+       theme=chartTheme("white"))
> # Plot OHLC bar chart with volume
> chartSeries(etf_env$VTI["2014-11"],
+       type="bars",
+       name="VTI",
+       theme=chartTheme("white"))
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/chartSeries_basic-1}\\
      Each \emph{candlestick} displays one period of data, and consists of a box representing the \emph{Open} and \emph{Close} prices, and a vertical line representing the \emph{High} and \emph{Low} prices.
      \vskip1ex
      The color of the box signifies whether the \emph{Close} price was higher or lower than the \emph{Open},
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Redrawing Plots Using \texttt{reChart()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{reChart()} redraws plots using the same data set, but using additional parameters that control the plot appearance.
      \vskip1ex
      The argument \texttt{"subset"} allows subsetting the data to a smaller range of dates.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot OHLC candlechart with volume
> chartSeries(etf_env$VTI["2008-11/2009-04"], name="VTI")
> # Redraw plot only for Feb-2009, with white theme
> reChart(subset="2009-02", theme=chartTheme("white"))
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/chartSeries_black.png}\\
      \includegraphics[width=0.45\paperwidth]{figure/chartSeries_white.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting Technical Indicators Using \texttt{chartSeries()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The argument \texttt{"TA"} allows adding technical indicators to the plot.
      \vskip1ex
      The technical indicators are functions provided by the package \emph{TTR}.
      \vskip1ex
      The function \texttt{newTA()} allows defining new technical indicators.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Candlechart with Bollinger Bands
> chartSeries(etf_env$VTI["2014"],
+       TA="addBBands(): addBBands(draw='percent'): addVo()",
+       name="VTI with Bollinger Bands",
+       theme=chartTheme("white"))
> # Candlechart with two Moving Averages
> chartSeries(etf_env$VTI["2014"],
+       TA="addVo(): addEMA(10): addEMA(30)",
+       name="VTI with Moving Averages",
+       theme=chartTheme("white"))
> # Candlechart with Commodity Channel Index
> chartSeries(etf_env$VTI["2014"],
+       TA="addVo(): addBBands(): addCCI()",
+       name="VTI with Technical Indicators",
+       theme=chartTheme("white"))
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/chartSeries_TA-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Adding Indicators and Lines Using \texttt{addTA()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{addTA()} adds indicators and lines to plots, and allows plotting lines representing a single vector of data.
      \vskip1ex
      The \texttt{addTA()} function argument \texttt{"on"} determines on which plot panel (subplot) the indicator is drawn.
      \vskip1ex
      \texttt{"on=NA"} is the default, and draws in a new plot panel below the existing plot.
      \vskip1ex
      \texttt{"on=1"} draws in the foreground of the main plot panel, and \texttt{"on=-1"} draws in the background.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> oh_lc <- rutils::etf_env$VTI["2009-02/2009-03"]
> VTI_close <- quantmod::Cl(oh_lc)
> VTI_vol <- quantmod::Vo(oh_lc)
> # Calculate volume-weighted average price
> v_wap <- TTR::VWAP(price=VTI_close, volume=VTI_vol, n=10)
> # Plot OHLC candlechart with volume
> chartSeries(oh_lc, name="VTI plus VWAP", theme=chartTheme("white"))
> # Add VWAP to main plot
> addTA(ta=v_wap, on=1, col='red')
> # Add price minus VWAP in extra panel
> addTA(ta=(VTI_close-v_wap), col='red')
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/chartSeries_addTA-3}\\
      The function \texttt{VWAP()} from package \emph{TTR} calculates the Volume Weighted Average Price as the average of past prices multiplied by their trading volumes, divided by the total volume.
      \vskip1ex
      The argument \texttt{"n"} represents the number of look-back intervals used for averaging,
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Shading Plots Using \texttt{addTA()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{addTA()} accepts Boolean vectors for shading of plots.
      \vskip1ex
      The function \texttt{addLines()} draws vertical or horizontal lines in plots.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot OHLC candlechart with volume
> chartSeries(oh_lc, name="VTI plus VWAP shaded",
+       theme=chartTheme("white"))
> # Add VWAP to main plot
> addTA(ta=v_wap, on=1, col='red')
> # Add price minus VWAP in extra panel
> addTA(ta=(VTI_close-v_wap), col='red')
> # Add background shading of areas
> addTA((VTI_close-v_wap) > 0, on=-1,
+ col="lightgreen", border="lightgreen")
> addTA((VTI_adj-v_wap) < 0, on=-1,
+ col="lightgrey", border="lightgrey")
> # Add vertical and horizontal lines at v_wap minimum
> addLines(v=which.min(v_wap), col='red')
> addLines(h=min(v_wap), col='red')
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/chartSeries_addTA_shade-7}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting Time Series Using \texttt{chart\_Series()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{chart\_Series()} from package \emph{quantmod} is an improved version of \texttt{chartSeries()}, with better aesthetics.
      \vskip1ex
      \texttt{chart\_Series()} plots are compatible with the base \texttt{graphics} package in \texttt{R}, so that standard plotting functions can be used in conjunction with \texttt{chart\_Series()}.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # OHLC candlechart VWAP in main plot,
> chart_Series(x=oh_lc, # Volume in extra panel
+        TA="add_Vo(); add_TA(v_wap, on=1)",
+        name="VTI plus VWAP shaded")
> # Add price minus VWAP in extra panel
> add_TA(VTI_adj-v_wap, col='red')
> # Add background shading of areas
> add_TA((VTI_adj-v_wap) > 0, on=-1,
+ col="lightgreen", border="lightgreen")
> add_TA((VTI_adj-v_wap) < 0, on=-1,
+ col="lightgrey", border="lightgrey")
> # Add vertical and horizontal lines
> abline(v=which.min(v_wap), col='red')
> abline(h=min(v_wap), col='red')
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/chart_Series_shaded}\\
      \texttt{chart\_Series()} also has its own functions for adding indicators: \texttt{add\_TA()}, \texttt{add\_BBands()}, etc.
      \vskip1ex
      Note that functions associated with \texttt{chart\_Series()} contain an underscore in their name,
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plot and Theme Objects of \texttt{chart\_Series()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{chart\_Series()} creates a \emph{plot object} and returns it \emph{invisibly}.
      \vskip1ex
      A \emph{plot object} is an environment of class \emph{replot}, containing parameters specifying a plot.
      \vskip1ex
      A plot can be rendered by calling, plotting, or printing the \emph{plot object}.
      \vskip1ex
      A plot \emph{theme object} is a \texttt{list} containing parameters that determine the plot appearance (colors, size, fonts).
      \vskip1ex
      The function \texttt{chart\_theme()} returns the \emph{theme object}.
      \vskip1ex
      \texttt{chart\_Series()} plots can be modified by modifying \emph{plot objects} or \emph{theme objects}.
      \vskip1ex
      Plot and theme objects can be modified directly, or by using accessor and setter functions.
      \vskip1ex
      The parameter \texttt{"plot=FALSE"} suppresses plotting and allows modifying \emph{plot objects}.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Extract plot object
> ch_ob <- chart_Series(x=oh_lc, plot=FALSE)
> class(ch_ob)
> ls(ch_ob)
> class(ch_ob$get_ylim)
> class(ch_ob$set_ylim)
> # ls(ch_ob$Env)
> class(ch_ob$Env$actions)
> plot_theme <- chart_theme()
> class(plot_theme)
> ls(plot_theme)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Customizing \texttt{chart\_Series()} Plots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{chart\_Series()} plots can be customized by modifying the plot and theme objects.
      \vskip1ex
      Plot and theme objects can be modified directly, or by using accessor and setter functions.
      \vskip1ex
      A plot is rendered by calling, plotting, or printing the plot object.
      \vskip1ex
      The parameter \texttt{"plot=FALSE"} suppresses plotting and allows modifying \emph{plot objects}.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> oh_lc <- rutils::etf_env$VTI["2010-04/2010-05"]
> # Extract, modify theme, format tick marks "%b %d"
> plot_theme <- chart_theme()
> plot_theme$format.labels <- "%b %d"
> # Create plot object
> ch_ob <- chart_Series(x=oh_lc, theme=plot_theme, plot=FALSE)
> # Extract ylim using accessor function
> y_lim <- ch_ob$get_ylim()
> y_lim[[2]] <- structure(range(quantmod::Cl(oh_lc)) + c(-1, 1),
+   fixed=TRUE)
> # Modify plot object to reduce y-axis range
> ch_ob$set_ylim(y_lim)  # use setter function
> # Render the plot
> plot(ch_ob)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/chart_Series_custom_axis-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting \texttt{chart\_Series()} in Multiple Panels}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{chart\_Series()} plots are compatible with the base \texttt{graphics} package, allowing easy plotting in multiple panels.
      \vskip1ex
      The parameter \texttt{"plot=FALSE"} suppresses plotting and allows adding extra plot elements.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate VTI and XLF volume-weighted average price
> v_wap <- TTR::VWAP(price=quantmod::Cl(rutils::etf_env$VTI),
+       volume=quantmod::Vo(rutils::etf_env$VTI), n=10)
> XLF_vwap <- TTR::VWAP(price=quantmod::Cl(rutils::etf_env$XLF),
+       volume=quantmod::Vo(rutils::etf_env$XLF), n=10)
> # Open graphics device, and define
> # Plot area with two horizontal panels
> x11(); par(mfrow=c(2, 1))
> ch_ob <- chart_Series(  # Plot in top panel
+   x=etf_env$VTI["2009-02/2009-04"],
+   name="VTI", plot=FALSE)
> add_TA(v_wap["2009-02/2009-04"], lwd=2, on=1, col='blue')
> # Plot in bottom panel
> ch_ob <- chart_Series(x=etf_env$XLF["2009-02/2009-04"],
+   name="XLF", plot=FALSE)
> add_TA(XLF_vwap["2009-02/2009-04"], lwd=2, on=1, col='blue')
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/chart_Series_panels.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Plotting \protect\emph{OHLC} Time Series Using Package \protect\emph{dygraphs}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{dygraph()} from package \emph{dygraphs} creates interactive plots for \emph{xts} time series.
      \vskip1ex
      The function \texttt{dyCandlestick()} creates a \emph{candlestick} plot object for \emph{OHLC} data, and uses the first four columns to plot \emph{candlesticks}, and it plots any additional columns as lines.
      \vskip1ex
      The function \texttt{dyOptions()} adds options (like colors, etc.) to a \emph{dygraph} plot.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(dygraphs)
> # Calculate volume-weighted average price
> oh_lc <- rutils::etf_env$VTI
> v_wap <- TTR::VWAP(price=quantmod::Cl(oh_lc),
+     volume=quantmod::Vo(oh_lc), n=20)
> # Add VWAP to OHLC data
> da_ta <- cbind(oh_lc[, 1:4], v_wap)["2009-01/2009-04"]
> # Create dygraphs object
> dy_graph <- dygraphs::dygraph(da_ta)
> # Increase line width and color
> dy_graph <- dygraphs::dyOptions(dy_graph, 
+   colors="red", strokeWidth=3)
> # Convert dygraphs object to candlestick plot
> dy_graph <- dygraphs::dyCandlestick(dy_graph)
> # Render candlestick plot
> dy_graph
> # Candlestick plot using pipes syntax
> dygraphs::dygraph(da_ta) %>% dyCandlestick() %>% 
+   dyOptions(colors="red", strokeWidth=3)
> # Candlestick plot without using pipes syntax
> dygraphs::dyCandlestick(dygraphs::dyOptions(dygraphs::dygraph(da_ta), 
+   colors="red", strokeWidth=3))
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth, height=0.35\paperwidth]{figure/dygraphs_candlestick.png}
      Each \emph{candlestick} displays one period of data, and consists of a box representing the \emph{Open} and \emph{Close} prices, and a vertical line representing the \emph{High} and \emph{Low} prices.
      \vskip1ex
      The color of the box signifies whether the \emph{Close} price was higher or lower than the \emph{Open},
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{dygraphs} \protect\emph{OHLC} Plots With Background Shading}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{dyShading()} adds shading to a \emph{dygraphs} plot object.
      \vskip1ex
      The function \texttt{dyShading()} requires a vector of dates for shading.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Create candlestick plot with background shading
> in_dic <- (Cl(da_ta) > da_ta[, "VWAP"])
> whi_ch <- which(rutils::diff_it(in_dic) != 0)
> in_dic <- rbind(first(in_dic), in_dic[whi_ch, ], last(in_dic))
> date_s <- index(in_dic)
> in_dic <- ifelse(drop(coredata(in_dic)), "lightgreen", "antiquewhite")
> # Create dygraph object without rendering it
> dy_graph <- dygraphs::dygraph(da_ta) %>% dyCandlestick() %>% 
+   dyOptions(colors="red", strokeWidth=3)
> # Add shading
> for (i in 1:(NROW(in_dic)-1)) {
+     dy_graph <- dy_graph %>% 
+ dyShading(from=date_s[i], to=date_s[i+1], color=in_dic[i])
+ }  # end for
> # Render the dygraph object
> dy_graph
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth, height=0.35\paperwidth]{figure/dygraphs_candlestick_shaded.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{dygraphs} Plots With Two \texttt{"y"} Axes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{dyAxis()} from package \emph{dygraphs} plots customized axes to a \emph{dygraphs} plot object.
      \vskip1ex
      The function \texttt{dySeries()} adds a time series to a \emph{dygraphs} plot object.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(dygraphs)
> # Prepare VTI and IEF prices
> price_s <- cbind(Cl(rutils::etf_env$VTI), Cl(rutils::etf_env$IEF))
> price_s <- na.omit(price_s)
> col_names <- rutils::get_name(colnames(price_s))
> colnames(price_s) <- col_names
> # dygraphs plot with two y-axes
> library(dygraphs)
> dygraphs::dygraph(price_s, main=paste(col_names, collapse=" and ")) %>%
+   dyAxis(name="y", label=col_names[1], independentTicks=TRUE) %>%
+   dyAxis(name="y2", label=col_names[2], independentTicks=TRUE) %>%
+   dySeries(name=col_names[1], axis="y", strokeWidth=2, col="red") %>%
+   dySeries(name=col_names[2], axis="y2", strokeWidth=2, col="blue")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth, height=0.35\paperwidth]{figure/dygraphs_2yaxis.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{\protect\emph{zoo} Plots With Two \texttt{"y"} Axes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{plot.zoo()} plots time series.
      \vskip1ex
      The function \texttt{axis()} plots customized axes in an existing plot.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Open plot window and set plot margins
> x11(width=6, height=5)
> par(mar=c(2, 2, 2, 2), oma=c(1, 1, 1, 1))
> # Plot first time series without x-axis
> zoo::plot.zoo(price_s[, 1], lwd=2, col="orange",
+         xlab=NA, ylab=NA, xaxt="n")
> # Create X-axis date labels and add X-axis
> date_s <- pretty(index(price_s))
> axis(side=1, at=date_s, labels=format(date_s, "%b-%d-%y"))
> # Plot second time series without y-axis
> par(new=TRUE)  # Allow new line on same plot
> zoo::plot.zoo(price_s[, 2], xlab=NA, ylab=NA,
+         lwd=2, yaxt="n", col="blue", xaxt="n")
> # Plot second y-axis on right
> axis(side=4, lwd=2, col="blue")
> # Add axis labels
> mtext(col_names[1], cex=1.2, lwd=3, side=2, las=2, adj=(-0.5), padj=(-5), col="orange")
> mtext(col_names[2], cex=1.2, lwd=3, side=4, las=2, adj=1.5, padj=(-5), col="blue")
> # Add title and legend
> title(main=paste(col_names, collapse=" and "), line=0.5)
> legend("top", legend=col_names, cex=1.0, bg="white",
+  lty=1, lwd=6, col=c("orange", "blue"), bty="n")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth, height=0.35\paperwidth]{figure/zoo_2yaxis.png}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Package \protect\emph{qmao} for Quantitative Financial Modeling}


%%%%%%%%%%%%%%%
\subsection{draft: Package \protect\emph{qmao} for Quantitative Financial Modeling}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{qmao} is designed for downloading, manipulating, and visualizing \emph{OHLC} time series data,
package \emph{quantmod}
      \vskip1ex
      \emph{qmao} uses time series of class \texttt{"xts"}, and provides many useful functions for building quantitative financial models:
      \begin{itemize}
        \item \texttt{getSymbols()} for downloading data from external sources (\emph{Yahoo}, \emph{FRED}, etc.),
        \item \texttt{getFinancials()} for downloading financial statements,
        \item \texttt{adjustOHLC()} for adjusting \emph{OHLC} data,
        \item \texttt{Op()}, \texttt{Cl()}, \texttt{Vo()}, etc. for extracting \emph{OHLC} data columns,
        \item \texttt{periodReturn()}, \texttt{dailyReturn()}, etc. for calculating periodic returns,
        \item \texttt{chartSeries()} for candlestick plots of \emph{OHLC} data,
        \item \texttt{addBBands()}, \texttt{addMA()}, \texttt{addVo()}, etc. for adding technical indicators (Moving Averages, Bollinger Bands) and volume data to a plot,
      \end{itemize}
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Load package qmao
> library(qmao)
> # Get documentation for package qmao
> # Get short description
> packageDescription("qmao")
> # Load help page
> help(package="qmao")
> # List all datasets in "qmao"
> data(package="qmao")
> # List all objects in "qmao"
> ls("package:qmao")
> # Remove qmao from search path
> detach("package:qmao")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Time Series of Asset Prices}


%%%%%%%%%%%%%%%
\subsection{Monte Carlo Simulation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Monte Carlo} simulation consists of generating random samples from a given probability distribution.
      \vskip1ex
      The \emph{Monte Carlo} data samples can then used to calculate different parameters of the probability distribution (moments, quantiles, etc.), and its functionals.
      \vskip1ex
      The \emph{quantile} of a probability distribution is the value of the \emph{random variable} \texttt{x}, such that the probability of values less than \texttt{x} is equal to the given \emph{probability} $p$.
      \vskip1ex
      The \emph{quantile} of a data sample can be calculated by first sorting the sample, and then finding the value corresponding closest to the given \emph{probability} $p$.
      \vskip1ex
	  The function \texttt{quantile()} calculates the sample quantiles.  It uses interpolation to improve the accuracy.  Information about the different interpolation methods can be found by typing \texttt{?quantile}.
     \vskip1ex
      The function \texttt{sort()} returns a vector sorted into ascending order.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> set.seed(1121)  # Reset random number generator
> # Sample from Standard Normal Distribution
> n_rows <- 1000
> da_ta <- rnorm(n_rows)
> # Sample mean - MC estimate
> mean(da_ta)
> # Sample standard deviation - MC estimate
> sd(da_ta)
> # Monte Carlo estimate of cumulative probability
> pnorm(1)
> sum(da_ta < 1)/n_rows
> # Monte Carlo estimate of quantile
> conf_level <- 0.99
> qnorm(conf_level)
> cut_off <- conf_level*n_rows
> da_ta <- sort(da_ta)
> da_ta[cut_off]
> quantile(da_ta, probs=conf_level)
> # Analyze the source code of quantile()
> stats:::quantile.default
> # Microbenchmark quantile
> library(microbenchmark)
> summary(microbenchmark(
+   monte_carlo=da_ta[cut_off],
+   quan_tile=quantile(da_ta, probs=conf_level),
+   times=100))[, c(1, 4, 5)]  # end microbenchmark summary
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Brownian Motion Using \texttt{while()} Loops}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \texttt{while()} loops are often used in simulations, when the number of required loops is unknown in advance.
      \vskip1ex
      Below is an example of a simulation of the path of \emph{Brownian Motion} crossing a barrier level.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> set.seed(1121)  # Reset random number generator
> bar_rier <- 20  # Barrier level
> n_rows <- 1000  # Number of simulation steps
> pa_th <- numeric(n_rows)  # Allocate path vector
> pa_th[1] <- 0  # Initialize path
> in_dex <- 2  # Initialize simulation index
> while ((in_dex <= n_rows) && (pa_th[in_dex - 1] < bar_rier)) {
+ # Simulate next step
+   pa_th[in_dex] <- pa_th[in_dex - 1] + rnorm(1)
+   in_dex <- in_dex + 1  # Advance in_dex
+ }  # end while
> # Fill remaining pa_th after it crosses bar_rier
> if (in_dex <= n_rows)
+   pa_th[in_dex:n_rows] <- pa_th[in_dex - 1]
> # Plot the Brownian motion
> x11(width=6, height=5)
> par(mar=c(3, 3, 2, 1), oma=c(1, 1, 1, 1))
> plot(pa_th, type="l", col="black",
+      lty="solid", lwd=2, xlab="", ylab="")
> abline(h=bar_rier, lwd=3, col="red")
> title(main="Brownian Motion Crossing a Barrier Level", line=0.5)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/simu_brown_barrier.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Brownian Motion Using Vectorized Functions}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Simulations in \texttt{R} can be accelerated by pre-computing a vector of random numbers, instead of generatng them one at a time in a loop.
      \vskip1ex
      Vectors of random numbers allow using \emph{vectorized} functions, instead of inefficient (slow) \texttt{while()} loops.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> set.seed(1121)  # Reset random number generator
> bar_rier <- 20  # Barrier level
> n_rows <- 1000  # Number of simulation steps
> # Simulate path of Brownian motion
> pa_th <- cumsum(rnorm(n_rows))
> # Find index when pa_th crosses bar_rier
> cro_ss <- which(pa_th > bar_rier)
> # Fill remaining pa_th after it crosses bar_rier
> if (NROW(cro_ss)>0) {
+   pa_th[(cro_ss[1]+1):n_rows] <- pa_th[cro_ss[1]]
+ }  # end if
> # Plot the Brownian motion
> x11(width=6, height=5)
> par(mar=c(3, 3, 2, 1), oma=c(1, 1, 1, 1))
> plot(pa_th, type="l", col="black",
+      lty="solid", lwd=2, xlab="", ylab="")
> abline(h=bar_rier, lwd=3, col="red")
> title(main="Brownian Motion Crossing a Barrier Level", line=0.5)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/simu_brown_barrier.png}
      The tradeoff between speed and memory usage: more memory may be used than necessary, since the simulation may stop before all the pre-computed random numbers are used up.
      \vskip1ex
      But the simulation is much faster because the path is simulated using \emph{vectorized} functions,
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Geometric Brownian Motion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If the percentage asset returns $r_t \mathrm{d} t = \mathrm{d} \log{p_t}$ follow \emph{Brownian motion}:
      \begin{displaymath}
        r_t \mathrm{d} t = \mathrm{d} \log{p_t} = ( \mu - \frac{\sigma^2}{2} ) \mathrm{d}t + \sigma \, \mathrm{d} W_t
      \end{displaymath}
      Then asset prices $p_t$ follow \emph{Geometric Brownian motion} (GBM):
      \begin{displaymath}
        \mathrm{d} p_t = \mu p_t \mathrm{d}t + \sigma \, p_t \mathrm{d} W_t
      \end{displaymath}
      Where $\sigma$ is the volatility of asset returns, and $W_t$ is a \emph{Brownian Motion}, with $\mathrm{d} W_t$ following the standard normal distribution $\phi(0, \sqrt{\mathrm{d}t})$.
      \vskip1ex
      The solution of \emph{Geometric Brownian motion} is equal to:
      \begin{displaymath}
        p_t = p_0 \exp[( \mu - \frac{\sigma^2}{2} ) t + \sigma \, W_t]
      \end{displaymath}
      The convexity correction: $-\frac{\sigma^2}{2}$ ensures that the growth rate of prices is equal to $\mu$, (according to Ito's lemma).
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/brown_geom.png}
      \vspace{-2em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Define daily volatility and growth rate
> sig_ma <- 0.01; dri_ft <- 0.0; n_rows <- 1000
> # Simulate geometric Brownian motion
> re_turns <- sig_ma*rnorm(n_rows) + dri_ft - sig_ma^2/2
> price_s <- exp(cumsum(re_turns))
> plot(price_s, type="l", xlab="time", ylab="prices",
+      main="geometric Brownian motion")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Random \protect\emph{OHLC} Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Random \emph{OHLC} prices are useful for testing financial models.
      \vskip1ex
      The function \texttt{sample()} selects a random sample from a vector of data elements.
      \vskip1ex
      The function \texttt{sample()} with \texttt{replace=TRUE} selects samples with replacement (the default is \texttt{replace=FALSE}).
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Simulate geometric Brownian motion
> sig_ma <- 0.01/sqrt(48)
> dri_ft <- 0.0
> n_rows <- 1e4
> date_s <- seq(from=as.POSIXct(paste(Sys.Date()-250, "09:30:00")),
+   length.out=n_rows, by="30 min")
> price_s <- exp(cumsum(sig_ma*rnorm(n_rows) + dri_ft - sig_ma^2/2))
> price_s <- xts(price_s, order.by=date_s)
> price_s <- cbind(price_s,
+   volume=sample(x=10*(2:18), size=n_rows, replace=TRUE))
> # Aggregate to daily OHLC data
> oh_lc <- xts::to.daily(price_s)
> quantmod::chart_Series(oh_lc, name="random prices")
> # dygraphs candlestick plot using pipes syntax
> library(dygraphs)
> dygraphs::dygraph(oh_lc[, 1:4]) %>% dyCandlestick()
> # dygraphs candlestick plot without using pipes syntax
> dygraphs::dyCandlestick(dygraphs::dygraph(oh_lc[, 1:4]))
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/random_ohlc.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The \protect\emph{Log-normal} Probability Distribution}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If \texttt{x} follows the \emph{Normal} distribution $\phi(x, \mu, \sigma)$, then the exponential of \texttt{x}: $y = e^x$ follows the \emph{Log-normal} distribution $\log\phi()$:
      \begin{displaymath}
        \log\phi(y, \mu, \sigma) = \frac{\exp(-(\log{y} - \mu)^2/2 \sigma^2)}{y \sigma \, \sqrt{2 \pi}}
      \end{displaymath}
      With mean equal to: $\bar{y} = \mathbb{E}[y] = \exp(\mu + \sigma^2/2)$, and median equal to: $\tilde{y} = \exp(\mu)$
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Standard deviations of log-normal distribution
> sig_mas <- c(0.5, 1, 1.5)
> # Create plot colors
> col_ors <- c("black", "red", "blue")
> # Plot all curves
> for (in_dex in 1:NROW(sig_mas)) {
+   curve(expr=dlnorm(x, sdlog=sig_mas[in_dex]),
+   type="l", lwd=2, xlim=c(0, 3),
+   xlab="", ylab="", col=col_ors[in_dex],
+   add=as.logical(in_dex-1))
+ }  # end for
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/log_norm_dist.png}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Add title and legend
> title(main="Log-normal Distributions", line=0.5)
> legend("topright", inset=0.05, title="Sigmas",
+  paste("sigma", sig_mas, sep="="),
+  cex=0.8, lwd=2, lty=rep(1, NROW(sig_mas)),
+  col=col_ors)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Standard Deviation of \protect\emph{Log-normal} Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \vskip1ex
      If percentage asset returns are \emph{normally} distributed and follow \emph{Brownian motion}, then asset prices follow \emph{Geometric Brownian motion}, and they are \emph{Log-normally} distributed at every point in time.
      \vskip1ex
      The standard deviation of \emph{log-normal} prices is equal to the return volatility $\sigma_r$ times the square root of time: $\sigma = \sigma_r \sqrt{t}$.
      \vskip1ex
      The \emph{Log-normal} distribution has a strong positive skewness (third moment) equal to: $\varsigma = \mathbb{E}[(y - \mathbb{E}[y])^3] = (e^{\sigma^2} + 2) \sqrt{e^{\sigma^2} - 1}$
      \vskip1ex
      For large standard deviation, the skewness increases exponentially with the standard deviation and with time: $\varsigma \propto e^{1.5 \sigma^2} = e^{1.5 t \sigma^2_r}$
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Return volatility of VTI etf
> sig_ma <- sd(rutils::diff_it(log(rutils::etf_env$VTI[, 4])))
> sigma2 <- sig_ma^2
> n_rows <- NROW(rutils::etf_env$VTI)
> # Standard deviation of log-normal prices
> sqrt(n_rows)*sig_ma
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/log_norm_skew.png}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Skewness of log-normal prices
> skew_ness <- function(t) {
+   ex_p <- exp(t*sigma2)
+   (ex_p + 2)*sqrt(ex_p - 1)
+ }  # end skew_ness
> curve(expr=skew_ness, xlim=c(1, n_rows), lwd=3,
+ xlab="Number of days", ylab="Skewness", col="blue",
+ main="Skewness of Log-normal Prices
+ as a Function of Time")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Mean and Median of \protect\emph{Log-normal} Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The mean of the \emph{Log-normal} distribution: $\bar{y} = \mathbb{E}[y] = \exp(\mu + \sigma^2/2)$ is greater than its median, which is equal to: $\tilde{y} = \exp(\mu)$.
      \vskip1ex
      So if stock prices follow \emph{Geometric Brownian motion} and are distributed \emph{log-normally}, then a stock selected at random will have a high probability of havng a lower price than the mean expected price.
      \vskip1ex
      The cumulative \emph{Log-normal} probability distribution is equal to $\operatorname{F}(x) = \Phi(\frac{\log{y}-\mu}{\sigma})$, where $\Phi()$ is the cumulative standard normal distribution.
      \vskip1ex
      So the probability that the price of a randomly selected stock will be lower than the mean price is equal to $\operatorname{F}(\bar{y}) = \Phi(\sigma/2)$.
      \vskip1ex
      Therefore an investor without skill, who selects stocks at random, has a high probability of underperforming the index.
      \vskip1ex
      Performing as well as the index requires \emph{significant} investment skill, while outperforming the index requires \emph{exceptional} investment skill.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/log_norm_prob.png}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Probability that random log-normal price will be lower than the mean price
> curve(expr=pnorm(sig_ma*sqrt(x)/2),
+ xlim=c(1, n_rows), lwd=3,
+ xlab="Number of days", ylab="Probability", col="blue",
+ main="Probability That Random Log-normal Price
+ Will be Lower Than the Mean Price")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Paths of Geometric Brownian Motion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The standard deviation of \emph{log-normal} prices $\sigma$ is equal to the volatility of returns $\sigma_r$ times the square root of time: $\sigma = \sigma_r \sqrt{t}$.
      \vskip1ex
      For large standard deviation, the skewness $\varsigma$ increases exponentially with the standard deviation and with time: $\varsigma \propto e^{1.5 \sigma^2} = e^{1.5 t \sigma^2_r}$
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Define daily volatility and growth rate
> sig_ma <- 0.01; dri_ft <- 0.0; n_rows <- 5000
> path_s <- 10
> # Simulate multiple paths of geometric Brownian motion
> price_s <- matrix(rnorm(path_s*n_rows, sd=sig_ma) +
+     dri_ft - sig_ma^2/2, nc=path_s)
> price_s <- exp(matrixStats::colCumsums(price_s))
> # Create xts time series
> price_s <- xts(price_s, order.by=seq.Date(Sys.Date()-NROW(price_s)+1, Sys.Date(), by=1))
> # Plot xts time series
> col_ors <- colorRampPalette(c("red", "blue"))(NCOL(price_s))
> col_ors <- col_ors[order(order(price_s[NROW(price_s), ]))]
> par(mar=c(3, 3, 2, 2), oma=c(0, 0, 0, 0))
> plot.zoo(price_s, main="Multiple paths of geometric Brownian motion",
+    xlab=NA, ylab=NA, plot.type="single", col=col_ors)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/brown_geom_paths.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Distribution of Paths of Geometric Brownian Motion}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Prices following \emph{Geometric Brownian motion} have a large positive skewness, so that the expected value of prices is skewed by a few paths with very high prices, while the prices of the majority of paths are below their expected value.
      \vskip1ex
      For large standard deviation, the skewness $\varsigma$ increases exponentially with the standard deviation and with time: $\varsigma \propto e^{1.5 \sigma^2} = e^{1.5 t \sigma^2_r}$
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Define daily volatility and growth rate
> sig_ma <- 0.01; dri_ft <- 0.0; n_rows <- 10000
> path_s <- 100
> # Simulate multiple paths of geometric Brownian motion
> price_s <- matrix(rnorm(path_s*n_rows, sd=sig_ma) +
+     dri_ft - sig_ma^2/2, nc=path_s)
> price_s <- exp(matrixStats::colCumsums(price_s))
> # Calculate percentage of paths below the expected value
> per_centage <- rowSums(price_s < 1.0) / path_s
> # Create xts time series of percentage of paths below the expected value
> per_centage <- xts(per_centage, order.by=seq.Date(Sys.Date()-NROW(per_centage)+1, Sys.Date(), by=1))
> # Plot xts time series of percentage of paths below the expected value
> par(mar=c(3, 3, 2, 2), oma=c(0, 0, 0, 0))
> plot.zoo(per_centage, main="Percentage of GBM paths below mean",
+    xlab=NA, ylab=NA, col="blue")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/brown_geom_percent.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Time Evolution of Stock Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Stock prices evolve over time similar to \emph{Geometric Brownian motion}, and they also exhibit a very skewed distribution of prices.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Load S&P500 stock prices
> load("C:/Develop/lecture_slides/data/sp500.RData")
> ls(sp500_env)
> # Extract closing prices
> price_s <- eapply(sp500_env, quantmod::Cl)
> # Flatten price_s into a single xts series
> price_s <- rutils::do_call(cbind, price_s)
> # Carry forward and backward non-NA prices
> price_s <- zoo::na.locf(price_s, na.rm=FALSE)
> price_s <- zoo::na.locf(price_s, fromLast=TRUE)
> sum(is.na(price_s))
> # Drop ".Close" from column names
> colnames(price_s[, 1:4])
> colnames(price_s) <- rutils::get_name(colnames(price_s))
> # Or
> # colnames(price_s) <- do.call(rbind,
> #   strsplit(colnames(price_s), split="[.]"))[, 1]
> # Normalize columns
> price_s <- xts(t(t(price_s) / as.numeric(price_s[1, ])),
+          order.by=index(price_s))
> # Calculate permution index for sorting the lowest to highest final price_s
> or_der <- order(price_s[NROW(price_s), ])
> # Select a few symbols
> sym_bols <- colnames(price_s)[or_der]
> sym_bols <- sym_bols[seq.int(from=1, to=(NROW(sym_bols)-1), length.out=20)]
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/stock_index_paths.png}
    \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot xts time series of price_s
> col_ors <- colorRampPalette(c("red", "blue"))(NROW(sym_bols))
> col_ors <- col_ors[order(order(price_s[NROW(price_s), sym_bols]))]
> par(mar=c(3, 3, 2, 2), oma=c(0, 0, 0, 0))
> plot.zoo(price_s[, sym_bols], main="20 S&P500 stock prices (normalized)",
+    xlab=NA, ylab=NA, plot.type="single", col=col_ors)
> legend(x="topleft", inset=0.05, cex=0.8,
+  legend=rev(sym_bols), col=rev(col_ors), lwd=6, lty=1)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Distribution of Stock Prices}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Usually, a small number of stocks in an index reach very high prices, while the prices of the majority of stocks remain below the index price (the average price of the index portfolio).
      \vskip1ex
      For example, the current prices of almost \texttt{80\%} of the S\&P500 constituent stocks from \texttt{1990} are now below the average price of that portfolio.
      \vskip1ex
      Therefore an investor without skill, who selects stocks at random, has a high probability of underperforming the index, because they will most likely miss selecting the best performing stocks.
      \vskip1ex
      Performing as well as the index requires \emph{significant} investment skill, while outperforming the index requires \emph{exceptional} investment skill.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate average of valid stock prices
> val_id <- (price_s != 1)  # Valid stocks
> n_stocks <- rowSums(val_id)
> n_stocks[1] <- NCOL(price_s)
> in_dex <- rowSums(price_s * val_id) / n_stocks
> # Calculate percentage of stock prices below the average price
> per_centage <- rowSums((price_s < in_dex) & val_id) / n_stocks
> # Create xts time series of average stock prices
> in_dex <- xts(in_dex, order.by=index(price_s))
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
    % \vspace{-1em}
    %   \includegraphics[width=0.45\paperwidth]{figure/stock_index_prices.png}
    % \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/stock_index_prices_percent.png}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot xts time series of average stock prices
> plot.zoo(in_dex, main="Average S&P500 stock prices (normalized from 1990)",
+    xlab=NA, ylab=NA, col="blue")
> # Create xts time series of percentage of stock prices below the average price
> per_centage <- xts(per_centage, order.by=index(price_s))
> # Plot percentage of stock prices below the average price
> plot.zoo(per_centage[-(1:2),],
+    main="Percentage of S&P500 stock prices below the average price",
+    xlab=NA, ylab=NA, col="blue")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Time Series Modeling}


%%%%%%%%%%%%%%%
\subsection{The Brownian Motion Process}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the \emph{Brownian Motion} process, the returns $r_i$ are equal to the random \emph{innovations}:
      \begin{align*}
        r_i &= p_i - p_{i-1} = \sigma \, \xi_i \\
        p_i &= p_{i-1} + r_i
      \end{align*}
      Where $\sigma$ is the volatility of returns, and $\xi_i$ are random normal \emph{innovations} with zero mean and unit variance.
      \vskip1ex
      The \emph{Brownian Motion} process for prices can be written as an \emph{AR(1)} autoregressive process with coefficient $\varphi = 1$:
      \begin{displaymath}
        p_i = \varphi p_{i-1} + \sigma \, \xi_i
      \end{displaymath}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/brown_paths.png}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Define Brownian Motion parameters
> n_rows <- 1000; sig_ma <- 0.01
> # Simulate 5 paths of Brownian motion
> price_s <- matrix(rnorm(5*n_rows, sd=sig_ma), nc=5)
> price_s <- matrixStats::colCumsums(price_s)
> # Open plot window on Mac
> dev.new(width=6, height=4, noRStudioGD=TRUE)
> # Set plot parameters to reduce whitespace around plot
> par(mar=c(2, 2, 3, 1), oma=c(0, 0, 0, 0))
> # Plot 5 paths of Brownian motion
> matplot(y=price_s, main="Brownian Motion Paths",
+   xlab="", ylab="", type="l", lty="solid", lwd=1, col="blue")
> # Save plot to png file on Mac
> quartz.save("/Users/jerzy/Develop/lecture_slides/figure/brown_paths.png", type="png", device=dev.cur(), width=6, height=5)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Ornstein-Uhlenbeck Process}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In the \emph{Ornstein-Uhlenbeck} process, the returns $r_i$ are equal to the difference between the equilibrium price $\mu$ minus the latest price $p_{i-1}$, times the mean reversion parameter $\theta$, plus random \emph{innovations}:
      \begin{align*}
        r_i &= p_i - p_{i-1} = \theta \, (\mu - p_{i-1}) + \sigma \, \xi_i \\
        p_i &= p_{i-1} + r_i
      \end{align*}
      Where $\sigma$ is the volatility of returns, and $\xi_i$ are random normal \emph{innovations} with zero mean and unit variance.
      \vskip1ex
      The \emph{Ornstein-Uhlenbeck} process for prices can be written as an \emph{AR(1)} process plus a drift:
      \begin{displaymath}
        p_i = \theta \, \mu + (1 - \theta ) \, p_{i-1} + \sigma \, \xi_i
      \end{displaymath}
      The \emph{Ornstein-Uhlenbeck} process cannot be simulated using the function \texttt{filter()} because of the drift term, so it must be simulated using explicit loops, either in \texttt{R} or in \texttt{C++}.
      \vskip1ex
      The compiled \emph{Rcpp} \texttt{C++} code can be over \texttt{100} times faster than loops in \texttt{R}!
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Define Ornstein-Uhlenbeck parameters
> eq_price <- 1.0; sig_ma <- 0.02
> the_ta <- 0.01; n_rows <- 1000
> # Initialize the data
> in_nov <- rnorm(n_rows)
> re_turns <- numeric(n_rows)
> price_s <- numeric(n_rows)
> # Simulate Ornstein-Uhlenbeck process in R
> price_s[1] <- sig_ma*in_nov[1]
> for (i in 2:n_rows) {
+   re_turns[i] <- the_ta*(eq_price - price_s[i-1]) + 
+     sig_ma*in_nov[i]
+   price_s[i] <- price_s[i-1] + re_turns[i]
+ }  # end for
> # Simulate Ornstein-Uhlenbeck process in Rcpp
> prices_cpp <- HighFreq::sim_ou(eq_price=eq_price, volat=sig_ma, theta=the_ta, innov=matrix(in_nov))
> all.equal(price_s, drop(prices_cpp))
> # Compare the speed of R code with Rcpp
> library(microbenchmark)
> summary(microbenchmark(
+   Rcode={for (i in 2:n_rows) {
+     re_turns[i] <- the_ta*(eq_price - price_s[i-1]) + sig_ma*in_nov[i]
+     price_s[i] <- price_s[i-1] + re_turns[i]}},
+   Rcpp=HighFreq::sim_ou(eq_price=eq_price, volat=sig_ma, theta=the_ta, innov=matrix(in_nov)),
+   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Solution of the Ornstein-Uhlenbeck Process}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Ornstein-Uhlenbeck} process in continuous time is:
      \begin{displaymath}
        \mathrm{d} p_t = \theta \, (\mu - p_t) \, \mathrm{d} t + \sigma \, \mathrm{d} W_t
      \end{displaymath}
      Where $W_t$ is a \emph{Brownian Motion}, with $\mathrm{d} W_t$ following the standard normal distribution $\phi(0, \sqrt{\mathrm{d}t})$.
      \vskip1ex
      The solution of the \emph{Ornstein-Uhlenbeck} process is given by:
      \begin{displaymath}
        p_t = p_0 e^{-\theta t} + \mu (1 - e^{-\theta t}) + \sigma \int_{0}^{t} {e^{\theta (s - t)} \mathrm{d} W_s}
      \end{displaymath}
      \vskip1ex
      The mean and variance are given by:\\
      $\mathbb{E}[p_t] = p_0 e^{-\theta t} + \mu (1 - e^{-\theta t}) \rightarrow \mu$ \\
      $\mathbb{E}[(p_t - \mathbb{E}[p_t])^2] = \frac{\sigma^2}{2 \theta} (1 - e^{-\theta t}) \rightarrow \frac{\sigma^2}{2 \theta}$
      \vskip1ex
      The \emph{Ornstein-Uhlenbeck} process is mean reverting to a non-zero equilibrium price $\mu$.
      \vskip1ex
      The \emph{Ornstein-Uhlenbeck} process needs a \emph{warmup period} before it reaches equilibrium.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/ou_proc.png}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> plot(price_s, type="l",
+      xlab="time", ylab="prices",
+      main="Ornstein-Uhlenbeck Process")
> legend("topright",
+  title=paste(c(paste0("sig_ma = ", sig_ma),
+                paste0("eq_price = ", eq_price),
+                paste0("the_ta = ", the_ta)),
+              collapse="\n"),
+  legend="", cex=0.8, inset=0.1, bg="white", bty="n")
> abline(h=eq_price, col='red', lwd=2)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Ornstein-Uhlenbeck Process Returns Correlation}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Under the \emph{Ornstein-Uhlenbeck} process, the returns are negatively correlated to the lagged prices.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> re_turns <- rutils::diff_it(price_s)
> lag_prices <- rutils::lag_it(price_s)
> for_mula <- re_turns ~ lag_prices
> l_m <- lm(for_mula)
> summary(l_m)
> # Plot regression
> plot(for_mula, main="OU Returns Versus Lagged Prices")
> abline(l_m, lwd=2, col="red")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/ou_scatter.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Calibrating the Ornstein-Uhlenbeck Parameters}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The volatility parameter of the Ornstein-Uhlenbeck process can be estimated directly from the standard deviation of the returns.
      \vskip1ex
      The $\theta$ and $\mu$ parameters can be estimated from the linear regression of the returns versus the lagged prices.
      \vskip1ex
      Calculating regression parameters directly from formulas has the advantage of much faster calculations.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate volatility parameter
> c(volatility=sig_ma, estimate=sd(re_turns))
> # Extract OU parameters from regression
> co_eff <- summary(l_m)$coefficients
> # Calculate regression alpha and beta directly
> be_ta <- cov(re_turns, lag_prices)/var(lag_prices)
> al_pha <- (mean(re_turns) - be_ta*mean(lag_prices))
> cbind(direct=c(alpha=al_pha, beta=be_ta), lm=co_eff[, 1])
> all.equal(c(alpha=al_pha, beta=be_ta), co_eff[, 1],
+     check.attributes=FALSE)
> # Calculate regression standard errors directly
> beta_s <- c(alpha=al_pha, beta=be_ta)
> fit_ted <- (al_pha + be_ta*lag_prices)
> residual_s <- (re_turns - fit_ted)
> prices_squared <- sum((lag_prices - mean(lag_prices))^2)
> beta_sd <- sqrt(sum(residual_s^2)/prices_squared/(n_rows-2))
> alpha_sd <- sqrt(sum(residual_s^2)/(n_rows-2)*(1/n_rows + mean(lag_prices)^2/prices_squared))
> cbind(direct=c(alpha_sd=alpha_sd, beta_sd=beta_sd), lm=co_eff[, 2])
> all.equal(c(alpha_sd=alpha_sd, beta_sd=beta_sd), co_eff[, 2],
+     check.attributes=FALSE)
> # Compare mean reversion parameter theta
> c(theta=(-the_ta), round(co_eff[2, ], 3))
> # Compare equilibrium price mu
> c(eq_price=eq_price, estimate=-co_eff[1, 1]/co_eff[2, 1])
> # Compare actual and estimated parameters
> co_eff <- cbind(c(the_ta*eq_price, -the_ta), co_eff[, 1:2])
> rownames(co_eff) <- c("drift", "theta")
> colnames(co_eff)[1] <- "actual"
> round(co_eff, 4)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Schwartz Process}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Ornstein-Uhlenbeck} prices can be negative, while actual prices are usually not negative.
      \vskip1ex
      So the \emph{Ornstein-Uhlenbeck} process is better suited for simulating the logarithm of prices, which can be negative.
      \vskip1ex
      The \emph{Schwartz} process is similar to the \emph{Ornstein-Uhlenbeck} process, but it avoids negative prices by compounding the percentage returns $r_i$ instead of summing them:
      \begin{align*}
        r_i &= \log{p_i} - \log{p_{i-1}} = \theta \, (\mu - p_{i-1}) + \sigma \, \xi_i \\
        p_i &= p_{i-1} \exp(r_i)
      \end{align*}
      Where the parameter $\theta$ is the strength of mean reversion, $\sigma$ is the volatility, and $\xi_i$ are random normal \emph{innovations} with zero mean and unit variance.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Simulate Schwartz process
> re_turns <- numeric(n_rows)
> price_s <- numeric(n_rows)
> price_s[1] <- sig_ma*in_nov[1]
> set.seed(1121)  # Reset random numbers
> for (i in 2:n_rows) {
+   re_turns[i] <- the_ta*(eq_price - price_s[i-1]) + sig_ma*in_nov[i]
+   price_s[i] <- price_s[i-1]*exp(re_turns[i])
+ }  # end for
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/ou_schwartz.png}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> plot(price_s, type="l", xlab="time", ylab="prices",
+      main="Schwartz Process")
> legend("topright",
+  title=paste(c(paste0("sig_ma = ", sig_ma),
+                paste0("eq_price = ", eq_price),
+                paste0("the_ta = ", the_ta)),
+              collapse="\n"),
+  legend="", cex=0.8, inset=0.12, bg="white", bty="n")
> abline(h=eq_price, col='red', lwd=2)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation Function of Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The estimator of \emph{autocorrelation} of a time series is equal to:
      \begin{displaymath}
        \rho_k = \frac{\sum_{i=k+1}^n (x_i-\bar{x})(x_{i-k}-\bar{x})}{(n-k) \, \sigma^2}
      \end{displaymath}
      The \emph{autocorrelation function} (\emph{ACF}) is the vector of autocorrelation coefficients.
      \vskip1ex
      The function \texttt{stats::acf()} calculates and plots the autocorrelation function of a time series.
      \vskip1ex
      The function \texttt{stats::acf()} has the drawback that it plots the lag zero autocorrelation (which is simply equal to $1$).
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> x11(width=6, height=5)
> par(mar=c(3, 2, 1, 1), oma=c(1, 0, 0, 0))
> re_turns <- na.omit(rutils::etf_env$re_turns$VTI)
> # Plot autocorrelations using stats::acf()
> stats::acf(re_turns, lag=10, xlab="lag", main="")
> title(main="ACF of VTI Returns", line=-1)
> # Two-tailed 95% confidence interval
> qnorm(0.975)/sqrt(NROW(re_turns))
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
    \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/acf_vti.png}\\
      The \emph{VTI} time series of returns does not appear to have statistically significant autocorrelations.
      \vskip1ex
      The horizontal dashed lines are two-tailed confidence intervals of the autocorrelation estimator at \texttt{95\%} significance level: $\frac{\Phi^{-1}(0.975)}{\sqrt{n}}$.
      \vskip1ex
      But the visual inspection of the \emph{ACF} plot alone is not enough to test whether autocorrelations are statistically significant or not.
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\subsection{draft: Standard Errors of Autocorrelations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Under the \emph{null hypothesis} of zero autocorrelation, the standard error of the autocorrelation estimator is equal to: $\frac{1}{\sqrt{n-2}}$, and slowly decreases as the square root of $n$ - the length of the time series.
      \vskip1ex
      The function \texttt{cor()} calculates the correlation between two numeric vectors.
      \vskip1ex
      The function \texttt{cor.test()} performs a test of the statistical significance of the correlation coefficient.
      \vskip1ex
      The horizontal dashed lines are two-tailed confidence intervals of the autocorrelation estimator at \texttt{95\%} significance level: .
      \vskip1ex
      The \emph{Ljung-Box} test, tests if the autocorrelations of a time series are \emph{statistically significant}.
      \vskip1ex
      The test statistic is:
      \begin{displaymath}
        Q = n(n+2) \sum_{k=1}^{maxlag} \frac{{\hat\rho}_k^2}{n-k}
      \end{displaymath}
      Where \texttt{n} is the sample size, and the ${\hat\rho}_k$ are sample autocorrelations.
      \vskip1ex
      The \emph{Ljung-Box} statistic follows the \emph{chi-squared} distribution with \emph{maxlag} degrees of freedom.
      \vskip1ex
      The \emph{Ljung-Box} statistic is small for time series that have \emph{statistically insignificant} autocorrelations.
      \vskip1ex
      The function \texttt{Box.test()} calculates the \emph{Ljung-Box} test and returns the test statistic and its p-value.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate VTI and XLF percentage returns
> re_turns <- rutils::etf_env$re_turns[, c("VTI", "XLF")]
> re_turns <- na.omit(re_turns)
> n_rows <- NROW(re_turns)
> # De-mean (center) and scale the returns
> re_turns <- apply(re_turns, MARGIN=2,
+   function(x) (x-mean(x))/sd(x))
> apply(re_turns, MARGIN=2, sd)
> # Calculate the correlation
> drop(re_turns[, "VTI"] %*% re_turns[, "XLF"])/(n_rows-1)
> co_r <- cor(re_turns[, "VTI"], re_turns[, "XLF"])
> # Test statistical significance of correlation
> cor_test <- cor.test(re_turns[, "VTI"], re_turns[, "XLF"])
> conf_int <- qnorm((1+0.95)/2)/sqrt(n_rows)
> co_r*c(1-conf_int, 1+conf_int)
> 
> # Get source code
> stats:::cor.test.default
> 
> # Ljung-Box test for VTI returns
> # 'lag' is the number of autocorrelation coefficients
> Box.test(re_turns, lag=10, type="Ljung")
> library(Ecdat)  # Load Ecdat
> macro_zoo <- as.zoo(Macrodat[, c("lhur", "fygm3")])
> colnames(macro_zoo) <- c("unemprate", "3mTbill")
> macro_diff <- na.omit(diff(macro_zoo))
> # Changes in 3 month T-bill rate are autocorrelated
> Box.test(macro_diff[, "3mTbill"], lag=10, type="Ljung")
> # Changes in unemployment rate are autocorrelated
> Box.test(macro_diff[, "unemprate"], lag=10, type="Ljung")
\end{verbatim}
\end{kframe}
\end{knitrout}
      The \emph{p}-value for \emph{VTI} returns is large, and we conclude that the \emph{null hypothesis} is \texttt{TRUE}, and that \emph{VTI} returns are \emph{not} autocorrelated.
      \vskip1ex
      The \emph{p}-value for changes in econometric data is extremely small, and we conclude that the \emph{null hypothesis} is \texttt{FALSE}, and that econometric data \emph{are} autocorrelated.
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\subsection{Ljung-Box Test for Autocorrelations of Time Series}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Ljung-Box} test, tests if the autocorrelations of a time series are \emph{statistically significant}.
      \vskip1ex
      The \emph{null hypothesis} of the \emph{Ljung-Box} test is that the autocorrelations are equal to zero.
      \vskip1ex
      The test statistic is:
      \begin{displaymath}
        Q = n(n+2) \sum_{k=1}^{maxlag} \frac{{\hat\rho}_k^2}{n-k}
      \end{displaymath}
      Where \texttt{n} is the sample size, and the ${\hat\rho}_k$ are sample autocorrelations.
      \vskip1ex
      The \emph{Ljung-Box} statistic follows the \emph{chi-squared} distribution with \emph{maxlag} degrees of freedom.
      \vskip1ex
      The \emph{Ljung-Box} statistic is small for time series that have \emph{statistically insignificant} autocorrelations.
      \vskip1ex
      The function \texttt{Box.test()} calculates the \emph{Ljung-Box} test and returns the test statistic and its p-value.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Ljung-Box test for VTI returns
> # 'lag' is the number of autocorrelation coefficients
> Box.test(re_turns, lag=10, type="Ljung")
> library(Ecdat)  # Load Ecdat
> macro_zoo <- as.zoo(Macrodat[, c("lhur", "fygm3")])
> colnames(macro_zoo) <- c("unemprate", "3mTbill")
> macro_diff <- na.omit(diff(macro_zoo))
> # Changes in 3 month T-bill rate are autocorrelated
> Box.test(macro_diff[, "3mTbill"], lag=10, type="Ljung")
> # Changes in unemployment rate are autocorrelated
> Box.test(macro_diff[, "unemprate"], lag=10, type="Ljung")
\end{verbatim}
\end{kframe}
\end{knitrout}
      The \emph{p}-value for \emph{VTI} returns is small, and we conclude that the \emph{null hypothesis} is \texttt{FALSE}, and that \emph{VTI} returns have some small autocorrelations.
      \vskip1ex
      The \emph{p}-value for changes in econometric data is extremely small, and we conclude that the \emph{null hypothesis} is \texttt{FALSE}, and that econometric data \emph{are} autocorrelated.
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Improved Autocorrelation Function}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{acf()} has the drawback that it plots the lag zero autocorrelation (which is simply equal to $1$).
      \vskip1ex
      Inspection of the data returned by \texttt{acf()} shows how to omit the lag zero autocorrelation.
      \vskip1ex
      The function \texttt{acf()} returns the \emph{ACF} data invisibly, i.e. the return value can be assigned to a variable, but otherwise it isn't automatically printed to the console.
      \vskip1ex
      The function \texttt{rutils::plot\_acf()} from package \emph{rutils} is a wrapper for \texttt{acf()}, and it omits the lag zero autocorrelation.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Get the ACF data returned invisibly
> acf_data <- acf(re_turns, plot=FALSE)
> summary(acf_data)
> # Print the ACF data
> print(acf_data)
> dim(acf_data$acf)
> dim(acf_data$lag)
> head(acf_data$acf)
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
    \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> plot_acf <- function(x_ts, lagg=10, plo_t=TRUE,
+                xlab="Lag", ylab="", main="", ...) {
+   # Calculate the ACF without a plot
+   acf_data <- acf(x=x_ts, lag.max=lagg, plot=FALSE, ...)
+   # Remove first element of ACF data
+   acf_data$acf <- array(data=acf_data$acf[-1],
+     dim=c((dim(acf_data$acf)[1]-1), 1, 1))
+   acf_data$lag <- array(data=acf_data$lag[-1],
+     dim=c((dim(acf_data$lag)[1]-1), 1, 1))
+   # Plot ACF
+   if (plo_t) {
+     ci <- qnorm((1+0.95)/2)*sqrt(1/NROW(x_ts))
+     ylim <- c(min(-ci, range(acf_data$acf[-1])),
+         max(ci, range(acf_data$acf[-1])))
+     plot(acf_data, xlab=xlab, ylab=ylab,
+    ylim=ylim, main="", ci=0)
+     title(main=main, line=0.5)
+     abline(h=c(-ci, ci), col="blue", lty=2)
+   }  # end if
+   # Return the ACF data invisibly
+   invisible(acf_data)
+ }  # end plot_acf
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of \protect\emph{VTI} Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{VTI} returns appear to have some small, yet significant negative autocorrelations at \texttt{lag=1}.
      \vskip1ex
      But the visual inspection of the \emph{ACF} plot alone is not enough to test whether autocorrelations are statistically significant or not.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Improved autocorrelation function
> x11(width=6, height=5)
> rutils::plot_acf(re_turns, lag=10, main="")
> title(main="ACF of VTI returns", line=-1)
> # Ljung-Box test for VTI returns
> Box.test(re_turns, lag=10, type="Ljung")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/acf_vti_bis.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of Squared \protect\emph{VTI} Returns}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Squared random returns are not autocorrelated.
      \vskip1ex
      But squared \emph{VTI} returns do have statistically significant autocorrelations.
      \vskip1ex
      The autocorrelations of squared asset returns are a very important feature.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> par(mar=c(3, 3, 2, 2), oma=c(0, 0, 0, 0), mgp=c(2, 1, 0))
> # Autocorrelation of squared random returns
> rutils::plot_acf(rnorm(NROW(re_turns))^2, lag=10, main="")
> title(main="ACF of Squared Random Returns", line=-1)
> # Autocorrelation of squared VTI returns
> rutils::plot_acf(re_turns^2, lag=10, main="")
> title(main="ACF of Squared VTI Returns", line=-1)
> # Ljung-Box test for squared VTI returns
> Box.test(re_turns^2, lag=10, type="Ljung")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/acf_vti_squared.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{U.S. Macroeconomic Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The package \emph{Ecdat} contains the \texttt{Macrodat} U.S. macroeconomic data.
      \vskip1ex
      \texttt{"lhur"} is the unemployment rate (average of months in quarter).
      \vskip1ex
      \texttt{"fygm3"} 3 month treasury bill interest rate (last month in quarter)
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> library(Ecdat)  # Load Ecdat
> colnames(Macrodat)  # United States Macroeconomic Time Series
> # Coerce to "zoo"
> macro_zoo <- as.zoo(Macrodat[, c("lhur", "fygm3")])
> colnames(macro_zoo) <- c("unemprate", "3mTbill")
> # ggplot2 in multiple panes
> autoplot(  # Generic ggplot2 for "zoo"
+   object=macro_zoo, main="US Macro",
+   facets=Series ~ .) + # end autoplot
+   xlab("") +
+ theme(  # Modify plot theme
+   legend.position=c(0.1, 0.5),
+   plot.title=element_text(vjust=-2.0),
+   plot.margin=unit(c(-0.5, 0.0, -0.5, 0.0), "cm"),
+   plot.background=element_blank(),
+   axis.text.y=element_blank()
+ )  # end theme
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \hspace*{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/macro_data-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelation of Econometric Data}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Most econometric data displays a high degree of autocorrelation.
      \vskip1ex
      But the time series of asset returns display very low autocorrelations.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> macro_diff <- na.omit(diff(macro_zoo))
> rutils::plot_acf(coredata(macro_diff[, "unemprate"]),
+   lag=10, main="quarterly unemployment rate")
> rutils::plot_acf(coredata(macro_diff[, "3mTbill"]),
+   lag=10, main="3 month T-bill EOQ")
\end{verbatim}
\end{kframe}
\end{knitrout}
      The function \texttt{zoo::coredata()} extracts the underlying numeric data from a complex data object.
    \column{0.5\textwidth}
      \vspace{-2em}
      \includegraphics[width=0.45\paperwidth]{figure/macro_corr-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{autoregressive} process \emph{AR(p)} of order \emph{p} for a time series $r_i$ is defined as:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i
      \end{displaymath}
      Where $\varphi_i$ are the \emph{AR(p)} coefficients, and $\xi_i$ are random normal \emph{innovations} with zero mean and unit variance.
      \vskip1ex
      The \emph{AR(p)} process is a special case of an \emph{ARIMA} process, and is simply called an \emph{AR(p)} process.
      \vskip1ex
      If the \emph{AR(p)} process is \emph{stationary} then the time series $r_i$ is mean reverting to zero.
      \vskip1ex
      The function \texttt{arima.sim()} simulates \emph{ARIMA} processes, with the \texttt{"model"} argument accepting a \texttt{list} of \emph{AR(p)} coefficients $\varphi_i$.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> date_s <- Sys.Date() + 0:728  # Two year daily series
> # AR time series of returns
> ari_ma <- xts(x=arima.sim(n=NROW(date_s), model=list(ar=0.2)), 
+           order.by=date_s)
> ari_ma <- cbind(ari_ma, cumsum(ari_ma))
> colnames(ari_ma) <- c("AR returns", "AR prices")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      % \hspace*{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/ar_process.png}
      \vspace{-3em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> autoplot(object=ari_ma, # ggplot AR process
+  facets="Series ~ .",
+  main="Autoregressive process (phi=0.2)") +
+   facet_grid("Series ~ .", scales="free_y") +
+   xlab("") + ylab("") +
+ theme(legend.position=c(0.1, 0.5),
+   plot.background=element_blank(),
+   axis.text.y=element_blank())
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Examples of Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The speed of mean reversion of an \emph{AR(1)} process depends on the \emph{AR(p)} coefficient $\varphi_1$, with a negative coefficient producing faster mean reversion, and a positive coefficient producing stronger diversion.
      \vskip1ex
      A positive coefficient $\varphi_1$ produces a diversion away from the mean, so that the time series $r_i$ wanders away from the mean for longer periods of time.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> ar_coeff <- c(-0.9, 0.01, 0.9)  # AR coefficients
> # Create three AR time series
> ari_ma <- sapply(ar_coeff, function(phi) {
+   set.seed(1121)  # Reset random numbers
+   arima.sim(n=NROW(date_s), model=list(ar=phi))
+ })  # end sapply
> colnames(ari_ma) <- paste("autocorr", ar_coeff)
> plot.zoo(ari_ma, main="AR(1) prices", xlab=NA)
> # Or plot using ggplot
> ari_ma <- xts(x=ari_ma, order.by=date_s)
> library(ggplot)
> autoplot(ari_ma, main="AR(1) prices",
+    facets=Series ~ .) +
+     facet_grid(Series ~ ., scales="free_y") +
+ xlab("") +
+ theme(
+   legend.position=c(0.1, 0.5),
+   plot.title=element_text(vjust=-2.0),
+   plot.margin=unit(c(-0.5, 0.0, -0.5, 0.0), "cm"),
+   plot.background=element_blank(),
+   axis.text.y=element_blank())
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/ar_processes.png}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{autoregressive} process \emph{AR(p)}:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i
      \end{displaymath}
      Can be simulated by using an explicit recursive loop in \texttt{R}.
      \vskip1ex
      \emph{AR(p)} processes can also be simulated by using the function \texttt{filter()} directly, with the argument \texttt{method="recursive"}.
      \vskip1ex
      The function \texttt{filter()} applies a linear filter to a vector, and returns a time series of class \texttt{"ts"}.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Define AR(3) coefficients and innovations
> co_eff <- c(0.1, 0.39, 0.5)
> n_rows <- 1e2
> set.seed(1121); in_nov <- rnorm(n_rows)
> # Simulate AR process using recursive loop in R
> ari_ma <- numeric(NROW(in_nov))
> ari_ma[1] <- in_nov[1]
> ari_ma[2] <- co_eff[1]*ari_ma[1] + in_nov[2]
> ari_ma[3] <- co_eff[1]*ari_ma[2] + co_eff[2]*ari_ma[1] + in_nov[3]
> for (it in 4:NROW(ari_ma)) {
+   ari_ma[it] <- ari_ma[(it-1):(it-3)] %*% co_eff + in_nov[it]
+ }  # End for
> # Simulate AR process using filter()
> arima_faster <- filter(x=in_nov, filter=co_eff, method="recursive")
> class(arima_faster)
> all.equal(ari_ma, as.numeric(arima_faster))
> # Fast simulation of AR process using C_rfilter()
> arima_fastest <- .Call(stats:::C_rfilter, in_nov, co_eff,
+                  double(NROW(co_eff) + NROW(in_nov)))[-(1:3)]
> all.equal(ari_ma, arima_fastest)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Simulating Autoregressive Processes Using \texttt{arima.sim()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The function \texttt{arima.sim()} simulates \emph{ARIMA} processes by calling the function \texttt{filter()}.
      \vskip1ex
      \emph{ARIMA} processes can also be simulated by using the function \texttt{filter()} directly, with the argument \texttt{method="recursive"}.
      \vskip1ex
      Simulating stationary \emph{autoregressive} processes requires a \emph{warmup period}, to allow the process to reach its stationary state.
      \vskip1ex
      The required length of the \emph{warmup period} depends on the smallest root of the characteristic equation, with a longer \emph{warmup period} needed for smaller roots, that are closer to $1$.
      \vskip1ex
      The \emph{rule of thumb} (heuristic rule, guideline) is for the \emph{warmup period} to be equal to \texttt{6} divided  by the logarithm of the smallest characteristic root plus the number of \emph{AR(p)} coefficients: $\frac{6}{\log(minroot)} + \operatorname{numcoeff}$
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate modulus of roots of characteristic equation
> root_s <- Mod(polyroot(c(1, -co_eff)))
> # Calculate warmup period
> warm_up <- NROW(co_eff) + ceiling(6/log(min(root_s)))
> set.seed(1121)
> n_rows <- 1e4
> in_nov <- rnorm(n_rows + warm_up)
> # Simulate AR process using arima.sim()
> ari_ma <- arima.sim(n=n_rows,
+   model=list(ar=co_eff),
+   start.innov=in_nov[1:warm_up],
+   innov=in_nov[(warm_up+1):NROW(in_nov)])
> # Simulate AR process using filter()
> arima_fast <- filter(x=in_nov, filter=co_eff, method="recursive")
> all.equal(arima_fast[-(1:warm_up)], as.numeric(ari_ma))
> # Benchmark the speed of the three methods of simulating AR process
> library(microbenchmark)
> summary(microbenchmark(
+   filter=filter(x=in_nov, filter=co_eff, method="recursive"),
+   arima_sim=arima.sim(n=n_rows,
+                   model=list(ar=co_eff),
+                   start.innov=in_nov[1:warm_up],
+                   innov=in_nov[(warm_up+1):NROW(in_nov)]),
+   arima_loop={for (it in 4:NROW(ari_ma)) {
+   ari_ma[it] <- ari_ma[(it-1):(it-3)] %*% co_eff + in_nov[it]}}
+   ), times=10)[, c(1, 4, 5)]
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Autocorrelations of Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The autocorrelation $\rho_i$ of an \emph{AR(1)} process (defined as $r_i = \varphi r_{i-1} + \xi_i$), satisfies the recursive equation: $\rho_i = \varphi \rho_{i-1}$, with $\rho_1 = \varphi$.
      \vskip1ex
      Therefore \emph{AR(1)} processes have exponentially decaying autocorrelations: $\rho_i = \varphi^i$.
      \vskip1ex
      The \emph{AR(1)} process can be solved recursively:
      \begin{align*}
        r_1 &= \xi_1 \\
        r_2 &= \varphi r_1 + \xi_2 = \xi_2 + \varphi \xi_1 \\
        r_3 &= \xi_3 + \varphi \xi_2 + \varphi^2 \xi_1 \\
        r_4 &= \xi_4 + \varphi \xi_3 + \varphi^2 \xi_2 + \varphi^3 \xi_1
      \end{align*}
      Therefore the \emph{AR(1)} process can be expressed as a \emph{moving average} (\emph{MA}) of the \emph{innovations} $\xi_i$: $r_i = \sum_{i=1}^n {\varphi^{i-1} \xi_i}$.
      \vskip1ex
      If $\varphi < 1.0$ then the influence of the innovation $\xi_i$ decays exponentially.
      \vskip1ex
      If $\varphi = 1.0$ then the influence of the random innovations $\xi_i$ persists indefinitely, so that the variance of $r_i$ increases linearly with time.
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/ar_acf.png}
      An \emph{AR(1)} process has an exponentially decaying \emph{ACF}.
      % \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Simulate AR(1) process
> ari_ma <- arima.sim(n=1e3, model=list(ar=0.8))
> # ACF of AR(1) process
> ac_f <- rutils::plot_acf(ari_ma, lag=10, xlab="", ylab="",
+   main="Autocorrelations of AR(1) process")
> ac_f$acf[1:5]
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Partial Autocorrelations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      If two random variables are both correlated to a third variable, then they are indirectly correlated with each other.
      \vskip1ex
      The indirect correlation can be removed by defining new variables with no correlation to the third variable.
      \vskip1ex
      The \emph{partial correlation} is the correlation after the correlations to the common variables are removed.
      \vskip1ex
      The \emph{partial autocorrelations} $\varrho_i$ of an \emph{AR(1)} process can be computed recursively from the autocorrelations $\rho_i$ using the Durbin-Levinson algorithm:
      \begin{align*}
        \varrho_1 &= \rho_1 \\
        \varrho_2 &= \rho_2 - \varrho_1 \rho_1 \\
        \varrho_3 &= \rho_3 - \varrho_1 \rho_2 - \varrho_2 \rho_1
      \end{align*}
      The function \texttt{pacf()} calculates and plots the \emph{partial autocorrelations}, but it performs regressions instead of using the Durbin-Levinson algorithm.
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/ar_pacf.png}
      % \vspace{-2em}
      An \emph{AR(1)} process has an exponentially decaying \emph{ACF} and a non-zero \emph{PACF} at lag one.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # PACF of AR(1) process
> pac_f <- pacf(ari_ma, lag=10, xlab="", ylab="", main="")
> title("Partial autocorrelations of AR(1) process", line=1)
> pac_f <- drop(pac_f$acf)
> pac_f[1:5]
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Partial Autocorrelations of \protect\emph{AR(1)} Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An autocorrelation of lag $1$ induces higher order autocorrelations of lag \texttt{2, 3, ...}, which may obscure the true higher order autocorrelations.
      \vskip1ex
      A linear combination of the time series and its own lag can be created, such that its lag $1$ autocorrelation is zero.
      \vskip1ex
      The lag $2$ autocorrelation of this new series is called the \emph{partial autocorrelation} of lag $2$, and represents the true second order autocorrelation.
      \vskip1ex
      The \emph{partial autocorrelation} of lag \texttt{k} is the autocorrelation of lag \texttt{k}, after all the autocorrelations of lag \texttt{1, ..., k-1} have been removed.
      \vskip1ex
      The \emph{partial autocorrelations} $\varrho_i$ of an \emph{AR(1)} process can be computed recursively from the autocorrelations $\rho_i$ using the Durbin-Levinson algorithm:
      \begin{displaymath}
        \varrho_k = \rho_k - \sum_{i=1}^{k-1} {\varrho_i \rho_{k-i}}
      \end{displaymath}
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Compute pacf recursively from acf
> ac_f <- rutils::plot_acf(ari_ma, lag=10, plo_t=FALSE)
> ac_f <- drop(ac_f$acf)
> pac_f <- numeric(3)
> pac_f[1] <- ac_f[1]
> pac_f[2] <- ac_f[2] - ac_f[1]^2
> pac_f[3] <- ac_f[3] - pac_f[2]*ac_f[1] - ac_f[2]*pac_f[1]
> # Compute pacf recursively in a loop
> pac_f <- numeric(NROW(ac_f))
> pac_f[1] <- ac_f[1]
> for (it in 2:NROW(pac_f)) {
+   pac_f[it] <- ac_f[it] - pac_f[1:(it-1)] %*% ac_f[(it-1):1]
+ }  # end for
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Higher Order Autocorrelations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{AR(3)} process of order \emph{three} is defined by the formula:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \varphi_3 r_{i-3} + \xi_i
      \end{displaymath}
      Autoregressive processes \emph{AR(p)} of order \emph{p} have an exponentially decaying \emph{ACF} and a non-zero \emph{PACF} up to lag \emph{p}.
      \vskip1ex
      The number of non-zero \emph{partial autocorrelations} is equal to the \emph{order} parameter $p$ of the \emph{AR(p)} process.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Simulate AR process of returns
> ari_ma <- arima.sim(n=1e3, model=list(ar=c(0.1, 0.5, 0.1)))
> # ACF of AR(3) process
> rutils::plot_acf(ari_ma, lag=10, xlab="", ylab="",
+    main="ACF of AR(3) process")
> # PACF of AR(3) process
> pacf(ari_ma, lag=10, xlab="", ylab="", main="PACF of AR(3) process")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \vspace{-2em}
      \includegraphics[width=0.45\paperwidth]{figure/ar_pacf-1}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Stationary Processes and Unit Root Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      A process is \emph{stationary} if its probability distribution does not change with time, which means that it has constant mean and variance.
      \vskip1ex
      The \emph{autoregressive} process \emph{AR(p)}:
      $r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i$
      \vskip1ex
      Has the following characteristic equation:
      $1 - \varphi_1 z - \varphi_2 z^2 - \ldots - \varphi_p z^p = 0$
      \vskip1ex
      An autoregressive process is \emph{stationary} only if the absolute values of all the roots of its characteristic equation are greater than $1$.
      \vskip1ex
      If the sum of the autoregressive coefficients is equal to $1$: $\sum_{i=1}^p \varphi_i = 1$, then the process has a root equal to $1$ (it has a \emph{unit root}), so it's not \emph{stationary}.
      \vskip1ex
      Non-stationary processes with unit roots are called \emph{unit root} processes.
      \vskip1ex
      A simple example of a \emph{unit root} process is the \emph{Brownian Motion}:
      $p_i = p_{i-1} + \xi_i$
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/stat_unit_root-1}
      \vspace{-2em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> rand_walk <- cumsum(zoo(matrix(rnorm(3*100), ncol=3),
+             order.by=(Sys.Date()+0:99)))
> colnames(rand_walk) <- paste("rand_walk", 1:3, sep="_")
> plot.zoo(rand_walk, main="Random walks",
+      xlab="", ylab="", plot.type="single",
+      col=c("black", "red", "blue"))
> # Add legend
> legend(x="topleft", legend=colnames(rand_walk),
+  col=c("black", "red", "blue"), lty=1)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Integrated and Unit Root Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The cumulative sum of a given process is called its \emph{integrated} process.
      \vskip1ex
      For example, asset prices follow an \emph{integrated} process with respect to asset returns: $p_n = {\sum_{i=1}^n r_i}$.
      \vskip1ex
      If returns follow an \emph{AR(p)} process:
      $r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i$
      \vskip1ex
      Then asset prices follow the process:
      $p_i = (1 + \varphi_1) p_{i-1} + (\varphi_2 - \varphi_1) p_{i-2} + \ldots + (\varphi_p - \varphi_{p-1}) p_{i-p} - \varphi_p p_{i-p-1} + \xi_i$
      \vskip1ex
      The sum of the coefficients of the price process is equal to $1$, so it has a \emph{unit root} for all values of the $\varphi_i$ coefficients.
      \vskip1ex
      The \emph{integrated} process of an \emph{AR(p)} process is always a \emph{unit root} process.
    \column{0.5\textwidth}
      For example, if returns follow an \emph{AR(1)} process: $r_i = \varphi r_{i-1} + \xi_i$.
      \vskip1ex
      Then asset prices follow the process: $p_i = (1 + \varphi) p_{i-1} - \varphi p_{i-2} + \xi_i$
      \vskip1ex
      Which is a \emph{unit root} process for all values of $\varphi$, because the sum of its coefficients is equal to $1$.
      \vskip1ex
      If $\varphi = 0$ then the above process is a \emph{Brownian Motion} (random walk).
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Simulate arima with large AR coefficient
> set.seed(1121)
> ari_ma <- arima.sim(n=n_rows, model=list(ar=0.99))
> tseries::adf.test(ari_ma)
> # Integrated series has unit root
> tseries::adf.test(cumsum(ari_ma))
> # Simulate arima with negative AR coefficient
> set.seed(1121)
> ari_ma <- arima.sim(n=n_rows, model=list(ar=-0.99))
> tseries::adf.test(ari_ma)
> # Integrated series has unit root
> tseries::adf.test(cumsum(ari_ma))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Variance of Unit Root Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{AR(1)} process:
      $r_i = \varphi r_{i-1} + \xi_i$
      has the following characteristic equation:
      $1 - \varphi z = 0$,
      with a root equal to:
      $z = 1 / \varphi$
      \vskip1ex
      If $\varphi = 1$, then the characteristic equation has a \emph{unit root} (and therefore it isn't \emph{stationary}), and the process follows:
      $r_i = r_{i-1} + \xi_i$
      \vskip1ex
      The above is called a \emph{Brownian Motion}, and it's an example of a \emph{unit root} process.
      \vskip1ex
      The expected value of the \emph{AR(1)} process $r_i = \varphi r_{i-1} + \xi_i$ is equal to zero: $\mathbb{E}[r_i] = \frac{\mathbb{E}[\xi_i]}{1 - \varphi} = 0$.
      \vskip1ex
      And its variance is equal to: $\sigma^2 = \mathbb{E}[r^2_i] = \frac{\sigma_{\xi}^2}{1 - \varphi^2}$.
      \vskip1ex
      If $\varphi = 1$, then the \emph{variance} grows over time and becomes infinite over time, so the process isn't \emph{stationary}.
      \vskip1ex
      The variance of the \emph{Brownian Motion} $r_i = r_{i-1} + \xi$ is proportional to time: $\sigma^2_i = \mathbb{E}[r^2_i] = i \sigma_{\xi}^2$
    \column{0.5\textwidth}
      \vspace{-1em}
      \hspace*{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/brownian_var.png}
      \vspace{-2em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Simulate random walks using apply() loops
> set.seed(1121)  # Initialize random number generator
> rand_walks <- matrix(rnorm(1000*100), ncol=1000)
> rand_walks <- apply(rand_walks, 2, cumsum)
> vari_ance <- apply(rand_walks, 1, var)
> # Simulate random walks using vectorized functions
> set.seed(1121)  # Initialize random number generator
> rand_walks <- matrixStats::colCumsums(matrix(rnorm(1000*100), ncol=1000))
> vari_ance <- matrixStats::rowVars(rand_walks)
> par(mar=c(5, 3, 2, 2), oma=c(0, 0, 0, 0))
> plot(vari_ance, xlab="time steps", ylab="",
+      t="l", col="blue", lwd=2,
+      main="Variance of Random Walk")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Dickey-Fuller Process}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Dickey-Fuller} process is a combination of the \emph{Ornstein-Uhlenbeck} and the \emph{autoregressive} processes.
      \vskip1ex
      In the \emph{Dickey-Fuller} process, the returns $r_i$ are equal to the sum of \emph{autoregressive} terms plus a mean reverting term:
      {\scriptsize
      \begin{align*}
        r_i = \gamma (p_{i-1} - p_{eq}) + \varphi_1 r_{i-1} + \ldots + \varphi_p r_{i-p} + \sigma \, \xi_i
        p_i &= p_{i-1} + r_i
      \end{align*}
      }
      Where $p_{eq}$ is the equilibrium price towards which prices revert, $\sigma$ is the volatility of returns, and $\xi_i$ are random normal \emph{innovations} with zero mean and unit variance.
      \vskip1ex
      If the mean reversion parameter $\gamma$ is negative: $\gamma < 0$, then the time series $p_i$ exhibits mean reversion and has no \emph{unit root}.
      \vskip1ex
      The \emph{Dickey-Fuller} process for prices can be written as an \emph{AR(1)} process plus a drift:
      \begin{displaymath}
        p_i = (1 + \varphi_1) p_{i-1} + (\varphi_2 - \varphi_1) p_{i-2} + \ldots + (\varphi_p - \varphi_{p-1}) p_{i-p} - \varphi_p p_{i-p-1} + \sigma \, \xi_i
      \end{displaymath}
      The \emph{Dickey-Fuller} process cannot be simulated using the function \texttt{filter()} because of the drift term, so it must be simulated using explicit loops, either in \texttt{R} or in \texttt{C++}.
      \vskip1ex
      The compiled \emph{Rcpp} \texttt{C++} code can be over \texttt{100} times faster than loops in \texttt{R}!
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Define Dickey-Fuller parameters
> eq_price <- 1.0; sig_ma <- 0.02
> the_ta <- 0.01; n_rows <- 1000
> # Initialize the data
> in_nov <- rnorm(n_rows)
> re_turns <- numeric(n_rows)
> price_s <- numeric(n_rows)
> # Simulate Dickey-Fuller process in R
> price_s[1] <- sig_ma*in_nov[1]
> for (i in 2:n_rows) {
+   re_turns[i] <- the_ta*(eq_price - price_s[i-1]) + 
+     sig_ma*in_nov[i]
+   price_s[i] <- price_s[i-1] + re_turns[i]
+ }  # end for
> # Simulate Dickey-Fuller process in Rcpp
> prices_cpp <- HighFreq::sim_ou(eq_price=eq_price, volat=sig_ma, theta=the_ta, innov=matrix(in_nov))
> all.equal(price_s, drop(prices_cpp))
> # Compare the speed of R code with Rcpp
> library(microbenchmark)
> summary(microbenchmark(
+   Rcode={for (i in 2:n_rows) {
+     re_turns[i] <- the_ta*(eq_price - price_s[i-1]) + sig_ma*in_nov[i]
+     price_s[i] <- price_s[i-1] + re_turns[i]}},
+   Rcpp=HighFreq::sim_ou(eq_price=eq_price, volat=sig_ma, theta=the_ta, innov=matrix(in_nov)),
+   times=10))[, c(1, 4, 5)]  # end microbenchmark summary
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Augmented Dickey-Fuller ADF Test for Unit Roots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{Augmented Dickey-Fuller} \emph{ADF} test is designed to test the \emph{null hypothesis} that a time series has a \emph{unit root}.
      \vskip1ex
      The \emph{ADF} test fits an autoregressive model with an extra mean reversion term:
      {\scriptsize
      \begin{displaymath}
        r_i = \gamma (p_{i-1} - p_{eq}) + \varphi_1 r_{i-1} + \ldots + \varphi_p r_{i-p} + \varepsilon_i
      \end{displaymath}
      }
      Where $p_{eq}$ is the equilibrium price towards which prices revert.
      \vskip1ex
      $\varepsilon_i$ are the \emph{residuals}, which are usually assumed to be standard normally distributed $\phi(0, \sigma_\varepsilon)$, independent, and stationary.
      \vskip1ex
      If the mean reversion parameter $\gamma$ is negative: $\gamma < 0$, then the time series $p_i$ exhibits mean reversion and has no \emph{unit root}.
      \vskip1ex
      The \emph{null hypothesis} is that prices have a unit root ($\gamma = 0$, no mean reversion), while the alternative hypothesis is that it's \emph{stationary} ($\gamma < 0$, mean reversion).
      \vskip1ex
      The \emph{ADF} test statistic is equal to the \emph{t}-value of the $\gamma$ parameter: $t_{\gamma} = \hat\gamma / SE_{\gamma}$ (which follows a distribution different from the \texttt{t}-distribution).
    \column{0.5\textwidth}
      The common practice is to use a small number of lags in the \emph{ADF} test, and if the residuals are autocorrelated, then to increase them until the correlations are no longer significant.
      \vskip1ex
      If the number of lags in the regression is zero: $p = 0$ then the \emph{ADF} test becomes the standard \emph{Dickey-Fuller} test: $r_i = \gamma p_{i-1} + \xi_i$.
      \vskip1ex
      The function \texttt{tseries::adf.test()} performs the \emph{ADF} test.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> set.seed(1121); in_nov <- rnorm(1e4, sd=0.01)
> # Simulate AR(1) process with coefficient=1, with unit root
> ari_ma <- filter(x=in_nov, filter=1.0, method="recursive")
> x11(); plot(ari_ma, t="l", main="AR(1) coefficient = 1.0")
> # Perform ADF test with lag = 1
> tseries::adf.test(ari_ma, k=1)
> # Perform standard Dickey-Fuller test
> tseries::adf.test(ari_ma, k=0)
> # Simulate AR(1) with coefficient close to 1, without unit root
> ari_ma <- filter(x=in_nov, filter=0.99, method="recursive")
> x11(); plot(ari_ma, t="l", main="AR(1) coefficient = 0.99")
> tseries::adf.test(ari_ma, k=1)
> # Simulate Ornstein-Uhlenbeck OU process with mean reversion
> eq_price <- 0.0; the_ta <- 0.001
> price_s <- HighFreq::sim_ou(eq_price=eq_price, vol_at=1.0, theta=the_ta, innov=in_nov)
> x11(); plot(price_s, t="l", main=paste("OU coefficient =", the_ta))
> tseries::adf.test(price_s, k=1)
> # Simulate Ornstein-Uhlenbeck OU process with zero reversion
> the_ta <- 0.0
> price_s <- HighFreq::sim_ou(eq_price=eq_price, vol_at=1.0, theta=the_ta, innov=in_nov)
> x11(); plot(price_s, t="l", main=paste("OU coefficient =", the_ta))
> tseries::adf.test(price_s, k=1)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Calculating the ADF Test Statistic}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Calculate the \emph{ADF} Test statistic using matrix algebra.
      \vskip1ex
      The \emph{Dickey-Fuller} and \emph{Augmented Dickey-Fuller} tests are designed to test the \emph{null hypothesis} that a time series process has a \emph{unit root}.
      \vskip1ex
      The \emph{Augmented Dickey-Fuller} (\emph{ADF}) test fits a regression model to determine if the price time series $p_i$ exhibits mean reversion:
      \begin{displaymath}
        r_i = \gamma p_{i-1} + \varphi_1 r_{i-1} + \ldots + \varphi_p r_{i-p} + \xi_i
      \end{displaymath}
      where $p_i = p_{i-1} + r_i$, so that:
      \begin{displaymath}
        p_i = (1 + \gamma) p_{i-1} + \varphi_1 r_{i-1} + \ldots + \varphi_p r_{i-p} + \xi_i
      \end{displaymath}
      If the mean reversion parameter $\gamma$ is negative: $\gamma < 0$, then the time series $p_i$ exhibits mean reversion and has no \emph{unit root}.
      \vskip1ex
      The \emph{null hypothesis} is that the price process has a unit root ($\gamma = 0$, no mean reversion), while the alternative hypothesis is that it's \emph{stationary} ($\gamma < 0$, mean reversion).
      \vskip1ex
      The \emph{ADF} test statistic is equal to the \emph{t}-value of the $\gamma$ parameter: $t_{\gamma} = \hat\gamma / SE_{\gamma}$ (which follows a distribution different from the \texttt{t}-distribution).
    \column{0.5\textwidth}
      The common practice is to perform the \emph{ADF} test with a small number of lags, and if the residuals are autocorrelated, then to increase the number of lags until the correlations are no longer significant.
      \vskip1ex
      If the number of lags in the regression is zero: $p = 0$ then the \emph{ADF} test becomes the standard \emph{Dickey-Fuller} test: $r_i = \gamma p_{i-1} + \xi_i$.
      \vskip1ex
      The function \texttt{tseries::adf.test()} performs the \emph{ADF} test.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> n_rows <- 1e3
> # Perform ADF test for AR(1) with small coefficient
> set.seed(1121)
> ari_ma <- arima.sim(n=n_rows, model=list(ar=0.01))
> tseries::adf.test(ari_ma)
> # Perform ADF test for AR(1) with large coefficient
> set.seed(1121)
> ari_ma <- arima.sim(n=n_rows, model=list(ar=0.99))
> tseries::adf.test(ari_ma)
> # Perform ADF test with lag = 1
> tseries::adf.test(ari_ma, k=1)
> # Perform Dickey-Fuller test
> tseries::adf.test(ari_ma, k=0)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Sensitivity of the ADF Test for Detecting Unit Roots}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{ADF null hypothesis} is that prices have a unit root, while the alternative hypothesis is that they're \emph{stationary}.
      \vskip1ex
      A \emph{true negative} test result is that the \emph{null hypothesis} is \texttt{TRUE} (prices have a unit root), while a \emph{true positive} result is that the \emph{null hypothesis} is \texttt{FALSE} (prices are stationary).
      \vskip1ex
      The \emph{ADF} test has low \emph{sensitivity}, i.e. the ability to correctly identify \emph{positive} cases of no \emph{unit root}, causing it to produce \emph{false negatives} (\emph{type II} errors).
      \vskip1ex
      Therefore the \emph{ADF} test often requires a lot of data before it's able to correctly identify \emph{stationary} prices with \emph{no unit root}.
      \vskip1ex
      The function \texttt{tseries::adf.test()} assumes that the data is \emph{normally distributed}, which may underestimate the standard errors of the parameters, and produce \emph{false positives} (\emph{type I} errors) by incorrectly rejecting the null hypothesis of a unit root process.
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.45\paperwidth]{figure/adf_tests.png}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Simulate AR(1) process with different coefficients
> coeff_s <- seq(0.99, 0.999, 0.001)
> re_turns <- as.numeric(na.omit(rutils::etf_env$re_turns$VTI))
> adf_test <- sapply(coeff_s, function(co_eff) {
+   ari_ma <- filter(x=re_turns, filter=co_eff, method="recursive")
+   ad_f <- suppressWarnings(tseries::adf.test(ari_ma))
+   c(adf_stat=unname(ad_f$statistic), pval=ad_f$p.value)
+ })  # end sapply
> x11(width=6, height=5)
> par(mar=c(4, 4, 2, 2), oma=c(0, 0, 0, 0), mgp=c(2.5, 1, 0))
> plot(x=coeff_s, y=adf_test["pval", ], main="ADF p-val Versus AR Coefficient",
+      xlab="AR coefficient", ylab="ADF pval", t="l", col="blue", lwd=2)
> plot(x=coeff_s, y=adf_test["adf_stat", ], main="ADF Stat Versus AR Coefficient",
+      xlab="AR coefficient", ylab="ADF stat", t="l", col="blue", lwd=2)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}



%%%%%%%%%%%%%%%
\section{Calibrating Time Series Models}


%%%%%%%%%%%%%%%
\subsection{Fitting Time Series to Autoregressive Models}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{autoregressive} process \emph{AR(p)} for the time series of returns $r_i$:
      \begin{multline*}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i = \\
        \sum_{j=1}^{p} {\varphi_j r_{i-j}} + \xi_i
      \end{multline*}
      Can be solved as a \emph{multivariate} linear regression, with the \emph{response} equal to $r_i$, and the columns of the \emph{design matrix} equal to the lags of $r_i$.
      \vskip1ex
      An intercept term can be added to the above formula by adding a unit column to the regression design matrix.
      \vskip1ex
      Adding the intercept term produces slightly different coefficients, depending on the mean of the returns.
      \vskip1ex
      The function \texttt{stats::ar.ols()} fits an \emph{AR(p)} model, but it produces slightly different coefficients than linear regression, because it uses a different calibration procedure.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Specify AR process parameters
> n_rows <- 1e3
> co_eff <- c(0.1, 0.39, 0.5); n_coeff <- NROW(co_eff)
> set.seed(1121); in_nov <- rnorm(n_rows)
> # ari_ma <- filter(x=in_nov, filter=co_eff, method="recursive")
> # Simulate AR process using C_rfilter()
> ari_ma <- .Call(stats:::C_rfilter, in_nov, co_eff,
+   double(n_rows + n_coeff))[-(1:n_coeff)]
> # Fit AR model using ar.ols()
> ar_fit <- ar.ols(ari_ma, order.max=n_coeff, aic=FALSE)
> class(ar_fit)
> is.list(ar_fit)
> drop(ar_fit$ar)
> # Define design matrix without intercept column
> de_sign <- sapply(1:n_coeff, rutils::lag_it, in_put=ari_ma)
> # Fit AR model using regression
> design_inv <- MASS::ginv(de_sign)
> coeff_fit <- drop(design_inv %*% ari_ma)
> all.equal(drop(ar_fit$ar), coeff_fit, check.attributes=FALSE)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: Calibrating Autoregressive Models Using Maximum Likelihood}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{autoregressive} process \emph{AR(p)} defined as:
      \begin{multline*}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i = \\
        \sum_{j=1}^{p} {\varphi_j r_{i-j}} + \xi_i
      \end{multline*}
      Can be expressed as a \emph{multivariate} linear regression model, with the \emph{response} equal to $r_i$, and the columns of the \emph{design matrix} equal to the lags of $r_i$.
      \vskip1ex
      The function \texttt{stats::arima()} calibrates (fits) an \emph{ARIMA} model to a univariate time series, using the \emph{maximum likelihood} method (which may give slightly different coefficients than the linear regression model).
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Specify AR process parameters
> n_rows <- 1e3
> co_eff <- c(0.1, 0.39, 0.5); n_coeff <- NROW(co_eff)
> # Simulate AR process using C_rfilter()
> set.seed(1121); in_nov <- rnorm(n_rows)
> ari_ma <- .Call(stats:::C_rfilter, in_nov, co_eff,
+   double(n_rows + n_coeff))[-(1:n_coeff)]
> 
> 
> # wippp
> # Calibrate ARIMA model using regression
> # Define design matrix
> ari_ma <- (ari_ma - mean(ari_ma))
> de_sign <- sapply(1:3, rutils::lag_it, in_put=ari_ma)
> # Calculate de-meaned re_turns matrix
> de_sign <- t(t(de_sign) - colMeans(de_sign))
> design_inv <- MASS::ginv(de_sign)
> # Regression coefficients with response equal to ari_ma
> coeff_fit <- drop(design_inv %*% ari_ma)
> 
> all.equal(arima_fit$coef, coeff_fit, check.attributes=FALSE)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Standard Errors of the \protect\emph{AR(p)} Coefficients}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{standard errors} of the fitted \emph{AR(p)} coefficients are proportional to the standard deviation of the fitted residuals.
      \vskip1ex
      Their \emph{t}-values are equal to the ratio of the fitted coefficients divided by their standard errors.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate the regression residuals
> fit_ted <- drop(de_sign %*% coeff_fit)
> residual_s <- drop(ari_ma - fit_ted)
> # Variance of residuals
> var_resid <- sum(residual_s^2)/(n_rows-NROW(coeff_fit))
> # Design matrix squared
> design_2 <- crossprod(de_sign)
> # Calculate covariance matrix of AR coefficients
> co_var <- var_resid*MASS::ginv(design_2)
> coeff_fitd <- sqrt(diag(co_var))
> # Calculate t-values of AR coefficients
> coeff_tvals <- drop(coeff_fit)/coeff_fitd
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Order Selection of \protect\emph{AR(p)} Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Order selection means determining the \emph{order parameter} $p$ of the \emph{AR(p)} model that best fits the time series.
      \vskip1ex
      The order parameter $p$ can be set equal to the number of significantly non-zero \emph{partial autocorrelations} of the time series.
      \vskip1ex
      The order parameter can also be determined by only selecting coefficients with statistically significant \emph{t}-values.
      \vskip1ex
      Fitting an \emph{AR(p)} model can be performed by first determining the order $p$, and then calculating the coefficients.
      \vskip1ex
      The function \texttt{stats::arima()} calibrates (fits) an \emph{ARIMA} model to a univariate time series.
      \vskip1ex
      The function \texttt{auto.arima()} from the package \emph{forecast} performs order selection, and calibrates an \emph{AR(p)} model to a univariate time series.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Fit AR(5) model into AR(3) process
> de_sign <- sapply(1:5, rutils::lag_it, in_put=ari_ma)
> design_inv <- MASS::ginv(de_sign)
> coeff_fit <- drop(design_inv %*% ari_ma)
> # Calculate t-values of AR(5) coefficients
> residual_s <- drop(ari_ma - drop(de_sign %*% coeff_fit))
> var_resid <- sum(residual_s^2)/(n_rows-NROW(coeff_fit))
> co_var <- var_resid*MASS::ginv(crossprod(de_sign))
> coeff_fitd <- sqrt(diag(co_var))
> coeff_tvals <- drop(coeff_fit)/coeff_fitd
> # Fit AR(5) model using arima()
> arima_fit <- arima(ari_ma, order=c(5, 0, 0), include.mean=FALSE)
> arima_fit$coef
> # Fit AR(5) model using auto.arima()
> library(forecast)  # Load forecast
> arima_fit <- forecast::auto.arima(ari_ma, max.p=5, max.q=0, max.d=0)
> # Fit AR(5) model into VTI returns
> re_turns <- drop(zoo::coredata(na.omit(rutils::etf_env$re_turns$VTI)))
> de_sign <- sapply(1:5, rutils::lag_it, in_put=re_turns)
> design_inv <- MASS::ginv(de_sign)
> coeff_fit <- drop(design_inv %*% re_turns)
> # Calculate t-values of AR(5) coefficients
> residual_s <- drop(re_turns - drop(de_sign %*% coeff_fit))
> var_resid <- sum(residual_s^2)/(n_rows-NROW(coeff_fit))
> co_var <- var_resid*MASS::ginv(crossprod(de_sign))
> coeff_fitd <- sqrt(diag(co_var))
> coeff_tvals <- drop(coeff_fit)/coeff_fitd
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


% wippp: add order selection and AIC
%%%%%%%%%%%%%%%
\subsection{draft: \protect\emph{AR(p)} Order Selection Using Information Criteria}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Fitting a time series to an \emph{AR(p)} model requires selecting the \emph{order} parameter $p$.
      \vskip1ex
      The \emph{order} parameter $p$ of the \emph{AR(p)} model is equal to the number of non-zero \emph{partial autocorrelations} of the time series.
      \vskip1ex
      Order selection means determining the order $p$ of the \emph{AR(p)} model that best fits the time series.
      \vskip1ex
      Calibrating an \emph{AR(p)} model is a two-step process: first determine the order $p$ of the \emph{AR(p)} model, and then calculate the coefficients.
      \vskip1ex
      The function \texttt{auto.arima()} from the package \emph{forecast} performs order selection, and calibrates an \emph{AR(p)} model to a univariate time series.
      \vskip1ex
      The function \texttt{arima()} from the base package \emph{stats} fits an \emph{AR(p)} model to a univariate time series.
      \vskip1ex
      The function \texttt{auto.arima()} from the package \emph{forecast} automatically calibrates an \emph{AR(p)} model to a univariate time series.
      \vskip1ex
      An \emph{autoregressive} process \emph{AR(p)} defined as:
      \begin{multline*}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i = \\
        \sum_{j=1}^{p} {\varphi_j r_{i-j}} + \xi_i
      \end{multline*}
      Can be solved as a \emph{multivariate} linear regression model, with the \emph{response} equal to $r_i$, and the \emph{design matrix} columns equal to the lags of $r_i$.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calibrate ARIMA model using auto.arima()
> # library(forecast)  # Load forecast
> forecast::auto.arima(ari_ma, max.p=3, max.q=0, max.d=0)
> # Calibrate ARIMA model using arima()
> arima_fit <- arima(ari_ma, order=c(3,0,0), include.mean=FALSE)
> arima_fit$coef
> # Calibrate ARIMA model using auto.arima()
> # library(forecast)  # Load forecast
> forecast::auto.arima(ari_ma, max.p=3, max.q=0, max.d=0)
> # Calibrate ARIMA model using regression
> ari_ma <- as.numeric(ari_ma)
> # Define design matrix for ari_ma
> de_sign <- sapply(1:3, rutils::lag_it, in_put=ari_ma)
> # Generalized inverse of design matrix
> design_inv <- MASS::ginv(de_sign)
> # Regression coefficients with response equal to ari_ma
> co_eff <- drop(design_inv %*% ari_ma)
> all.equal(arima_fit$coef, co_eff, check.attributes=FALSE)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Yule-Walker Equations}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      To lighten the notation we can assume that the time series $r_i$ has zero mean $\mathbb{E}[r_i] = 0$ and unit variance $\mathbb{E}[r^2_i] = 1$.  ($\mathbb{E}$ is the expectation operator.)
      \vskip1ex
      Then the \emph{autocorrelations} of $r_i$ are equal to: $\rho_k = \mathbb{E}[r_i r_{i-k}]$.
      \vskip1ex
      If we multiply the \emph{autoregressive} process \emph{AR(p)}: $r_i = \sum_{j=1}^{p} {\varphi_j r_{i-j}} + \xi_i$, by $r_{i-k}$ and take the expectations, then we obtain the Yule-Walker equations:
      \begin{displaymath}
        \begin{pmatrix}
          \rho_1 \\
          \rho_2 \\
          \rho_3 \\
          \vdots \\
          \rho_p
        \end{pmatrix} =
        \begin{pmatrix}
          1 & \rho_1 & \dots & \rho_{p-1} \\
          \rho_1 & 1 & \dots & \rho_{p-2} \\
          \rho_2 & \rho_1 & \dots & \rho_{p-3} \\
          \vdots & \vdots & \ddots & \vdots \\
          \rho_{p-1} & \rho_{p-2} & \dots & 1
        \end{pmatrix}
        \begin{pmatrix}
          \varphi_1 \\
          \varphi_2 \\
          \varphi_3 \\
          \vdots \\
          \varphi_p
        \end{pmatrix}
      \end{displaymath}
      The Yule-Walker equations relate the \emph{autocorrelation coefficients} $\rho_i$ with the coefficients of the \emph{AR(p)} process $\varphi_i$.
      \vskip1ex
      The Yule-Walker equations can be solved for the \emph{AR(p)} coefficients $\varphi_i$ using matrix inversion.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Compute autocorrelation coefficients
> ac_f <- acf(ari_ma, lag=10, plot=FALSE)
> ac_f <- drop(ac_f$acf)
> acf1 <- ac_f[-NROW(ac_f)]
> # Define Yule-Walker matrix
> yule_walker <- sapply(2:9, function(lagg) {
+   c(acf1[lagg:1], acf1[2:(NROW(acf1)-lagg+1)])
+ })  # end sapply
> yule_walker <- cbind(acf1, yule_walker, rev(acf1))
> # Generalized inverse of Yule-Walker matrix
> yule_walker_inv <- MASS::ginv(yule_walker)
> # Solve Yule-Walker equations
> coeff_yw <- drop(yule_walker_inv %*% ac_f[-1])
> round(coeff_yw, 5)
> coeff_fit
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Forecasting Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      An \emph{autoregressive} process \emph{AR(p)}:
      \begin{displaymath}
        r_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p} + \xi_i
      \end{displaymath}
      Can be simulated using the function \texttt{filter()} with the argument \texttt{method="recursive"}.
      \vskip1ex
      Filtering can be performed even faster by directly calling the compiled \texttt{C++} function \texttt{stats:::C\_rfilter()}.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> n_rows <- 1e2
> co_eff <- c(0.1, 0.39, 0.5); n_coeff <- NROW(co_eff)
> set.seed(1121); in_nov <- rnorm(n_rows)
> # Simulate AR process using filter()
> ari_ma <- filter(x=in_nov, filter=co_eff, method="recursive")
> ari_ma <- as.numeric(ari_ma)
> # Simulate AR process using C_rfilter()
> arima_fast <- .Call(stats:::C_rfilter, in_nov, co_eff,
+   double(n_rows + n_coeff))
> all.equal(ari_ma, arima_fast[-(1:n_coeff)],
+   check.attributes=FALSE)
\end{verbatim}
\end{kframe}
\end{knitrout}
      The one step ahead \emph{forecast} $f_i$ is equal to the \emph{convolution} of the time series $r_i$ with the \emph{AR(p)} coefficients:
      \begin{displaymath}
        f_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p}
      \end{displaymath}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/ar_forecast.png}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Forecast AR(3) process using loop in R
> forecast_s <- numeric(NROW(ari_ma)+1)
> forecast_s[1] <- 0
> forecast_s[2] <- co_eff[1]*ari_ma[1]
> forecast_s[3] <- co_eff[1]*ari_ma[2] + co_eff[2]*ari_ma[1]
> for (it in 4:NROW(forecast_s)) {
+   forecast_s[it] <- ari_ma[(it-1):(it-3)] %*% co_eff
+ }  # end for
> # Plot with legend
> x11(width=6, height=5)
> par(mar=c(3, 3, 2, 1), oma=c(0, 0, 0, 0), mgp=c(2, 1, 0))
> plot(ari_ma, main="Forecasting Using AR(3) Model",
+   xlab="", ylab="", type="l")
> lines(forecast_s, col="orange", lwd=3)
> legend(x="topright", legend=c("series", "forecasts"),
+  col=c("black", "orange"), lty=1, lwd=6,
+  cex=0.9, bg="white", bty="n")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fast Forecasting of Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The one step ahead \emph{forecast} $f_i$ is equal to the \emph{convolution} of the time series $r_i$ with the \emph{AR(p)} coefficients:
      \begin{displaymath}
        f_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p}
      \end{displaymath}
      The above \emph{convolution} can be quickly calculated by using the function \texttt{filter()} with the argument \texttt{method="convolution"}.
      \vskip1ex
      The convolution can be calculated even faster by directly calling the compiled \texttt{C++} function \texttt{stats:::C\_cfilter()}.
      \vskip1ex
      The forecasts can also be calculated using the design matrix multiplied by the \emph{AR(p)} coefficients.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Forecast using filter()
> filter_fast <- filter(x=ari_ma, sides=1,
+   filter=co_eff, method="convolution")
> filter_fast <- as.numeric(filter_fast)
> # Compare excluding warmup period
> all.equal(forecast_s[-(1:n_coeff)], filter_fast[-(1:(n_coeff-1))],
+     check.attributes=FALSE)
> # Filter using C_cfilter() compiled C++ function directly
> filter_fast <- .Call(stats:::C_cfilter, ari_ma, filter=co_eff,
+                sides=1, circular=FALSE)
> # Compare excluding warmup period
> all.equal(forecast_s[-(1:n_coeff)], filter_fast[-(1:(n_coeff-1))],
+     check.attributes=FALSE)
> # Define predictor matrix for forecasting
> predic_tor <- sapply(0:(n_coeff-1), function(lagg) {
+   rutils::lag_it(ari_ma, lagg=lagg)
+ })  # end sapply
> # Forecast using predictor matrix
> filter_fast <- c(0, drop(predic_tor %*% co_eff))
> # Compare with loop in R
> all.equal(forecast_s, filter_fast, check.attributes=FALSE)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Forecasting Using \texttt{predict.Arima()}}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The forecasts of the \emph{AR(p)} process can also be calculated using the function \texttt{predict()}.
      \vskip1ex
      The function \texttt{predict()} is a \emph{generic function} for forecasting based on a given model.
      \vskip1ex
      The \emph{method} \texttt{predict.Arima()} is \emph{dispatched} by \texttt{R} for calculating predictions from \emph{ARIMA} models produced by the function \texttt{stats::arima()}.
      \vskip1ex
      The \emph{method} \texttt{predict.Arima()} returns a prediction object which is a \texttt{list} containing the predicted value and its standard error.
      \vskip1ex
      The function \texttt{stats::arima()} calibrates (fits) an \emph{ARIMA} model to a univariate time series, using the \emph{maximum likelihood} method (which may give slightly different coefficients than the linear regression model).
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Fit ARIMA model using arima()
> arima_fit <- arima(ari_ma, order=c(3,0,0), include.mean=FALSE)
> arima_fit$coef
> co_eff
> # One-step-ahead forecast using predict.Arima()
> pre_dict <- predict(arima_fit, n.ahead=1)
> # Or directly call predict.Arima()
> # pre_dict <- predict.Arima(arima_fit, n.ahead=1)
> # Inspect the prediction object
> class(pre_dict)
> names(pre_dict)
> class(pre_dict$pred)
> unlist(pre_dict)
> # One-step-ahead forecast using matrix algebra
> fore_cast <- drop(ari_ma[n_rows:(n_rows-2)] %*% arima_fit$coef)
> # Compare one-step-ahead forecasts
> all.equal(pre_dict$pred[[1]], fore_cast)
> # Get information about predict.Arima()
> ?stats:::predict.Arima
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Forecasting Residuals}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{forecasting residuals} $\varepsilon_i$ are equal to the differences between the actual values $r_i$ minus their \emph{forecasts} $f_i$: $\varepsilon_i = r_i - f_i$.
      \vskip1ex
      Accurate forecasting of an \emph{AR(p)} process requires knowing its coefficients.
      \vskip1ex
      If the coefficients of the \emph{AR(p)} process are known exactly, then its \emph{in-sample residuals} $\varepsilon_i$ are equal to its \emph{innovations} $\xi_i$: $\varepsilon_i = r_i - f_i = \xi_i$.
      \vskip1ex
      In practice, the \emph{AR(p)} coefficients are not known, so they must be fitted to the empirical time series.
      \vskip1ex
      If the \emph{AR(p)} coefficients are fitted to the empirical time series, then its \emph{residuals} are \emph{not} equal to its \emph{innovations}.
    \column{0.5\textwidth}
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate the in-sample forecasting residuals
> residual_s <- (ari_ma - forecast_s[-NROW(forecast_s)])
> # Compare residuals with innovations
> all.equal(in_nov, residual_s, check.attributes=FALSE)
> plot(residual_s, t="l", lwd=3, xlab="", ylab="",
+      main="ARIMA Forecast Errors")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{draft: The Standard Errors of Forecasts from Autoregressive Processes}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      Trivial: The variance of the predicted value is equal to the predictor vector multiplied by the covariance matrix of the regression coefficients.
      \vskip1ex
      The one step ahead \emph{forecast} $f_i$ of the time series $r_i$ using the process \emph{AR(p)} is defined as:
      \begin{displaymath}
        f_i = \varphi_1 r_{i-1} + \varphi_2 r_{i-2} + \ldots + \varphi_p r_{i-p}
      \end{displaymath}
      The function \texttt{filter()} with the argument \texttt{method="convolution"} calculates the convolution of a vector with a filter.
      \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Simulate AR process using filter()
> n_rows <- 1e2
> co_eff <- c(0.1, 0.39, 0.5); n_coeff <- NROW(co_eff)
> set.seed(1121)
> ari_ma <- filter(x=rnorm(n_rows), filter=co_eff, method="recursive")
> ari_ma <- as.numeric(ari_ma)
> # Forecast AR(3) process
> forecast_s <- numeric(NROW(ari_ma))
> forecast_s[2] <- co_eff[1]*ari_ma[1]
> forecast_s[3] <- co_eff[1]*ari_ma[2] + co_eff[2]*ari_ma[1]
> for (it in 4:NROW(forecast_s)) {
+   forecast_s[it] <- ari_ma[(it-1):(it-3)] %*% co_eff
+ }  # end for
> # Forecast using filter()
> forecasts_filter <- filter(x=ari_ma, sides=1,
+   filter=co_eff, method="convolution")
> class(forecasts_filter)
> all.equal(forecast_s[-(1:4)],
+   forecasts_filter[-c(1:3, NROW(forecasts_filter))],
+   check.attributes=FALSE)
> # Compare residuals with innovations
> residual_s <- (ari_ma-forecast_s)
> tail(cbind(in_nov, residual_s))
> 
> 
> # ari_ma <- as.numeric(lh)
> # n_rows <- NROW(ari_ma)
> # Compare one-step-ahead forecasts
> # arima_fit <- arima(ari_ma, order=c(3,0,0), method="ML", include.mean=FALSE)
> 
> # Compare many one-step-ahead forecasts
> forecast_s <- sapply(31:n_rows, function(x) {
+   cat("len = ", x, "\n")
+   # ari_ma <- filter(x=rnorm(n_rows+1), filter=co_eff, method="recursive")
+   arima_fit <- arima(ari_ma[1:x], order=c(3,0,0), include.mean=FALSE)
+   pre_dict <- predict(arima_fit, n.ahead=1)
+   fore_cast <- drop(ari_ma[x:(x-2)] %*% arima_fit$coef)
+   c(actual=ari_ma[x+1], forecast=fore_cast, predict=as.numeric(pre_dict$pred))
+ })  # end sapply
> foo <- t(foo)
> # hist(foo[, 1], breaks=30,
> #   main="", ylim=c(0, 60), xlim=c(-0.04, 0.04),
> #   xlab="", ylab="", freq=FALSE)
> 
> hist(foo[, 1], ylim=c(0, 0.15), freq=FALSE)
> lines(density(foo[, 1]), col='blue', lwd=3)
> lines(density(foo[, 2]), col='green', lwd=3)
> lines(density(foo[, 3]), col='red', lwd=3)
> 
> 
> # Forecast AR(3) process
> forecast_s <- numeric(NROW(ari_ma))
> forecast_s[2] <- co_eff[1]*ari_ma[1]
> forecast_s[3] <- co_eff[1]*ari_ma[2] + co_eff[2]*ari_ma[1]
> for (it in 4:NROW(forecast_s)) {
+   forecast_s[it] <- ari_ma[(it-1):(it-3)] %*% co_eff
+ }  # end for
> # Forecast using filter()
> forecasts_filter <- filter(x=ari_ma, sides=1,
+   filter=co_eff, method="convolution")
> class(forecasts_filter)
> all.equal(forecast_s[-(1:4)],
+   forecasts_filter[-c(1:3, NROW(forecasts_filter))],
+   check.attributes=FALSE)
> # Compare residuals with innovations
> residual_s <- (ari_ma-forecast_s)
> tail(cbind(in_nov, residual_s))
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/ar_forecast.png}
      Accurate forecasting requires knowing the order $p$ of the \emph{AR(p)} process and its coefficients.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Plot with legend
> plot(ari_ma, main="Forecasting Using AR(3) Model",
+   xlab="", ylab="", type="l")
> lines(forecast_s, col="orange", lwd=3)
> legend(x="topright", legend=c("series", "forecasts"),
+  col=c("black", "orange"), lty=1, lwd=6,
+  cex=0.9, bg="white", bty="n")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Fitting and Forecasting Autoregressive Models}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      In practice, the \emph{AR(p)} coefficients are not known, so they must be fitted to the empirical time series first, before forecasting.
      \vskip1ex
      Forecasting using an autoregressive model is performed by first fitting an \emph{AR(p)} model to past data, and calculating its coefficients.
      \vskip1ex
      The fitted coefficients are then applied to calculating the \emph{out-of-sample} forecasts.
      \vskip1ex
      The model fitting procedure depends on two unknown \emph{meta-parameters}: the order $p$ of the \emph{AR(p)} model and the length of the look-back interval (\texttt{look\_back}).
    \column{0.5\textwidth}
    \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Define AR process parameters
> n_rows <- 1e3
> co_eff <- c(0.5, 0.0, 0.0); n_coeff <- NROW(co_eff)
> set.seed(1121); in_nov <- rnorm(n_rows)
> # Simulate AR process using C_rfilter()
> ari_ma <- .Call(stats:::C_rfilter, in_nov, co_eff,
+   double(n_rows + n_coeff))[-(1:n_coeff)]
> # Define order of the AR(p) forecasting model
> or_der <- 5
> # Define predictor matrix for forecasting
> de_sign <- sapply(1:or_der, rutils::lag_it, in_put=ari_ma)
> colnames(de_sign) <- paste0("pred_", 1:NCOL(de_sign))
> # Add response equal to series
> de_sign <- cbind(ari_ma, de_sign)
> colnames(de_sign)[1] <- "response"
> # Specify length of look-back interval
> look_back <- 100
> # Invert the predictor matrix
> rang_e <- (n_rows-look_back):(n_rows-1)
> design_inv <- MASS::ginv(de_sign[rang_e, -1])
> # Calculate fitted coefficients
> coeff_fit <- drop(design_inv %*% de_sign[rang_e, 1])
> # Calculate forecast
> drop(de_sign[n_rows, -1] %*% coeff_fit)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Autoregressive Forecasting Models}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      \emph{Backtesting} is the simulation of a model on historical data to test its forecasting accuracy.
      \vskip1ex
      The autoregressive forecasting model can be \emph{backtested} by calculating forecasts over either a \emph{rolling} or an \emph{expanding} look-back interval.
      \vskip1ex
      If the start date is fixed at the first row then the look-back interval is \emph{expanding}.
      \vskip1ex
      The coefficients of the \emph{AR(p)} process are fitted to past data, and then applied to calculating out-of-sample forecasts.
      \vskip1ex
      The \emph{backtesting} procedure allows determining the optimal \emph{meta-parameters} of the forecasting model: the order $p$ of the \emph{AR(p)} model and the length of look-back interval (\texttt{look\_back}).
    \column{0.5\textwidth}
    \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Calculate a vector of daily VTI log returns
> re_turns <- na.omit(rutils::etf_env$re_turns$VTI)
> date_s <- index(re_turns)
> re_turns <- as.numeric(re_turns)
> n_rows <- NROW(re_turns)
> # Define predictor as a rolling sum
> n_agg <- 5
> predic_tor <- rutils::roll_sum(re_turns, look_back=n_agg)
> # Shift the res_ponse forward out-of-sample
> res_ponse <- rutils::lag_it(predic_tor, lagg=(-n_agg))
> # Define predictor matrix for forecasting
> order_max <- 5
> predic_tor <- sapply(1+n_agg*(0:order_max), rutils::lag_it,
+                in_put=predic_tor)
> predic_tor <- cbind(rep(1, n_rows), predic_tor)
> # Define de_sign matrix
> de_sign <- cbind(res_ponse, predic_tor)
> # Perform rolling forecasting
> look_back <- 100
> forecast_s <- sapply((look_back+1):n_rows, function(end_p) {
+   # Define rolling look-back range
+   start_p <- max(1, end_p-look_back)
+   # Or expanding look-back range
+   # start_p <- 1
+   rang_e <- start_p:(end_p-1)
+   # Invert the predictor matrix
+   design_inv <- MASS::ginv(de_sign[rang_e, -1])
+   # Calculate fitted coefficients
+   coeff_fit <- drop(design_inv %*% de_sign[rang_e, 1])
+   # Calculate forecast
+   drop(de_sign[end_p, -1] %*% coeff_fit)
+ })  # end sapply
> # Add warmup period
> forecast_s <- c(rep(0, look_back), forecast_s)
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Accuracy of the Autoregressive Forecasting Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The accuracy of a forecasting model can be measured using the \emph{mean squared error} and the \emph{correlation}.
      \vskip1ex
      The mean squared error (\emph{MSE}) of a forecasting model is the average of the squared forecasting residuals $\varepsilon_i$, equal to the differences between the actual values $r_i$ minus the \emph{forecasts} $f_i$: $\varepsilon_i = r_i - f_i$:
      \begin{displaymath}
        \operatorname{MSE} = \frac{1}{n} \sum_{i=1}^n (r_i - f_i)^2
      \end{displaymath}
    \column{0.5\textwidth}
      \vspace{-1em}
      \includegraphics[width=0.4\paperwidth]{figure/ar_forecast_resid.png}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Mean squared error
> mean((re_turns - forecast_s)^2)
> # Correlation
> cor(forecast_s, re_turns)
> # Plot forecasting series with legend
> x11(width=6, height=5)
> par(mar=c(3, 3, 2, 1), oma=c(0, 0, 0, 0))
> plot(forecast_s[(n_rows-look_back):n_rows], col="red", 
+      xlab="", ylab="", type="l", lwd=2,
+      main="Rolling Forecasting Using AR Model")
> lines(re_turns[(n_rows-look_back):n_rows], col="blue", lwd=2)
> legend(x="top", legend=c("re_turns", "forecasts"),
+  col=c("blue", "red"), lty=1, lwd=6,
+  cex=0.9, bg="white", bty="n")
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{Backtesting Function for the Forecasting Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{meta-parameters} of the \emph{backtesting} function are the order $p$ of the \emph{AR(p)} model and the length of the look-back interval (\texttt{look\_back}).
    \column{0.5\textwidth}
    \vspace{-1em}
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> # Define backtesting function
> sim_forecasts <- function(res_ponse, predic_tor=res_ponse, n_agg=5,
+                 or_der=5, look_back=100) {
+   n_rows <- NROW(res_ponse)
+   # Define predictor as a rolling sum
+   predic_tor <- rutils::roll_sum(res_ponse, look_back=n_agg)
+   # Shift the res_ponse forward out-of-sample
+   res_ponse <- rutils::lag_it(predic_tor, lagg=(-n_agg))
+   # Define predictor matrix for forecasting
+   predic_tor <- sapply(1+n_agg*(0:or_der), rutils::lag_it,
+                  in_put=predic_tor)
+   predic_tor <- cbind(rep(1, n_rows), predic_tor)
+   # Define de_sign matrix
+   de_sign <- cbind(res_ponse, predic_tor)
+   # Perform rolling forecasting
+   forecast_s <- sapply((look_back+1):n_rows, function(end_p) {
+     # Define rolling look-back range
+     start_p <- max(1, end_p-look_back)
+     # Or expanding look-back range
+     # start_p <- 1
+     rang_e <- start_p:(end_p-1)
+     # Invert the predictor matrix
+     design_inv <- MASS::ginv(de_sign[rang_e, -1])
+     # Calculate fitted coefficients
+     coeff_fit <- drop(design_inv %*% de_sign[rang_e, 1])
+     # Calculate forecast
+     drop(de_sign[end_p, -1] %*% coeff_fit)
+   })  # end sapply
+   # Add warmup period
+   forecast_s <- c(rep(0, look_back), forecast_s)
+   rutils::roll_sum(forecast_s, look_back=n_agg)
+ }  # end sim_forecasts
> # Simulate the rolling autoregressive forecasts
> forecast_s <- sim_forecasts(re_turns, or_der=5, look_back=100)
> c(mse=mean((re_turns - forecast_s)^2), cor=cor(re_turns, forecast_s))
\end{verbatim}
\end{kframe}
\end{knitrout}
  \end{columns}
\end{block}

\end{frame}


%%%%%%%%%%%%%%%
\subsection{The Optimal Parameters of the Forecasting Model}
\begin{frame}[fragile,t]{\subsecname}
\vspace{-1em}
\begin{block}{}
  \begin{columns}[T]
    \column{0.5\textwidth}
      The \emph{backtesting} function can be used to find the optimal \emph{meta-parameters} of the autoregressive forecasting model.
      \vskip1ex
      The accuracy of the forecasting model depends on the order $p$ of the \emph{AR(p)} model and on the length of the look-back interval (\texttt{look\_back}).
      \vskip1ex
      The two \emph{meta-parameters} can be chosen by minimizing the \emph{MSE} of the model forecasts in a \emph{backtest} simulation.
\begin{knitrout}\tiny
\definecolor{shadecolor}{rgb}{0.933, 0.933, 0.933}\color{fgcolor}\begin{kframe}
\begin{verbatim}
> look_backs <- seq(20, 200, 20)
> back_tests <- sapply(look_backs, back_test, se_ries=ari_ma, or_der=or_der)
> back_tests <- t(back_tests)
> rownames(back_tests) <- look_backs
> # Plot forecasting series with legend
> plot(x=look_backs, y=back_tests[, 1],
+   xlab="look-back", ylab="MSE", type="l", lwd=2,
+   main="MSE of AR(5) Forecasting Model")
\end{verbatim}
\end{kframe}
\end{knitrout}
    \column{0.5\textwidth}
      \includegraphics[width=0.45\paperwidth]{figure/ar_forecast_mse.png}
  \end{columns}
\end{block}

\end{frame}


\end{document}
