pre_d <- drop(new_datav %*% beta_s)
pre_d + t_quant*s_d
s_d <- drop(sqrt(
new_datav %*% beta_covar %*% t(new_datav)))
pre_d
predict.lm(object=mod_el, newdata=new_data, interval="confidence", level=1-2*(1-pnorm(2)))
pre_d
(pre_d + t_quant*s_d)
(pre_d - t_quant*s_d)
new_data
col_names <- colnames(de_sign)
col_names
# New data predictor is a data frame or row vector
set.seed(1121)
new_data <- data.frame(1, rnorm(5))
col_names <- colnames(de_sign)
colnames(new_data) <- col_names
col_names
new_data
# New data predictor is a data frame or row vector
set.seed(1121)
new_data <- data.frame(c(1, rnorm(5)))
col_names <- colnames(de_sign)
colnames(new_data) <- col_names
new_data
# New data predictor is a data frame or row vector
set.seed(1121)
new_data <- data.frame(cbind(1, rnorm(5)))
new_data
cbind(1, rnorm(5))
new_data <- data.frame(matrix(c(1, rnorm(5)), nr=1))
new_data
col_names <- colnames(de_sign)
colnames(new_data) <- col_names
new_data
new_datav
# New data predictor is a data frame or row vector
set.seed(1121)
new_data <- data.frame(matrix(c(1, rnorm(5)), nr=1))
col_names <- colnames(de_sign)
colnames(new_data) <- col_names
new_datav
new_data
pre_d
predict.lm(object=mod_el, newdata=new_data, interval="confidence", level=1-2*(1-pnorm(2)))
for_mula <- paste0("res_ponse ~ ", paste(colnames(de_sign), collapse=" + "), " - 1")
# Define multivariate regression using formula
mod_el <- lm(for_mula, data=data.frame(cbind(res_ponse, de_sign)))
model_sum <- summary(mod_el)
predict.lm(object=mod_el, newdata=new_data, interval="confidence", level=1-2*(1-pnorm(2)))
for_mula <- paste0("res_ponse ~ ", paste(colnames(de_sign), collapse=" + "), " - 1")
# Specify multivariate regression using formula
mod_el <- lm(for_mula, data=data.frame(cbind(res_ponse, de_sign)))
model_sum <- summary(mod_el)
# Predict from lm object
predict_lm <- predict.lm(object=mod_el, newdata=new_data,
interval="confidence", level=1-2*(1-pnorm(2)))
predict_lm
predic_tion <- drop(new_datav %*% beta_s)
std_dev <- drop(sqrt(
new_datav %*% beta_covar %*% t(new_datav)))
predic_tion
std_dev
1-2*(1-pnorm(2))
all.equal(predict_lm$fit, predic_tion)
predict_lm
all.equal(predict_lm["fit"], predic_tion)
predict_lm
all.equal(predict_lm[["fit"]], predic_tion)
predict_lm[["fit"]]
all.equal(predict_lm[1, "fit"], predic_tion)
all.equal(predict_lm[1, "fit"], predic_tion)
all.equal(predict_lm[1, "lwr"], predict_low)
all.equal(predict_lm[1, "upr"], predict_high)
predict_high <- (predic_tion + t_quant*std_dev)
predict_low <- (predic_tion - t_quant*std_dev)
all.equal(predict_lm[1, "fit"], predic_tion)
all.equal(predict_lm[1, "lwr"], predict_low)
all.equal(predict_lm[1, "upr"], predict_high)
rm(list = ls())
set.seed(1121)  # initialize random number generator
# Define explanatory (design) variable
len_gth <- 100
de_sign <- runif(len_gth)
noise <- rnorm(len_gth)
# Response equals linear form plus random noise
res_ponse <- (1 + de_sign + noise)
design_zm <- de_sign - mean(de_sign)
response_zm <- res_ponse - mean(res_ponse)
# Solve for the regression beta
be_ta <- sum(design_zm*response_zm) / sum(design_zm^2)
# Solve for the regression alpha
al_pha <- mean(res_ponse) - be_ta*mean(de_sign)
for_mula <- res_ponse ~ de_sign
mod_el <- lm(for_mula)  # Perform regression
class(mod_el)  # Regressions have class lm
attributes(mod_el)
eval(mod_el$call$formula)  # Regression formula
mod_el$coeff  # Regression coefficients
all.equal(coef(mod_el), c(al_pha, be_ta),
check.attributes=FALSE)
# Sum of residuals = 0
sum(mod_el$residuals)
# Sum of residuals = 0
# wippp
all.equal(mean(resid_uals), target=0)
# Sum of residuals = 0
# wippp
all.equal(mean(mod_el$residuals), target=0)
# Sum of residuals = 0
# wippp
all.equal(sum(mod_el$residuals), target=0)
model_sum <- summary(mod_el)  # Copy regression summary
library(lmtest)  # Load lmtest
# Perform Durbin-Watson test
lmtest::dwtest(mod_el)
ls()
# Calculate generalized inverse of the design matrix
design_inv <- MASS::ginv(de_sign)
de_sign
design_inv
# Calculate the influence matrix
influ_ence <- de_sign %*% design_inv
influ_ence
# Calculate the influence matrix
influ_ence <- de_sign %*% design_inv
# Plot the leverage vector
plot(x=de_sign[,2], y=diag(influ_ence),
type="l", lwd=3, col="blue",
xlab="predictor", ylab="leverage",
main="Leverage as Function of Predictor")
plot(x=de_sign, y=diag(influ_ence),
type="l", lwd=3, col="blue",
xlab="predictor", ylab="leverage",
main="Leverage as Function of Predictor")
foo <- order(len_gth)
foo
foo <- order(de_sign)
foo
de_sign[foo]
# Plot the leverage vector
plot(x=de_sign[foo], y=diag(influ_ence[foo]),
type="l", lwd=3, col="blue",
xlab="predictor", ylab="leverage",
main="Leverage as Function of Predictor")
de_sign
# Plot the leverage vector
plot(x=de_sign[foo], y=diag(influ_ence)[foo],
type="l", lwd=3, col="blue",
xlab="predictor", ylab="leverage",
main="Leverage as Function of Predictor")
model_sum
for_mula
de_sign
de_sign
# Univariate regression with linear predictor
de_sign <- cbind(rep(1, NROW(de_sign)), de_sign)
# Calculate generalized inverse of the design matrix
design_inv <- MASS::ginv(de_sign)
# Calculate the influence matrix
influ_ence <- de_sign %*% design_inv
de_sign
# Plot the leverage vector
foo <- order(de_sign[, 2])
plot(x=de_sign[foo, 2], y=diag(influ_ence)[foo],
type="l", lwd=3, col="blue",
xlab="predictor", ylab="leverage",
main="Leverage as Function of Predictor")
beta_s <- design_inv %*% res_ponse
beta_s
fit_ted <- drop(de_sign %*% beta_s)
resid_uals <- drop(res_ponse - fit_ted)
n_cols
n_rows
deg_free
deg_free <- (NROW(de_sign) - NCOL(de_sign))
deg_free
var_resid <- sqrt(sum(resid_uals^2)/deg_free)
fit_covar <- var_resid*influ_ence
fit_sd <- sqrt(diag(fit_covar))
fit_sd <- cbind(fitted=fit_ted, stddev=fit_sd)
fit_sd <- fit_sd[order(fit_ted), ]
# Plot the standard deviations
plot(fit_sd, type="l", lwd=3, col="blue",
xlab="Fitted Value", ylab="Standard Deviation",
main="Standard Deviations of Fitted Values\nin Univariate Regression")
weight_s <- c(-1, 1)
res_ponse <- de_sign %*% weight_s
# Perform loop over different realizations of random noise
fit_ted <- lapply(1:50, function(it) {
# Add random noise to response
res_ponse <- res_ponse + rnorm(n_rows, sd=1.0)
# Calculate fitted values using influence matrix
influ_ence %*% res_ponse
})  # end lapply
fit_ted <- rutils::do_call(cbind, fit_ted)
res_ponse
fit_ted <- lapply(1:50, function(it) {
# Add random noise to response
res_ponse <- res_ponse + rnorm(len_gth, sd=1.0)
# Calculate fitted values using influence matrix
influ_ence %*% res_ponse
})  # end lapply
fit_ted <- rutils::do_call(cbind, fit_ted)
matplot(x=de_sign[,2], y=fit_ted,
type="l", lty="solid", lwd=1, col="blue",
xlab="predictor", ylab="fitted",
main="Fitted Values for Different Realizations
of Random Noise")
lines(x=de_sign[,2], y=res_ponse, col="red", lwd=4)
legend(x="topleft", # Add legend
legend=c("response without noise", "fitted values"),
title=NULL, inset=0.05, cex=0.8, lwd=6,
lty=1, col=c("red", "blue"))
ls()
var_resid
# Inverse of design matrix squared
design_2 <- MASS::ginv(crossprod(de_sign))
n_rows
# Define new predictors
new_data <- (max(de_sign[, 2]) + 10*(1:5)/len_gth)
design_new <- cbind(rep(1, NROW(new_data)), new_data)
predic_tions <- cbind(
prediction=drop(design_new %*% beta_s),
stddev=diag(var_resid*sqrt(design_new %*% design_2 %*% t(design_new))))
predic_tions
x_data <- c(de_sign[,2], new_data)
x_lim <- range(x_data)
y_data <- c(fit_ted, predic_tions[, 1])
# Calculate t-quantile
t_quant <- qt(pnorm(2), df=deg_free)
predict_low <- predic_tions[, 1]-t_quant*predic_tions[, 2]
predict_high <- predic_tions[, 1]+t_quant*predic_tions[, 2]
y_lim <- range(c(res_ponse, y_data, predict_low, predict_high))
# Plot the regression predictions
plot(x=x_data, y=y_data,
xlim=x_lim, ylim=y_lim,
type="l", lwd=3, col="blue",
xlab="predictor", ylab="fitted or predicted",
main="Predictions from Linear Regression")
points(x=de_sign[,2], y=res_ponse, col="blue")
points(x=new_data, y=predic_tions[, 1], pch=16, col="blue")
lines(x=new_data, y=predict_high, lwd=3, col="red")
lines(x=new_data, y=predict_low, lwd=3, col="green")
legend(x="topleft", # Add legend
legend=c("predictions", "+2SD", "-2SD"),
title=NULL, inset=0.05, cex=0.8, lwd=6,
lty=1, col=c("blue", "red", "green"))
x_data
y_data
predic_tions
NROW(y_data)
NROW(fit_ted)
y_data <- c(fit_ted, predic_tions[, 1])
NROW(y_data)
dim(fit_ted)
fit_ted <- drop(de_sign %*% beta_s)
beta_s
fit_ted
# Define new predictors
new_data <- (max(de_sign[, 2]) + 10*(1:5)/len_gth)
design_new <- cbind(rep(1, NROW(new_data)), new_data)
predic_tions <- cbind(
prediction=drop(design_new %*% beta_s),
stddev=diag(var_resid*sqrt(design_new %*% design_2 %*% t(design_new))))
x_data <- c(de_sign[,2], new_data)
x_lim <- range(x_data)
y_data <- c(fit_ted, predic_tions[, 1])
# Calculate t-quantile
t_quant <- qt(pnorm(2), df=deg_free)
predict_low <- predic_tions[, 1]-t_quant*predic_tions[, 2]
predict_high <- predic_tions[, 1]+t_quant*predic_tions[, 2]
y_lim <- range(c(res_ponse, y_data, predict_low, predict_high))
# Plot the regression predictions
plot(x=x_data, y=y_data,
xlim=x_lim, ylim=y_lim,
type="l", lwd=3, col="blue",
xlab="predictor", ylab="fitted or predicted",
main="Predictions from Linear Regression")
points(x=de_sign[,2], y=res_ponse, col="blue")
points(x=new_data, y=predic_tions[, 1], pch=16, col="blue")
lines(x=new_data, y=predict_high, lwd=3, col="red")
lines(x=new_data, y=predict_low, lwd=3, col="green")
legend(x="topleft", # Add legend
legend=c("predictions", "+2SD", "-2SD"),
title=NULL, inset=0.05, cex=0.8, lwd=6,
lty=1, col=c("blue", "red", "green"))
rm(list = ls())
vec_tor <- 1:5
for (i in vec_tor) {
if (vec_tor[i]>3)
3
else
NULL
}  # end for
vec_tor
vec_tor <- 1:5
sapply(vec_tor, function(x) {
if (vec_tor[i] > 3)
vec_tor[i] <- 3
else
vec_tor[i] <- NULL
})  # end sapply
set.seed(1121)
li_st <- lapply(1:3, function(x) sample(6))
# You should get the following output:
li_st[[1]]
# You should get the following output:
li_st
set.seed(1121)
rep(6, 3)
set.seed(1121)
lapply(rep(6, 3), sample)
lapply(rep(3, 6), sample(3))
do.call(rbind, li_st)
do.call(cbind, li_st)
getwd()
knitr::purl("C:/Develop/R/lecture_slides/FRE6871_Lecture_3.Rnw", documentation=0)
set.seed(1121)
sample(3)
sample(3)
set.seed(1121)
sample(3)
set.seed(1121)
rm(list = l())
rm(list = ls())
li_st <- lapply(rep(3, 6), sample(3))
li_st
li_st <- lapply(rep(6, 3), sample)
li_st
set.seed(1121)
li_st <- lapply(rep(6, 3), sample)
li_st
rep(6, 3)
set.seed(1121)
lapply(1:3, function(x) sample(6))
do.call(rbind, li_st)
do.call(cbind, li_st)
vec_tor <- 1:5
for (i in vec_tor) {
if (vec_tor[i]>3)
3
else
NULL
}  # end for
vec_tor
# One possible answer:
for (i in seq_along(vec_tor)) {
if (vec_tor[i]>3)
vec_tor[i] <- 3
}  # end for
vec_tor
seq_along(vec_tor)
# 2. (20pts)
# Run this to reset the vlue of vec_tor:
vec_tor <- 1:5
sapply(vec_tor, function(x) {
if (vec_tor[i] > 3)
vec_tor[i] <- 3
else
vec_tor[i] <- NULL
})  # end sapply
vec_tor
# One possible answer:
vec_tor <- sapply(vec_tor, function(x) {
if (x > 3)
3
else
x
})  # end sapply
vec_tor
# 3. (20pts)
# Run this to reset the vlue of vec_tor:
vec_tor <- 1:5
vec_tor
vec_tor <- sapply(vec_tor, min, 3)
vec_tor
str(min)
min(vec_tor[1], 3)
min(vec_tor[2], 3)
min(vec_tor[5], 3)
big_vector <- rnorm(5000)
summary(microbenchmark(
# Allocate full memory for cumulative sum
for_loop={cum_sum <- numeric(NROW(big_vector))
cum_sum[1] <- big_vector[1]
for (i in 2:NROW(big_vector)) {
cum_sum[i] <- cum_sum[i-1] + big_vector[i]
}},  # end for
# Allocate zero memory for cumulative sum
grow_vec={cum_sum <- numeric(0)
cum_sum[1] <- big_vector[1]
for (i in 2:NROW(big_vector)) {
# Add new element to "cum_sum" ("grow" it)
cum_sum[i] <- cum_sum[i-1] + big_vector[i]
}},  # end for
# Allocate zero memory for cumulative sum
com_bine={cum_sum <- numeric(0)
cum_sum[1] <- big_vector[1]
for (i in 2:NROW(big_vector)) {
# Add new element to "cum_sum" ("grow" it)
cum_sum <- c(cum_sum, big_vector[i])
}},  # end for
times=10))[, c(1, 4, 5)]
library(microbenchmark)
summary(microbenchmark(
# Allocate full memory for cumulative sum
for_loop={cum_sum <- numeric(NROW(big_vector))
cum_sum[1] <- big_vector[1]
for (i in 2:NROW(big_vector)) {
cum_sum[i] <- cum_sum[i-1] + big_vector[i]
}},  # end for
# Allocate zero memory for cumulative sum
grow_vec={cum_sum <- numeric(0)
cum_sum[1] <- big_vector[1]
for (i in 2:NROW(big_vector)) {
# Add new element to "cum_sum" ("grow" it)
cum_sum[i] <- cum_sum[i-1] + big_vector[i]
}},  # end for
# Allocate zero memory for cumulative sum
com_bine={cum_sum <- numeric(0)
cum_sum[1] <- big_vector[1]
for (i in 2:NROW(big_vector)) {
# Add new element to "cum_sum" ("grow" it)
cum_sum <- c(cum_sum, big_vector[i])
}},  # end for
times=10))[, c(1, 4, 5)]
# Calculate cumulative sum in two different ways
summary(microbenchmark(
# Cumulative sum using "for" loop
r_loop=(for (i in 2:NROW(big_vector)) {
cum_sum[i] <- cum_sum[i-1] + big_vector[i]
}),
# Cumulative sum using "cumsum"
vec_torized=cumsum(big_vector),
times=10))[, c(1, 4, 5)]  # end microbenchmark summary
set.seed(1121) # reset random number generator
n_rows <- 1000
da_ta <- rnorm(n_rows)
da_ta <- sort(da_ta)
head(da_ta)
tail(da_ta)
# Monte Carlo cumulative probability
pnorm(-2)
tnorm(-2)
integrate(dnorm, low=2, up=Inf)
integrate(dnorm, low=-Inf, up=-2)
1-pnorm(2)
sum(da_ta > 2)/n_rows
# Generate importance sample
lamb_da <- 1.5
lamb_da
data_is <- da_ta + lamb_da
# Importance cumulative probability
sum(data_is > 2)/n_rows
weight_s <- exp(-lamb_da*data_is + lamb_da^2/2)
sum((data_is > 2)*weight_s)/n_rows
# Bootstrap of standard errors of cumulative probability
boot_data <- sapply(1:1000, function(x) {
da_ta <- rnorm(n_rows)
m_c <- sum(da_ta > 2)/n_rows
da_ta <- (da_ta + lamb_da)
weight_s <- exp(-lamb_da*da_ta + lamb_da^2/2)
i_s <- sum((da_ta > 2)*weight_s)/n_rows
c(MC=m_c,Importance=i_s)
}) # end sapply
apply(boot_data, MARGIN=1,
function(x) c(mean=mean(x), sd=sd(x)))
# Monte Carlo expected value
integrate(function(x) x*dnorm(x), low=2, up=Inf)
sum((da_ta > 2)*da_ta)/n_rows
sum((data_is > 2)*data_is*weight_s)/n_rows
# Bootstrap of standard errors of expected value
boot_data <- sapply(1:1000, function(x) {
da_ta <- rnorm(n_rows)
m_c <- sum((da_ta > 2)*da_ta)/n_rows
da_ta <- (da_ta + lamb_da)
weight_s <- exp(-lamb_da*da_ta + lamb_da^2/2)
i_s <- sum((da_ta > 2)*da_ta*weight_s)/n_rows
c(MC=m_c,Importance=i_s)
}) # end sapply
apply(boot_data, MARGIN=1,
function(x) c(mean=mean(x), sd=sd(x)))
0.01128710/0.002943942
library(shiny); runApp('C:/Develop/R/presentations/app_credit_portfolio_loss.R')
runApp('C:/Develop/R/presentations/app_credit_portfolio_loss.R')
# Create a plotting expression
ex_pr <- quote({
par(mar=c(2, 2, 2, 1), oma=c(1, 1, 1, 1))
deg_free <- 2:20
rang_e <- (1:NROW(deg_free))
in_dex <- 4
# Plot a curve
curve(expr=dchisq(x, df=deg_free[in_dex]),
xlim=c(0, 30), ylim=c(0, 0.2),
xlab="", ylab="", lwd=3, col="red")
# Add grey lines to plot
for (it in rang_e[-in_dex]) {
curve(expr=dchisq(x, df=deg_free[it]),
xlim=c(0, 30), ylim=c(0, 0.2),
xlab="", ylab="", lwd=2, col="grey80", add=TRUE)
}  # end for
# Add title
title(main="Chi-squared Distributions", line=-1.5, cex.main=1.5)
# Add legend
text(x=20, y=0.15, labels=paste0("Degrees of freedom=",
deg_free[in_dex]), pos=1, cex=1.3)
})  # end quote
ex_pr
# Create plot by evaluating the plotting expression
x11(width=6, height=4)
eval(ex_pr)
